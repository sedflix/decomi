{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "from words import *\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import *\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "vocab_size = 100000\n",
    "max_length = 64\n",
    "batch_size = 8\n",
    "embedding_size = 50\n",
    "hidden = 50\n",
    "input_file = './datasets/wiki_dataset/wiki_en.txt' # wiki_es.txt is the other file\n",
    "\n",
    "# loging info\n",
    "data_dir = './dumps/'\n",
    "experiment_name ='en'\n",
    "extra_tokens = {'<PAD>':4, '<START>':2, '<UNK>':1, '<EOS>':3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = data_dir+experiment_name+\"./\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011141\n",
      "2013873\n"
     ]
    }
   ],
   "source": [
    "en_dict = Dictionary.load('./datasets/wiki_dataset/wiki_en.vocab')\n",
    "print(len(en_dict.token2id))\n",
    "es_dict = Dictionary.load('./datasets/wiki_dataset/wiki_es.vocab')\n",
    "print(len(es_dict.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.VocabTransform at 0x7f1b7c4e48d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_dict.merge_with(es_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3194833\n",
      "2013873\n"
     ]
    }
   ],
   "source": [
    "print(len(en_dict.token2id))\n",
    "print(len(es_dict.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54058"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dict = en_dict\n",
    "combined_dict.filter_extremes(keep_n=vocab_size, keep_tokens=None)\n",
    "combined_dict.patch_with_special_tokens(extra_tokens)\n",
    "print(len(combined_dict.token2id))\n",
    "combined_dict.token2id['lol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict.save('datasets/wiki_dataset/combined_vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict = Dictionary.load('datasets/wiki_dataset/combined_vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file) as f:\n",
    "    sentences = f.read().split(\"\\n\")\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def each(sentence):\n",
    "    x_ = combined_dict.doc2idx(text_to_word_sequence(sentence), unknown_word_index=combined_dict.token2id['<UNK>'])\n",
    "    x_.append(combined_dict.token2id['<EOS>'])\n",
    "    return sequence.pad_sequences([x_], maxlen=max_length, dtype='int32', padding='post', truncating='post',value=combined_dict.token2id['<PAD>'])[0]\n",
    "each(sentences[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why can't it share its memory. Use joblib or something thread base if RAM is a bottle-neck\n",
    "pool = multiprocessing.Pool(processes=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pool.map(each, (sentence for sentence in sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 28.6 s, total: 28.6 s\n",
      "Wall time: 31.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.save('datasets/wiki_dataset/wiki_en_100004_vocab.npy', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102435443, 64)\n",
      "(22362530, 64)\n",
      "(36866, 64)\n"
     ]
    }
   ],
   "source": [
    "x_en = np.load('datasets/wiki_dataset/wiki_en_100004_vocab.npy')\n",
    "print(x_en.shape)\n",
    "x_es = np.load('datasets/wiki_dataset/wiki_es_100004_vocab.npy')\n",
    "print(x_es.shape)\n",
    "x_en_es = np.load('datasets/wiki_dataset/twitter_en_es_100004_vocab.npy')\n",
    "print(x_en_es.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_en_es_combined = np.concatenate([x_en_es for _ in range(min(x_es.shape[0],x_en.shape[0])//x_en_es.shape[0] -1)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numwords = len(combined_dict.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_x(x):\n",
    "    batches = []\n",
    "    for i in range(1,len(x)-batch_size,batch_size):\n",
    "        batches.append(x[i:i+batch_size])\n",
    "    batches = np.array(batches)\n",
    "    print(batches.shape)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(x_en)\n",
    "np.random.shuffle(x_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(x_en_es_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_en_batched = batch_x(x_en)\n",
    "# x_es_batched = batch_x(x_es)\n",
    "# x_en_es_batched = batch_x(x_en_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(seq):\n",
    "    return ' '.join(combined_dict[id_] for id_ in seq)\n",
    "# print('Finished loading. ', sum([b.shape[0] for b in batches]), ' sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_38 (InputLayer)        (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_14 (Embedding)     (None, None, 100)         10000400  \n",
      "_________________________________________________________________\n",
      "lstm_37 (LSTM)               (None, None, 100)         80400     \n",
      "_________________________________________________________________\n",
      "time_distributed_48 (TimeDis (None, None, 100004)      10100404  \n",
      "=================================================================\n",
      "Total params: 20,181,204\n",
      "Trainable params: 20,181,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_ = Input(shape=(None, ))\n",
    "embedding = Embedding(numwords, 100, input_length=None)\n",
    "embedded = embedding(input_)\n",
    "\n",
    "decoder_lstm = LSTM(100, return_sequences=True)\n",
    "h = decoder_lstm(embedded)\n",
    "\n",
    "fromhidden = Dense(numwords, activation='linear')\n",
    "out = TimeDistributed(fromhidden)(h)\n",
    "\n",
    "model = Model(input_, out)\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "lss = sparse_loss\n",
    "\n",
    "model.compile(opt, lss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GiretTwoCell(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, cell_1 , cell_2 , nHidden , **kwargs):\n",
    "        self.cell_1 = cell_1\n",
    "        self.cell_2 = cell_2\n",
    "        self.nHidden = nHidden\n",
    "        self.state_size = [nHidden,nHidden]\n",
    "        super(GiretTwoCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        nHidden = self.nHidden\n",
    "        \n",
    "        input_shape_n = ( input_shape[0] , input_shape[1]- 2 )\n",
    "#         print \"pp\", input_shape_n\n",
    "        \n",
    "#         self.cell_1.build(input_shape_n)\n",
    "#         self.cell_2.build(input_shape_n)\n",
    "        \n",
    "        self._trainable_weights += ( self.cell_1.trainable_weights )\n",
    "        self._trainable_weights += ( self.cell_2.trainable_weights )\n",
    "        \n",
    "        self._non_trainable_weights += (  self.cell_1.non_trainable_weights )\n",
    "        self._non_trainable_weights += (  self.cell_2.non_trainable_weights )\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        \n",
    "        nHidden = self.nHidden\n",
    "        \n",
    "        gate_val_1 = inputs[ : , 0:1]\n",
    "        gate_val_2 = inputs[ : , 1:2]\n",
    "        \n",
    "        inputs  = inputs[ : , 2: ]\n",
    "                \n",
    "        gate_val_1 = K.repeat_elements(gate_val_1 , nHidden , -1 ) # shape # bs , hidden\n",
    "        gate_val_2 = K.repeat_elements(gate_val_2 , nHidden , -1 ) # shape # bs , hidden\n",
    "        \n",
    "        _ , [h1 , c1 ]  = self.cell_1.call( inputs , states )\n",
    "        _ , [h2 , c2 ]  = self.cell_2.call( inputs , states )\n",
    "        \n",
    "        h = gate_val_1*h1 + gate_val_2*h2  + (1 - gate_val_1 -  gate_val_2 )*states[0]\n",
    "        c = gate_val_1*c1 + gate_val_2*c2  + (1 - gate_val_1 -  gate_val_2 )*states[1]\n",
    "        \n",
    "        return h, [h , c ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(numwords, embedding_size, mask_zero=True)\n",
    "\n",
    "rnn_en = LSTM(hidden, return_sequences=True)\n",
    "rnn_hi = LSTM(hidden , return_sequences=True)\n",
    "\n",
    "       \n",
    "# en\n",
    "inp_en = Input((None, ))\n",
    "x = embed(inp_en)\n",
    "x = rnn_en(x)\n",
    "out_en = TimeDistributed(Dense(numwords, activation='linear'))(x)\n",
    "\n",
    "\n",
    "# hi\n",
    "inp_hi = Input((None, ))\n",
    "x = embed(inp_hi)\n",
    "x = rnn_hi( x )\n",
    "out_hi = TimeDistributed(Dense(numwords, activation='linear'))(x)\n",
    "\n",
    "\n",
    "cell_combined = GiretTwoCell(rnn_hi.cell , rnn_en.cell , hidden)\n",
    "\n",
    "        \n",
    "inp_enhi = Input((None, ))\n",
    "x = embed(inp_enhi )\n",
    "\n",
    "x_att = x\n",
    "x_att = Bidirectional(LSTM(32 , return_sequences=True))( x )\n",
    "bider_h = x_att \n",
    "x_att = TimeDistributed(Dense(3, activation='softmax') )(x_att)\n",
    "x_att = Lambda(lambda x : x[... , 1: ])(x_att)\n",
    "\n",
    "x = Concatenate(-1)([x_att , x ])\n",
    "\n",
    "x =  RNN(cell_combined , return_sequences=True)(x)\n",
    "out_enhi = TimeDistributed(Dense(numwords , activation='linear'))(x)\n",
    "        \n",
    "model = Model( [inp_hi , inp_en , inp_enhi  ] , [ out_hi , out_en , out_enhi ] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, None, 50)     5000200     input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, None, 64)     21248       embedding_7[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_27 (TimeDistri (None, None, 3)      195         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, None, 2)      0           time_distributed_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, None, 52)     0           lambda_7[0][0]                   \n",
      "                                                                 embedding_7[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  (None, None, 50)     20200       embedding_7[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  (None, None, 50)     20200       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "rnn_7 (RNN)                     (None, None, 50)     40400       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_26 (TimeDistri (None, None, 100004) 5100204     lstm_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_25 (TimeDistri (None, None, 100004) 5100204     lstm_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_28 (TimeDistri (None, None, 100004) 5100204     rnn_7[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 20,362,655\n",
      "Trainable params: 20,362,655\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(clipvalue=0.4)\n",
    "lss = sparse_loss\n",
    "\n",
    "model.compile(loss=sparse_loss, optimizer=opt)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=16, write_graph=False, write_grads=False, write_images=False, embeddings_freq=1, embeddings_layer_names=['embedding_6'], embeddings_metadata=None, embeddings_data=None, update_freq='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(folder_path+\"weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20073537 samples, validate on 2230393 samples\n",
      "Epoch 1/1\n",
      "    4192/20073537 [..............................] - ETA: 301:36:11 - loss: 3.9649 - time_distributed_26_loss: 1.6498 - time_distributed_25_loss: 1.7732 - time_distributed_28_loss: 0.5418"
     ]
    }
   ],
   "source": [
    "n = x_en_es_combined.shape[0]\n",
    "model.fit(\n",
    "    [x_en[:n,:-1], x_es[:n,:-1], x_en_es_combined[:n,:-1]],\n",
    "    [x_en[0:n,1:], x_es[:n,1:], x_en_es_combined[:n,1:]], \n",
    "    batch_size=16, \n",
    "    epochs=1, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[tb],\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(folder_path+\"weights\")\n",
    "with open(folder_path+\"model.json\",'w') as f:\n",
    "    f.write(str(model.to_json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_logits(preds, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample an index from a logit vector.\n",
    "\n",
    "    :param preds:\n",
    "    :param temperature:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        return np.argmax(preds)\n",
    "\n",
    "    preds = preds / temperature\n",
    "    preds = preds - logsumexp(preds)\n",
    "\n",
    "    choice = np.random.choice(len(preds), 1, p=np.exp(preds))\n",
    "\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model : Model, seed, size, temperature=1.0):\n",
    "\n",
    "    ls = seed.shape[0]\n",
    "\n",
    "    # Due to the way Keras RNNs work, we feed the model a complete sequence each time. At first it's just the seed,\n",
    "    # zero-padded to the right length. With each iteration we sample and set the next character.\n",
    "\n",
    "    tokens = np.concatenate([seed, np.zeros(size - ls)])\n",
    "    token_combined = [tokens, tokens, tokens]\n",
    "\n",
    "    for i in range(ls, size):\n",
    "\n",
    "        probs_ = model.predict(token_combined)\n",
    "        # Extract the i-th probability vector and sample an index from it\n",
    "        for j, probs in enumerate(probs_):\n",
    "            probs = probs.reshape((1,probs.shape[0],-1))\n",
    "            next_token = sample_logits(probs[0, i-1, :], temperature=temperature)\n",
    "            token_combined[j][i] = next_token\n",
    "\n",
    "    return [tokens.astype('int') for tokens in token_combined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: `logsumexp` is deprecated!\n",
      "Importing `logsumexp` from scipy.misc is deprecated in scipy 1.0.0. Use `scipy.special.logsumexp` instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "seed = x_en[-2][:2]\n",
    "# seed = np.insert(seed, 0, 2)\n",
    "a = generate_seq(model, seed, 50, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanwhile madhu stereotypes <UNK> <UNK> <UNK> <UNK> <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "meanwhile madhu stereotypes <UNK> <UNK> <UNK> <UNK> <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "meanwhile madhu stereotypes <UNK> <UNK> <UNK> <UNK> <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "print(decode(a[0]))\n",
    "print(decode(a[1]))\n",
    "print(decode(a[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## Data Pre-processing Note\n",
    "\n",
    "### Genism\n",
    "https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html uses [WikiCorpus of gensim](https://radimrehurek.com/gensim/corpora/wikicorpus.html). I wasn't able to find a way to preseve line ending and that sucks.\n",
    "\n",
    "### wikiextractor\n",
    "Using a modified version of https://tiefenauer.github.io/blog/wiki-n-gram-lm/. Uses https://github.com/attardi/wikiextractor in its first step followed by a bash and weird script.\n",
    "The script being\n",
    "\n",
    "```\n",
    "result=$(find ./cleaned_wiki/ -name '*bz2' -exec bzcat {} \\+ \\\n",
    "        | pv \\\n",
    "        | tee >(    sed 's/<[^>]*>//g' \\\n",
    "                  | sed 's|[\"'\\''„“‚‘]||g' \\\n",
    "                  | python3 ./wiki_cleaner2.py es >> wiki_es2.txt \\\n",
    "               ) \\\n",
    "        | grep -e \"<doc\" \\\n",
    "        | wc -l);\n",
    "\n",
    "```\n",
    "\n",
    "## news\n",
    "\n",
    "cat news.es.all | ../normalize-punctuation.perl -l es | ../scripts/tokenizer.perl -l es -no-escape -threads 40 > new.es.all.tok\n",
    "cat news*es.shuffled > news.es.all \n",
    "\n",
    "\n",
    "## Making Vocab\n",
    "\n",
    "```\n",
    "from gensim.corpora import WikiCorpus\n",
    "wiki = WikiCorpus('datasets/wiki_dataset/raw/eswiki-latest-pages-articles-multistream.xml.bz2')\n",
    "from gensim.corpora import MmCorpus\n",
    "MmCorpus.serialize('datasets/wiki_dataset/wiki_es.mm', wiki)\n",
    "wiki.dictionary.save('datasets/wiki_dataset/wiki_es.vocab')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
