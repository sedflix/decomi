{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include useful folders\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../vendors/mtl_girnet/data_prep/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable or disable cuda\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "\n",
    "# tokenizer\n",
    "from twokenize import tokenizeRawTweetText as tokenize\n",
    "\n",
    "# for a particular dataset\n",
    "from xml.dom import minidom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trying differnet types of tokenizer\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# from  nltk.stem import SnowballStemmer\n",
    "# from tokensize_deepmoji import tokenize\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "# tokenizer = TweetTokenizer(a)\n",
    "# from tokenizer import tokenizer\n",
    "# T = tokenizer.TweetTokenizer(preserve_handles=False, preserve_hashes=False, preserve_case=False, preserve_url=False, regularize=True)\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = stopwords.words(\"english\")\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "# TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "# def preprocess(text, stem=False):\n",
    "#     # Remove link,user and special characters\n",
    "#     text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "#     tokens = []\n",
    "#     for token in text.split():\n",
    "#         if token not in stop_words: \n",
    "#             if stem:\n",
    "#                 tokens.append(stemmer.stem(token))\n",
    "#             else:\n",
    "#                 tokens.append(token)\n",
    "#     return tokenizer.tokenize(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment140 dataset \n",
    "https://www.kaggle.com/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essential functions/declarations\n",
    "decode_map = {0: -1, 2: 0, 4: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.6 millionss tweets dataset\n",
    "df = pd.read_csv('../data/'+'training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1' , names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is goning to take some time. chill \n",
    "df.target = df.target.apply(lambda x: decode_map[int(x)])\n",
    "df.text = df.text.apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = map( lambda x :{'sentiment': x[0] , 'tokens': x[-1] , } , df.to_numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentiment140 = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English-Spanish Code Mixed Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = {\"N\":-1 , \"P\" :1 , \"NONE\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/cs-corpus-with-tweets_train.txt\", encoding='utf-8').read().split(\"\\n\") \n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': sents[x[1]] , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "en_es_wssa_data_train = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/cs-corpus-with-tweets_test.txt\", encoding='utf-8').read().split(\"\\n\") \n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': sents[x[1]] , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "en_es_wssa_data_test = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_es_wssa_data = list(en_es_wssa_data_train) + list(en_es_wssa_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish Tweet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc = minidom.parse(\"../vendors/mtl_girnet/data_prep/data_cm_senti/general-tweets-train-tagged.xml\")\n",
    "tweets = xmldoc.getElementsByTagName('tweet')\n",
    "\n",
    "sents = {\"N\":-1 , \"P\" :1 , \"NEU\":0 , 'NONE':0 , \"P+\" : 1 , \"N+\":-1 }\n",
    "\n",
    "\n",
    "es_tass1_data = []\n",
    "\n",
    "for i in range( len(tweets)-1) :\n",
    "    if i == 6055:\n",
    "        continue # bad jogar\n",
    "    textt = tweets[i].getElementsByTagName('content')[0].childNodes[0].data\n",
    "    words = tokenize( textt )\n",
    "    sentiment = tweets[i].getElementsByTagName('polarity')[0].getElementsByTagName('value')[0].childNodes[0].data\n",
    "    assert len(tweets[i].getElementsByTagName('polarity')[0].getElementsByTagName('entity'))==0\n",
    "    es_tass1_data.append({'text':textt , 'tokens':words , 'sentiment': sents[sentiment] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some english tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/twitter4242.txt\", \"r\", encoding=\"utf-8\",errors='ignore').read().split(\"\\n\")[1:-1]\n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': int(np.sign(int(x[0])-int(x[1]))) , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "\n",
    "en_twitter_data = list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### es2_twitter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/1600_tweets_dev_complete.txt\", encoding=\"utf-8\").read().split(\"\\n\")[1:-1]\n",
    "data += open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/1600_tweets_test_average_complete.tsv\", encoding=\"utf-8\").read().split(\"\\n\")[1:-2]\n",
    "\n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': int(np.sign(int(x[0])-int(x[1]))) , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "\n",
    "es2_twitter_data = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code-Mixed: en_es_wssa_data: 3062\n",
      "Spanish: es2_twitter_data: 3202\n",
      "Spanish: es_tass1_data: 7217\n",
      "English: en_twitter_data: 4241\n",
      "English: en_sentiment140: 1600000\n"
     ]
    }
   ],
   "source": [
    "print(\"Code-Mixed: en_es_wssa_data: %d\" % len(en_es_wssa_data))\n",
    "print(\"Spanish: es2_twitter_data: %d\" % len(es2_twitter_data))\n",
    "print(\"Spanish: es_tass1_data: %d\" % len(es_tass1_data))\n",
    "print(\"English: en_twitter_data: %d\" % len(en_twitter_data))\n",
    "print(\"English: en_sentiment140: %d\" %len(en_sentiment140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## essential functions\n",
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEED TO RUN MUSE BEFORE THIS and to get this path\n",
    "src_path = '../vendors//MUSE/dumped/debug/4u9hakomha/vectors-en.txt'\n",
    "tgt_path = '../vendors//MUSE/dumped/debug/4u9hakomha/vectors-es.txt'\n",
    "nmax = 100000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n",
    "    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
    "    word2id = {v: k for k, v in src_id2word.items()}\n",
    "    word_emb = src_emb[word2id[word]]\n",
    "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    for i, idx in enumerate(k_best):\n",
    "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"cat\":\n",
      "1.0000 - cat\n",
      "0.7322 - cats\n",
      "0.6453 - kitten\n",
      "0.6381 - dog\n",
      "0.6218 - kittens\n"
     ]
    }
   ],
   "source": [
    "# printing nearest neighbors in the source space\n",
    "src_word = 'cat'\n",
    "get_nn(src_word, src_embeddings, src_id2word, src_embeddings, src_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"cat\":\n",
      "0.6266 - gato\n",
      "0.5317 - perro\n",
      "0.5213 - gatito\n",
      "0.4872 - gorila\n",
      "0.4767 - ratoncito\n"
     ]
    }
   ],
   "source": [
    "# printing nearest neighbors in the target space\n",
    "src_word = 'cat'\n",
    "get_nn(src_word, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 300)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_embeddings(src_embeddings, tgt_embeddings):\n",
    "    \n",
    "    # make combined embedding mattrix\n",
    "    embedding_matrix = src_embeddings.copy().tolist()\n",
    "    embedding_matrix.extend(tgt_embeddings.tolist())\n",
    "    embedding_matrix = np.array(embedding_matrix)\n",
    "    \n",
    "    # make combined id2word and word2id\n",
    "    id2word = src_id2word.copy()\n",
    "    word2id = src_word2id.copy()\n",
    "    \n",
    "    next_id = len(id2word.keys())\n",
    "    counter = len(id2word.keys())\n",
    "    \n",
    "    to_be_removed_id = []\n",
    "    common_words = []\n",
    "    \n",
    "    for key in tgt_id2word:\n",
    "        if tgt_id2word[key] in word2id:\n",
    "            to_be_removed_id.append(counter)\n",
    "            common_words.append(tgt_id2word[key])\n",
    "            embedding_matrix[word2id[tgt_id2word[key]]] =  (embedding_matrix[word2id[tgt_id2word[key]]] + embedding_matrix[counter])/2\n",
    "        else:\n",
    "            id2word[next_id] = tgt_id2word[key]\n",
    "            word2id[tgt_id2word[key]] = next_id\n",
    "            next_id += 1\n",
    "        counter += 1\n",
    "        \n",
    "    embedding_matrix = np.delete(embedding_matrix, to_be_removed_id, axis=0)\n",
    "        \n",
    "    return embedding_matrix, id2word, word2id, common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, id2word, word2id, common_words = merge_embeddings(src_embeddings, tgt_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size:  (161832, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"embedding size: \", str(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common words in both the embedding 38168\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of common words in both the embedding %d\" % len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD UNK\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_not_found = 0\n",
    "def from_datas_to_x_y(list_data, word2id, max_seq_len=20, max_classes=3, seed=0):\n",
    "    stemmer = SnowballStemmer(\"english\")  \n",
    "    words_not_found = 0\n",
    "    def to_x(sample):\n",
    "        global words_not_found\n",
    "        x = []\n",
    "        for word in sample['tokens']:\n",
    "            # lower the word \n",
    "            word = word.lower()\n",
    "            if word in word2id:\n",
    "                x.append(word2id[word]) \n",
    "            else:\n",
    "                stem = stemmer.stem(word) # find stem\n",
    "                if stem in word2id:\n",
    "                    x.append(word2id[stem])\n",
    "                else:\n",
    "                    words_not_found = words_not_found + 1\n",
    "                    pass\n",
    "        return x\n",
    "\n",
    "    def to_x_y(data):\n",
    "        temp =  np.array(list(map(lambda x : [to_x(x), x['sentiment']], data)))\n",
    "        x = list(sequence.pad_sequences(temp[:,0], maxlen=max_seq_len))\n",
    "        y = list(to_categorical(temp[:,1],num_classes=max_classes))\n",
    "        return x, y\n",
    "    \n",
    "    x,y = [],[]\n",
    "    for data in list_data:\n",
    "        x_, y_ = to_x_y(data)\n",
    "        print(\"x: %d \\t y: %d\" % (len(x_),len(y_)))\n",
    "        x.extend(x_)\n",
    "        y.extend(y_)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    x,y = shuffle(x, y, replace=True)\n",
    "    \n",
    "    print(\"Not Found words = %f\" % (float(words_not_found)/(x.shape[0])))\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 3062 \t y: 3062\n",
      "Not Found words = 0.000000\n"
     ]
    }
   ],
   "source": [
    "x_test,y_test = from_datas_to_x_y([en_es_wssa_data],word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    print(\"One-Shot Code Mixed\")\n",
    "    x_test,y_test = from_datas_to_x_y([en_es_wssa_data],word2id)\n",
    "    print(model.evaluate(x_test,y_test,batch_size=128,verbose=0))\n",
    "    \n",
    "    print(\"One-Shot Spanish: 1\")\n",
    "    x_test,y_test = from_datas_to_x_y([es2_twitter_data],word2id)\n",
    "    print(model.evaluate(x_test,y_test,batch_size=128,verbose=0))\n",
    "    \n",
    "    print(\"One-Shot Spanish: 2\")\n",
    "    x_test,y_test = from_datas_to_x_y([es_tass1_data],word2id)\n",
    "    print(model.evaluate(x_test,y_test,batch_size=128,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x.shape)\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding\n",
    "EMBEDDING_DIM = embedding_matrix.shape[1] \n",
    "MAX_NUM_WORDS = embedding_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cnn_lstm\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 1\n",
    "# lstm\n",
    "lstm_output_size = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NUM_WORDS,\n",
    "                     EMBEDDING_DIM,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=MAX_SEQUENCE_LENGTH,\n",
    "                     trainable=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(filters,\n",
    "#                  kernel_size,\n",
    "#                  activation='relu',\n",
    "#                  strides=1))\n",
    "# model.add(MaxPooling1D(pool_size=pool_size))\n",
    "# model.add(LSTM(lstm_output_size))\n",
    "model.add(Bidirectional(LSTM(512, dropout=0.1, recurrent_dropout=0.1)))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 20, 300)           48549600  \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 1024)              3330048   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 3075      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 51,882,723\n",
      "Trainable params: 51,882,723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "earlystop = EarlyStopping(monitor='val_f1', min_delta=0.01, patience=4, \\\n",
    "                          verbose=1, mode='auto', restore_best_weights=True)\n",
    "checkpointer = ModelCheckpoint(filepath='weights.{epoch:02d}-{val_f1:.2f}.hdf5', verbose=1, save_best_only=True, save_weights_only=True, monitor='val_f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 1600000 \t y: 1600000\n",
      "x: 4241 \t y: 4241\n",
      "Not Found words = 0.000000\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = from_datas_to_x_y([en_sentiment140,en_twitter_data],word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 273.9482581967213, 1: 0.6673159957071905, 2: 0.6676417599622448}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_class_weight(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1122968 samples, validate on 481273 samples\n",
      "Epoch 1/10\n",
      "1122968/1122968 [==============================] - 487s 434us/step - loss: 0.4482 - acc: 0.7907 - f1: 0.7901 - val_loss: 0.4164 - val_acc: 0.8100 - val_f1: 0.8100\n",
      "\n",
      "Epoch 00001: val_f1 improved from inf to 0.81003, saving model to weights.01-0.81.hdf5\n",
      "Epoch 2/10\n",
      "1122968/1122968 [==============================] - 483s 430us/step - loss: 0.3958 - acc: 0.8221 - f1: 0.8220 - val_loss: 0.4089 - val_acc: 0.8140 - val_f1: 0.8139\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.81003\n",
      "Epoch 3/10\n",
      "1122968/1122968 [==============================] - 483s 430us/step - loss: 0.3650 - acc: 0.8383 - f1: 0.8382 - val_loss: 0.4058 - val_acc: 0.8175 - val_f1: 0.8174\n",
      "\n",
      "Epoch 00003: val_f1 did not improve from 0.81003\n",
      "Epoch 4/10\n",
      "1122968/1122968 [==============================] - 483s 430us/step - loss: 0.3314 - acc: 0.8551 - f1: 0.8551 - val_loss: 0.4262 - val_acc: 0.8154 - val_f1: 0.8154\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.81003\n",
      "Epoch 5/10\n",
      "1122968/1122968 [==============================] - 483s 430us/step - loss: 0.2961 - acc: 0.8719 - f1: 0.8719 - val_loss: 0.4550 - val_acc: 0.8131 - val_f1: 0.8131\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.81003\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit( x_train , y_train, epochs=10, batch_size=648, validation_split=0.3, shuffle=True, callbacks=[earlystop,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Shot Code Mixed\n",
      "x: 3062 \t y: 3062\n",
      "Not Found words = 0.000000\n",
      "[3.7075264187594166, 0.40855649919544146, 0.40830850908757815]\n",
      "One-Shot Spanish: 1\n",
      "x: 3202 \t y: 3202\n",
      "Not Found words = 0.000000\n",
      "[2.3992607895654863, 0.4216114928169894, 0.4099066541911214]\n",
      "One-Shot Spanish: 2\n",
      "x: 7217 \t y: 7217\n",
      "Not Found words = 0.000000\n",
      "[2.1086025899576812, 0.4521269225068282, 0.4442575112577784]\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 7217 \t y: 7217\n",
      "x: 3202 \t y: 3202\n",
      "Not Found words = 0.000000\n"
     ]
    }
   ],
   "source": [
    "x_train_es, y_train_es =  from_datas_to_x_y([es_tass1_data, es2_twitter_data], word2id=word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0385765550239234, 1: 0.8748110831234257, 2: 1.1185185185185185}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_class_weight(y_train_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9377 samples, validate on 1042 samples\n",
      "Epoch 1/10\n",
      "9377/9377 [==============================] - 10s 1ms/step - loss: 0.9260 - acc: 0.5711 - f1: 0.5076 - val_loss: 0.8132 - val_acc: 0.6305 - val_f1: 0.6080\n",
      "\n",
      "Epoch 00001: val_f1 improved from 0.81003 to 0.60796, saving model to weights.01-0.61.hdf5\n",
      "Epoch 2/10\n",
      "9377/9377 [==============================] - 10s 1ms/step - loss: 0.5668 - acc: 0.7657 - f1: 0.7533 - val_loss: 0.8871 - val_acc: 0.6286 - val_f1: 0.6157\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.60796\n",
      "Epoch 3/10\n",
      "9377/9377 [==============================] - 10s 1ms/step - loss: 0.3149 - acc: 0.8781 - f1: 0.8781 - val_loss: 1.1300 - val_acc: 0.6305 - val_f1: 0.6276\n",
      "\n",
      "Epoch 00003: val_f1 did not improve from 0.60796\n",
      "Epoch 4/10\n",
      "9377/9377 [==============================] - 11s 1ms/step - loss: 0.1661 - acc: 0.9395 - f1: 0.9401 - val_loss: 1.4100 - val_acc: 0.6209 - val_f1: 0.6195\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.60796\n",
      "Epoch 5/10\n",
      "9377/9377 [==============================] - 11s 1ms/step - loss: 0.0892 - acc: 0.9661 - f1: 0.9660 - val_loss: 1.7211 - val_acc: 0.6075 - val_f1: 0.6042\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.60796 to 0.60421, saving model to weights.05-0.60.hdf5\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_es, y_train_es, epochs=10, batch_size=128, validation_split=0.1, shuffle=True, callbacks=[earlystop,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Shot Code Mixed\n",
      "x: 3062 \t y: 3062\n",
      "Not Found words = 0.000000\n",
      "[1.0251387802313388, 0.47028086239570577, 0.4191458797158952]\n",
      "One-Shot Spanish: 1\n",
      "x: 3202 \t y: 3202\n",
      "Not Found words = 0.000000\n",
      "[0.7380538467762249, 0.6714553404122423, 0.6372328620266721]\n",
      "One-Shot Spanish: 2\n",
      "x: 7217 \t y: 7217\n",
      "Not Found words = 0.000000\n",
      "[0.6001646815285188, 0.754745739069906, 0.7380338265110585]\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both English and Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 4241 \t y: 4241\n",
      "x: 3202 \t y: 3202\n",
      "x: 7217 \t y: 7217\n",
      "Not Found words = 0.000000\n"
     ]
    }
   ],
   "source": [
    "x_train_both, y_train_both = from_datas_to_x_y([en_twitter_data,es2_twitter_data,es_tass1_data],word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14660 samples, validate on 3062 samples\n",
      "Epoch 1/10\n",
      "14660/14660 [==============================] - 16s 1ms/step - loss: 0.6653 - acc: 0.7113 - f1: 0.6953 - val_loss: 0.9687 - val_acc: 0.5411 - val_f1: 0.4765\n",
      "\n",
      "Epoch 00001: val_f1 improved from 0.60421 to 0.47652, saving model to weights.01-0.48.hdf5\n",
      "Epoch 2/10\n",
      "14660/14660 [==============================] - 17s 1ms/step - loss: 0.4512 - acc: 0.8194 - f1: 0.8162 - val_loss: 1.0669 - val_acc: 0.5346 - val_f1: 0.5106\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.47652\n",
      "Epoch 3/10\n",
      "14660/14660 [==============================] - 17s 1ms/step - loss: 0.2734 - acc: 0.8982 - f1: 0.8962 - val_loss: 1.4242 - val_acc: 0.4951 - val_f1: 0.4859\n",
      "\n",
      "Epoch 00003: val_f1 did not improve from 0.47652\n",
      "Epoch 4/10\n",
      "14660/14660 [==============================] - 17s 1ms/step - loss: 0.1511 - acc: 0.9452 - f1: 0.9450 - val_loss: 1.6123 - val_acc: 0.5124 - val_f1: 0.5096\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.47652\n",
      "Epoch 5/10\n",
      "14660/14660 [==============================] - 17s 1ms/step - loss: 0.0902 - acc: 0.9677 - f1: 0.9682 - val_loss: 2.0645 - val_acc: 0.4951 - val_f1: 0.4882\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.47652\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f30e3b49f60>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_both,y_train_both,validation_data=(x_test,y_test),batch_size=128, epochs=10, shuffle=True, callbacks=[earlystop,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Shot Code Mixed\n",
      "x: 3062 \t y: 3062\n",
      "Not Found words = 0.000000\n",
      "[0.9686988688309468, 0.5411495764141843, 0.47658517440886844]\n",
      "One-Shot Spanish: 1\n",
      "x: 3202 \t y: 3202\n",
      "Not Found words = 0.000000\n",
      "[0.5271762280744139, 0.7960649594003748, 0.7858611731734744]\n",
      "One-Shot Spanish: 2\n",
      "x: 7217 \t y: 7217\n",
      "Not Found words = 0.000000\n",
      "[0.3729386384614009, 0.8654565610217638, 0.8553500539199028]\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Lingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        Only computes a batch-wise average of recall.\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "def get_class_weight(y):\n",
    "    \"\"\"\n",
    "    Used from: https://stackoverflow.com/a/50695814\n",
    "    TODO: check validity and 'balanced' option\n",
    "    :param y: A list of one-hot-encoding labels [[0,0,1,0],[0,0,0,1],..]\n",
    "    :return: class-weights to be used by keras model.fit(.. class_weight=\"\") -> {0:0.52134, 1:1.adas..}\n",
    "    \"\"\"\n",
    "    y_integers = np.argmax(y, axis=1)\n",
    "    class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "    return d_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlblocks import text\n",
    "from dlblocks.pyutils import mapArrays , loadJson , saveJson , selectKeys , oneHotVec , padList\n",
    "from dlblocks.pyutils import int64Arr , floatArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = text.Vocabulary()\n",
    "\n",
    "for d in es_tass1_data + en_es_wssa_data + en_twitter_data + es2_twitter_data :\n",
    "    vocab.add_words( d['tokens']  )\n",
    "\n",
    "    \n",
    "vocab.keepTopK(25000)\n",
    "\n",
    "\n",
    "\n",
    "maxSentenceL = 150\n",
    "\n",
    "def vecc( d ):\n",
    "    ret = {}\n",
    "    words   = d['tokens']\n",
    "    wordids = map( vocab , words )\n",
    "    ret['sentence'] = int64Arr( padList( wordids , maxSentenceL , 0 , 'left') )\n",
    "    ret['sentiment_val'] =  floatArr( d['sentiment'] )\n",
    "    ret['sentiment_id'] =  int64Arr( d['sentiment'] + 1 )\n",
    "    ret['sentiment_onehot'] =  floatArr( oneHotVec( d['sentiment']+1 , 3  ) )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "en_es_wssa_data_train_arr = mapArrays( en_es_wssa_data_train , vecc )\n",
    "en_es_wssa_data_test_arr = mapArrays( en_es_wssa_data_test , vecc )\n",
    "\n",
    "en_twitter_data_train_arr = mapArrays( en_twitter_data , vecc )\n",
    "es_tass1_datatrain_arr = mapArrays( es_tass1_data , vecc )\n",
    "\n",
    "datasets = {\"en_es_wssa_data_train_arr\":en_es_wssa_data_train_arr ,\n",
    "           \"en_es_wssa_data_test_arr\":en_es_wssa_data_test_arr ,\n",
    "           \"en_twitter_data_train_arr\":en_twitter_data_train_arr ,\n",
    "           \"es_tass1_datatrain_arr\": es_tass1_datatrain_arr }\n",
    "\n",
    "\n",
    "\n",
    "outFNN = \"../data/senti_prepped.h5\"\n",
    "\n",
    "f = h5py.File(outFNN , \"w\")\n",
    "for kk in datasets.keys():\n",
    "    f.create_group( kk  )\n",
    "    for k in datasets[kk].keys():\n",
    "        f[ kk ].create_dataset( k , data=datasets[kk][k] )\n",
    "\n",
    "print \"HDF5 file created !\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
