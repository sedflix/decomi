{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include useful folders\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../vendors/mtl_girnet/data_prep/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable or disable cuda\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "\n",
    "# tokenizer\n",
    "from twokenize import tokenizeRawTweetText as tokenize\n",
    "\n",
    "# for a particular dataset\n",
    "from xml.dom import minidom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trying differnet types of tokenizer\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# from  nltk.stem import SnowballStemmer\n",
    "# from tokensize_deepmoji import tokenize\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "# tokenizer = TweetTokenizer(a)\n",
    "# from tokenizer import tokenizer\n",
    "# T = tokenizer.TweetTokenizer(preserve_handles=False, preserve_hashes=False, preserve_case=False, preserve_url=False, regularize=True)\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = stopwords.words(\"english\")\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "# TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "# def preprocess(text, stem=False):\n",
    "#     # Remove link,user and special characters\n",
    "#     text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "#     tokens = []\n",
    "#     for token in text.split():\n",
    "#         if token not in stop_words: \n",
    "#             if stem:\n",
    "#                 tokens.append(stemmer.stem(token))\n",
    "#             else:\n",
    "#                 tokens.append(token)\n",
    "#     return tokenizer.tokenize(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment140 dataset \n",
    "https://www.kaggle.com/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essential functions/declarations\n",
    "decode_map = {0: -1, 2: 0, 4: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.6 millionss tweets dataset\n",
    "df = pd.read_csv('../data/'+'training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1' , names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is goning to take some time. chill \n",
    "df.target = df.target.apply(lambda x: decode_map[int(x)])\n",
    "df.text = df.text.apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = map( lambda x :{'sentiment': x[0] , 'tokens': x[-1] , } , df.to_numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentiment140 = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English-Spanish Code Mixed Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = {\"N\":-1 , \"P\" :1 , \"NONE\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/cs-corpus-with-tweets_train.txt\", encoding='utf-8').read().split(\"\\n\") \n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': sents[x[1]] , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "en_es_wssa_data_train = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/cs-corpus-with-tweets_test.txt\", encoding='utf-8').read().split(\"\\n\") \n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': sents[x[1]] , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "en_es_wssa_data_test = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_es_wssa_data = list(en_es_wssa_data_train) + list(en_es_wssa_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish Tweet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc = minidom.parse(\"../vendors/mtl_girnet/data_prep/data_cm_senti/general-tweets-train-tagged.xml\")\n",
    "tweets = xmldoc.getElementsByTagName('tweet')\n",
    "\n",
    "sents = {\"N\":-1 , \"P\" :1 , \"NEU\":0 , 'NONE':0 , \"P+\" : 1 , \"N+\":-1 }\n",
    "\n",
    "\n",
    "es_tass1_data = []\n",
    "\n",
    "for i in range( len(tweets)-1) :\n",
    "    if i == 6055:\n",
    "        continue # bad jogar\n",
    "    textt = tweets[i].getElementsByTagName('content')[0].childNodes[0].data\n",
    "    words = tokenize( textt )\n",
    "    sentiment = tweets[i].getElementsByTagName('polarity')[0].getElementsByTagName('value')[0].childNodes[0].data\n",
    "    assert len(tweets[i].getElementsByTagName('polarity')[0].getElementsByTagName('entity'))==0\n",
    "    es_tass1_data.append({'text':textt , 'tokens':words , 'sentiment': sents[sentiment] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some english tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/twitter4242.txt\", \"r\", encoding=\"utf-8\",errors='ignore').read().split(\"\\n\")[1:-1]\n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': int(np.sign(int(x[0])-int(x[1]))) , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "\n",
    "en_twitter_data = list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### es2_twitter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/1600_tweets_dev_complete.txt\", encoding=\"utf-8\").read().split(\"\\n\")[1:-1]\n",
    "data += open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/1600_tweets_test_average_complete.tsv\", encoding=\"utf-8\").read().split(\"\\n\")[1:-2]\n",
    "\n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': int(np.sign(int(x[0])-int(x[1]))) , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "\n",
    "es2_twitter_data = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code-Mixed: en_es_wssa_data: 3062\n",
      "Spanish: es2_twitter_data: 3202\n",
      "Spanish: es_tass1_data: 7217\n",
      "English: en_twitter_data: 4241\n",
      "English: en_sentiment140: 1600000\n"
     ]
    }
   ],
   "source": [
    "print(\"Code-Mixed: en_es_wssa_data: %d\" % len(en_es_wssa_data))\n",
    "print(\"Spanish: es2_twitter_data: %d\" % len(es2_twitter_data))\n",
    "print(\"Spanish: es_tass1_data: %d\" % len(es_tass1_data))\n",
    "print(\"English: en_twitter_data: %d\" % len(en_twitter_data))\n",
    "print(\"English: en_sentiment140: %d\" %len(en_sentiment140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For LASER: To get embeddings for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sentences(data, name):\n",
    "    with open(name,\"w\") as f:\n",
    "        temp = \"\"\n",
    "        for row in data:\n",
    "            temp = temp + str(row['text']).strip() + \"\\n\"\n",
    "        f.write(temp)\n",
    "\n",
    "def write_sentences_sep(data, name):\n",
    "    with open(name,\"w\") as f:\n",
    "        temp = []\n",
    "        for row in data:\n",
    "            temp.append(str(\" \".join(row['tokens'])).strip() + \"\\n\")\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_sentences(en_es_wssa_data,\"en_es.txt\")\n",
    "write_sentences(es2_twitter_data,\"es2_twitter.txt\")\n",
    "write_sentences(es_tass1_data,\"es_tass1.txt\")\n",
    "write_sentences(en_twitter_data, \"en_twiter.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRLEASE DON'T EXECUTE THIS MOTHERFUCKER\n",
    "lines_ = write_sentences_sep(en_sentiment140, \"en_sentiment140.txt\")\n",
    "with open(\"en_sentiment140.txt\", \"w\") as f:\n",
    "    for line in lines_:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(data):\n",
    "    from keras.utils import to_categorical\n",
    "    y = []\n",
    "    for row in data:\n",
    "        y.append(int(row['sentiment']))\n",
    "    y = to_categorical(y,num_classes=3)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_es_y =  get_y(en_es_wssa_data)\n",
    "es_twitter_y = get_y(es2_twitter_data)\n",
    "es_tass_y = get_y(es_tass1_data)\n",
    "en_twitter_y = get_y(en_twitter_data)\n",
    "en_sentiment140_y = get_y(en_sentiment140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For LASER: Load Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file):\n",
    "    dim = 1024\n",
    "    X = np.fromfile(file, dtype=np.float32, count=-1)                                                                          \n",
    "    X.resize(X.shape[0] // dim, dim) \n",
    "    print(file + \": \" + str(X.shape))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_es.raw: (3062, 1024)\n",
      "es2_twitter.raw: (3202, 1024)\n",
      "es_tass.taw: (7219, 1024)\n",
      "en_twiter.raw: (4241, 1024)\n",
      "en_sentiment140.raw: (1600000, 1024)\n"
     ]
    }
   ],
   "source": [
    "en_es_emb = load(\"en_es.raw\")\n",
    "es_twitter_emb =load(\"es2_twitter.raw\")\n",
    "es_tass_emb =load(\"es_tass.taw\")\n",
    "en_twitter_emb =load(\"en_twiter.raw\")\n",
    "en_sentiment140_emb = load(\"en_sentiment140.raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7217, 3)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_tass_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For LASER: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(1024,)))\n",
    "# # model.add(Dropout(0.4))\n",
    "# model.add(Dense(512))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3 , activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 526,339\n",
      "Trainable params: 526,339\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 960000 samples, validate on 640000 samples\n",
      "Epoch 1/15\n",
      "268032/960000 [=======>......................] - ETA: 52s - loss: 0.3325 - acc: 0.8641 - f1: 0.8636"
     ]
    }
   ],
   "source": [
    "model.fit(en_sentiment140_emb, en_sentiment140, batch_size=128, epochs=15, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3816 samples, validate on 425 samples\n",
      "Epoch 1/15\n",
      "3816/3816 [==============================] - 1s 287us/step - loss: 0.7539 - acc: 0.7078 - f1: 0.7012 - val_loss: 0.8980 - val_acc: 0.6447 - val_f1: 0.6431\n",
      "Epoch 2/15\n",
      "3816/3816 [==============================] - 1s 290us/step - loss: 0.5397 - acc: 0.7707 - f1: 0.7691 - val_loss: 1.0082 - val_acc: 0.6071 - val_f1: 0.6129\n",
      "Epoch 3/15\n",
      "3816/3816 [==============================] - 1s 303us/step - loss: 0.5219 - acc: 0.7849 - f1: 0.7799 - val_loss: 0.9984 - val_acc: 0.6165 - val_f1: 0.6155\n",
      "Epoch 4/15\n",
      "3816/3816 [==============================] - 1s 307us/step - loss: 0.5238 - acc: 0.7786 - f1: 0.7744 - val_loss: 1.1235 - val_acc: 0.5906 - val_f1: 0.6002\n",
      "Epoch 5/15\n",
      "3816/3816 [==============================] - 1s 334us/step - loss: 0.5151 - acc: 0.7830 - f1: 0.7802 - val_loss: 1.0336 - val_acc: 0.6259 - val_f1: 0.6200\n",
      "Epoch 6/15\n",
      "3816/3816 [==============================] - 1s 317us/step - loss: 0.5144 - acc: 0.7911 - f1: 0.7862 - val_loss: 1.0563 - val_acc: 0.6024 - val_f1: 0.6080\n",
      "Epoch 7/15\n",
      "3816/3816 [==============================] - 1s 328us/step - loss: 0.5235 - acc: 0.7796 - f1: 0.7782 - val_loss: 1.0415 - val_acc: 0.6306 - val_f1: 0.6269\n",
      "Epoch 8/15\n",
      "3816/3816 [==============================] - 1s 336us/step - loss: 0.5227 - acc: 0.7906 - f1: 0.7821 - val_loss: 1.0659 - val_acc: 0.5976 - val_f1: 0.5990\n",
      "Epoch 9/15\n",
      "3816/3816 [==============================] - 1s 333us/step - loss: 0.5123 - acc: 0.7909 - f1: 0.7888 - val_loss: 1.0080 - val_acc: 0.6400 - val_f1: 0.6411\n",
      "Epoch 10/15\n",
      "3816/3816 [==============================] - 1s 332us/step - loss: 0.5047 - acc: 0.7898 - f1: 0.7891 - val_loss: 1.0956 - val_acc: 0.5929 - val_f1: 0.5995\n",
      "Epoch 11/15\n",
      "3816/3816 [==============================] - 1s 322us/step - loss: 0.5075 - acc: 0.7890 - f1: 0.7885 - val_loss: 1.0646 - val_acc: 0.6024 - val_f1: 0.6052\n",
      "Epoch 12/15\n",
      "3816/3816 [==============================] - 1s 322us/step - loss: 0.4964 - acc: 0.7909 - f1: 0.7882 - val_loss: 1.1390 - val_acc: 0.5882 - val_f1: 0.5879\n",
      "Epoch 13/15\n",
      "3816/3816 [==============================] - 1s 327us/step - loss: 0.5073 - acc: 0.7877 - f1: 0.7845 - val_loss: 1.0464 - val_acc: 0.6141 - val_f1: 0.6221\n",
      "Epoch 14/15\n",
      "3816/3816 [==============================] - 1s 337us/step - loss: 0.5072 - acc: 0.7914 - f1: 0.7876 - val_loss: 1.1591 - val_acc: 0.5859 - val_f1: 0.5787\n",
      "Epoch 15/15\n",
      "3816/3816 [==============================] - 1s 319us/step - loss: 0.5004 - acc: 0.7914 - f1: 0.7886 - val_loss: 1.1219 - val_acc: 0.5906 - val_f1: 0.5924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5c24a26a20>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(en_twitter_emb, en_twitter_y, batch_size=32, epochs=15, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3202/3202 [==============================] - 0s 78us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4272800997448742, 0.5137414116177389, 0.506156767367721]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(es_twitter_emb, es_twitter_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2881 samples, validate on 321 samples\n",
      "Epoch 1/20\n",
      "2881/2881 [==============================] - 1s 317us/step - loss: 0.5167 - acc: 0.7921 - f1: 0.7843 - val_loss: 1.3556 - val_acc: 0.5016 - val_f1: 0.4973\n",
      "Epoch 2/20\n",
      "2881/2881 [==============================] - 1s 318us/step - loss: 0.5185 - acc: 0.7904 - f1: 0.7851 - val_loss: 1.3300 - val_acc: 0.5452 - val_f1: 0.5385\n",
      "Epoch 3/20\n",
      "2881/2881 [==============================] - 1s 332us/step - loss: 0.5101 - acc: 0.7886 - f1: 0.7847 - val_loss: 1.2849 - val_acc: 0.5483 - val_f1: 0.5510\n",
      "Epoch 4/20\n",
      "2881/2881 [==============================] - 1s 317us/step - loss: 0.4923 - acc: 0.8028 - f1: 0.8028 - val_loss: 1.2878 - val_acc: 0.5389 - val_f1: 0.5429\n",
      "Epoch 5/20\n",
      "2881/2881 [==============================] - 1s 315us/step - loss: 0.5024 - acc: 0.7886 - f1: 0.7822 - val_loss: 1.2795 - val_acc: 0.5701 - val_f1: 0.5655\n",
      "Epoch 6/20\n",
      "2881/2881 [==============================] - 1s 307us/step - loss: 0.4831 - acc: 0.7990 - f1: 0.7971 - val_loss: 1.3566 - val_acc: 0.5514 - val_f1: 0.5566\n",
      "Epoch 7/20\n",
      "2881/2881 [==============================] - 1s 287us/step - loss: 0.5118 - acc: 0.7876 - f1: 0.7828 - val_loss: 1.2941 - val_acc: 0.5576 - val_f1: 0.5404\n",
      "Epoch 8/20\n",
      "2881/2881 [==============================] - 1s 300us/step - loss: 0.4808 - acc: 0.8070 - f1: 0.8037 - val_loss: 1.3881 - val_acc: 0.5327 - val_f1: 0.5297\n",
      "Epoch 9/20\n",
      "2881/2881 [==============================] - 1s 301us/step - loss: 0.4823 - acc: 0.8042 - f1: 0.8013 - val_loss: 1.3924 - val_acc: 0.5234 - val_f1: 0.5188\n",
      "Epoch 10/20\n",
      "2881/2881 [==============================] - 1s 291us/step - loss: 0.4866 - acc: 0.7997 - f1: 0.7979 - val_loss: 1.3535 - val_acc: 0.5327 - val_f1: 0.5281\n",
      "Epoch 11/20\n",
      "2881/2881 [==============================] - 1s 276us/step - loss: 0.4769 - acc: 0.8094 - f1: 0.8035 - val_loss: 1.3769 - val_acc: 0.5452 - val_f1: 0.5389\n",
      "Epoch 12/20\n",
      "2881/2881 [==============================] - 1s 301us/step - loss: 0.4763 - acc: 0.8077 - f1: 0.8036 - val_loss: 1.4521 - val_acc: 0.5296 - val_f1: 0.5168\n",
      "Epoch 13/20\n",
      "2881/2881 [==============================] - 1s 298us/step - loss: 0.4883 - acc: 0.8025 - f1: 0.7993 - val_loss: 1.4057 - val_acc: 0.5670 - val_f1: 0.5573\n",
      "Epoch 14/20\n",
      "2881/2881 [==============================] - 1s 313us/step - loss: 0.4656 - acc: 0.8129 - f1: 0.8093 - val_loss: 1.6331 - val_acc: 0.5047 - val_f1: 0.4971\n",
      "Epoch 15/20\n",
      "2881/2881 [==============================] - 1s 318us/step - loss: 0.4980 - acc: 0.7886 - f1: 0.7866 - val_loss: 1.3351 - val_acc: 0.5545 - val_f1: 0.5420\n",
      "Epoch 16/20\n",
      "2881/2881 [==============================] - 1s 318us/step - loss: 0.4648 - acc: 0.8049 - f1: 0.8015 - val_loss: 1.5587 - val_acc: 0.5047 - val_f1: 0.5007\n",
      "Epoch 17/20\n",
      "2881/2881 [==============================] - 1s 310us/step - loss: 0.4620 - acc: 0.8112 - f1: 0.8115 - val_loss: 1.3756 - val_acc: 0.5452 - val_f1: 0.5414\n",
      "Epoch 18/20\n",
      "2881/2881 [==============================] - 1s 300us/step - loss: 0.4691 - acc: 0.8112 - f1: 0.8068 - val_loss: 1.4557 - val_acc: 0.5358 - val_f1: 0.5280\n",
      "Epoch 19/20\n",
      "2881/2881 [==============================] - 1s 301us/step - loss: 0.4634 - acc: 0.8018 - f1: 0.8007 - val_loss: 1.4178 - val_acc: 0.5452 - val_f1: 0.5462\n",
      "Epoch 20/20\n",
      "2881/2881 [==============================] - 1s 306us/step - loss: 0.4573 - acc: 0.8108 - f1: 0.8096 - val_loss: 1.5550 - val_acc: 0.4984 - val_f1: 0.4946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5c24a25cc0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(es_twitter_emb, es_twitter_y, batch_size=32, epochs=20, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3062/3062 [==============================] - 0s 73us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9322782474588833, 0.4866100587072441, 0.4831954719271246]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(en_es_emb,en_es_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## essential functions\n",
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEED TO RUN MUSE BEFORE THIS and to get this path\n",
    "src_path = '../vendors//MUSE/dumped/debug/4u9hakomha/vectors-en.txt'\n",
    "tgt_path = '../vendors//MUSE/dumped/debug/4u9hakomha/vectors-es.txt'\n",
    "nmax = 100000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n",
    "    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
    "    word2id = {v: k for k, v in src_id2word.items()}\n",
    "    word_emb = src_emb[word2id[word]]\n",
    "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    for i, idx in enumerate(k_best):\n",
    "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"cat\":\n",
      "1.0000 - cat\n",
      "0.7322 - cats\n",
      "0.6453 - kitten\n",
      "0.6381 - dog\n",
      "0.6218 - kittens\n"
     ]
    }
   ],
   "source": [
    "# printing nearest neighbors in the source space\n",
    "src_word = 'cat'\n",
    "get_nn(src_word, src_embeddings, src_id2word, src_embeddings, src_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"cat\":\n",
      "0.6266 - gato\n",
      "0.5317 - perro\n",
      "0.5213 - gatito\n",
      "0.4872 - gorila\n",
      "0.4767 - ratoncito\n"
     ]
    }
   ],
   "source": [
    "# printing nearest neighbors in the target space\n",
    "src_word = 'cat'\n",
    "get_nn(src_word, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_embeddings(src_embeddings, tgt_embeddings):\n",
    "    \n",
    "    # make combined embedding mattrix\n",
    "    embedding_matrix = src_embeddings.copy().tolist()\n",
    "    embedding_matrix.extend(tgt_embeddings.tolist())\n",
    "    embedding_matrix = np.array(embedding_matrix)\n",
    "    \n",
    "    # make combined id2word and word2id\n",
    "    id2word = src_id2word.copy()\n",
    "    word2id = src_word2id.copy()\n",
    "    \n",
    "    next_id = len(id2word.keys())\n",
    "    counter = len(id2word.keys())\n",
    "    \n",
    "    to_be_removed_id = []\n",
    "    common_words = []\n",
    "    \n",
    "    for key in tgt_id2word:\n",
    "        if tgt_id2word[key] in word2id:\n",
    "            to_be_removed_id.append(counter)\n",
    "            common_words.append(tgt_id2word[key])\n",
    "            embedding_matrix[word2id[tgt_id2word[key]]] =  (embedding_matrix[word2id[tgt_id2word[key]]] + embedding_matrix[counter])/2\n",
    "        else:\n",
    "            id2word[next_id] = tgt_id2word[key]\n",
    "            word2id[tgt_id2word[key]] = next_id\n",
    "            next_id += 1\n",
    "        counter += 1\n",
    "        \n",
    "    embedding_matrix = np.delete(embedding_matrix, to_be_removed_id, axis=0)\n",
    "        \n",
    "    return embedding_matrix, id2word, word2id, common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, id2word, word2id, common_words = merge_embeddings(src_embeddings, tgt_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size:  (161832, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"embedding size: \", str(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common words in both the embedding 38168\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of common words in both the embedding %d\" % len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD UNK\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_not_found = 0\n",
    "def from_datas_to_x_y(list_data, word2id, max_seq_len=20, max_classes=3, seed=0):\n",
    "    stemmer = SnowballStemmer(\"english\")  \n",
    "    words_not_found = 0\n",
    "    def to_x(sample):\n",
    "        global words_not_found\n",
    "        x = []\n",
    "        for word in sample['tokens']:\n",
    "            # lower the word \n",
    "            word = word.lower()\n",
    "            if word in word2id:\n",
    "                x.append(word2id[word]) \n",
    "            else:\n",
    "                stem = stemmer.stem(word) # find stem\n",
    "                if stem in word2id:\n",
    "                    x.append(word2id[stem])\n",
    "                else:\n",
    "                    words_not_found = words_not_found + 1\n",
    "                    pass\n",
    "        return x\n",
    "\n",
    "    def to_x_y(data):\n",
    "        temp =  np.array(list(map(lambda x : [to_x(x), x['sentiment']], data)))\n",
    "        x = list(sequence.pad_sequences(temp[:,0], maxlen=max_seq_len))\n",
    "        y = list(to_categorical(temp[:,1],num_classes=max_classes))\n",
    "        return x, y\n",
    "    \n",
    "    x,y = [],[]\n",
    "    for data in list_data:\n",
    "        x_, y_ = to_x_y(data)\n",
    "        print(\"x: %d \\t y: %d\" % (len(x_),len(y_)))\n",
    "        x.extend(x_)\n",
    "        y.extend(y_)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    x,y = shuffle(x, y, replace=True)\n",
    "    \n",
    "    print(\"Not Found words = %f\" % (float(words_not_found)/(x.shape[0])))\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test,y_test = from_datas_to_x_y([en_es_wssa_data],word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one(model, x, y):\n",
    "    y_pred = model.predict(x,verbose=0)\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(np.argmax(y, axis=1), np.argmax(y_pred,axis=1)))\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(np.argmax(y, axis=1), np.argmax(y_pred,axis=1)))\n",
    "def evaluate(model):\n",
    "    print(\"Code Mixed: \")\n",
    "    x_test,y_test = from_datas_to_x_y([en_es_wssa_data],word2id)\n",
    "    evaluate_one(model, x_test, y_test)\n",
    "    \n",
    "    print(\"\\nEnglish: \")\n",
    "    x_test,y_test = from_datas_to_x_y([en_twitter_data],word2id)\n",
    "    evaluate_one(model, x_test, y_test)\n",
    "    \n",
    "    print(\"\\nSpanish: 1\")\n",
    "    x_test,y_test = from_datas_to_x_y([es2_twitter_data],word2id)\n",
    "    evaluate_one(model, x_test, y_test)\n",
    "    \n",
    "    print(\"\\nSpanish: 2\")\n",
    "    x_test,y_test = from_datas_to_x_y([es_tass1_data],word2id)\n",
    "    evaluate_one(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x.shape)\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding\n",
    "EMBEDDING_DIM = embedding_matrix.shape[1] \n",
    "MAX_NUM_WORDS = embedding_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cnn_lstm\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 1\n",
    "# lstm\n",
    "lstm_output_size = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NUM_WORDS,\n",
    "                     EMBEDDING_DIM,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=MAX_SEQUENCE_LENGTH,\n",
    "                     trainable=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(filters,\n",
    "#                  kernel_size,\n",
    "#                  activation='relu',\n",
    "#                  strides=1))\n",
    "# model.add(MaxPooling1D(pool_size=pool_size))\n",
    "# model.add(LSTM(lstm_output_size))\n",
    "model.add(Bidirectional(LSTM(512, dropout=0.1, recurrent_dropout=0.1)))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 20, 300)           48549600  \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 1024)              3330048   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 3075      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 51,882,723\n",
      "Trainable params: 51,882,723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "early_stop = EarlyStopping(monitor='val_f1', patience=4, restore_best_weights=True, verbose=5)\n",
    "checkpointer = ModelCheckpoint(filepath='weights.{epoch:02d}-{val_f1:.2f}.hdf5', verbose=1, save_best_only=True, save_weights_only=True, monitor='val_f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 1600000 \t y: 1600000\n",
      "x: 4241 \t y: 4241\n",
      "Not Found words = 0.000000\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = from_datas_to_x_y([en_sentiment140,en_twitter_data],word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 273.9482581967213, 1: 0.6673159957071905, 2: 0.6676417599622448}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_class_weight(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1122968 samples, validate on 481273 samples\n",
      "Epoch 1/5\n",
      "1122968/1122968 [==============================] - 687s 612us/step - loss: 0.2604 - acc: 0.8884 - f1: 0.8883 - val_loss: 0.5404 - val_acc: 0.8080 - val_f1: 0.8080\n",
      "Epoch 2/5\n",
      " 205416/1122968 [====>.........................] - ETA: 8:44 - loss: 0.2025 - acc: 0.9153 - f1: 0.9153"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-87816860087b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m648\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train , y_train, \n",
    "                    epochs=5, \n",
    "                    batch_size=648,\n",
    "                    validation_split=0.3, \n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Mixed: \n",
      "x: 3062 \t y: 3062\n",
      "Not Found words = 0.000000\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      1313\n",
      "           1       0.41      0.69      0.52       963\n",
      "           2       0.37      0.68      0.48       786\n",
      "\n",
      "   micro avg       0.39      0.39      0.39      3062\n",
      "   macro avg       0.59      0.46      0.33      3062\n",
      "weighted avg       0.65      0.39      0.29      3062\n",
      "\n",
      "Confusion Matrix\n",
      "[[  3 687 623]\n",
      " [  0 660 303]\n",
      " [  0 252 534]]\n",
      "\n",
      "English: \n",
      "x: 4241 \t y: 4241\n",
      "Not Found words = 0.000000\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.55      0.71      1952\n",
      "           1       0.65      0.96      0.78      1340\n",
      "           2       0.75      0.94      0.83       949\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      4241\n",
      "   macro avg       0.80      0.82      0.77      4241\n",
      "weighted avg       0.83      0.77      0.76      4241\n",
      "\n",
      "Confusion Matrix\n",
      "[[1080  625  247]\n",
      " [   0 1288   52]\n",
      " [   2   57  890]]\n",
      "\n",
      "Spanish: 1\n",
      "x: 3202 \t y: 3202\n",
      "Not Found words = 0.000000\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.28      0.44      1192\n",
      "           1       0.66      0.87      0.75      1087\n",
      "           2       0.62      0.95      0.75       923\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      3202\n",
      "   macro avg       0.75      0.70      0.65      3202\n",
      "weighted avg       0.77      0.67      0.63      3202\n",
      "\n",
      "Confusion Matrix\n",
      "[[337 448 407]\n",
      " [  6 949 132]\n",
      " [  0  49 874]]\n",
      "\n",
      "Spanish: 2\n",
      "x: 7217 \t y: 7217\n",
      "Not Found words = 0.000000\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.22      0.37      2152\n",
      "           1       0.73      0.91      0.81      2883\n",
      "           2       0.68      0.97      0.80      2182\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      7217\n",
      "   macro avg       0.80      0.70      0.66      7217\n",
      "weighted avg       0.80      0.73      0.68      7217\n",
      "\n",
      "Confusion Matrix\n",
      "[[ 484  901  767]\n",
      " [   1 2633  249]\n",
      " [   0   56 2126]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 7217 \t y: 7217\n",
      "x: 3202 \t y: 3202\n",
      "Not Found words = 0.000000\n"
     ]
    }
   ],
   "source": [
    "x_train_es, y_train_es =  from_datas_to_x_y([es_tass1_data, es2_twitter_data], word2id=word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0385765550239234, 1: 0.8748110831234257, 2: 1.1185185185185185}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_class_weight(y_train_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9377 samples, validate on 1042 samples\n",
      "Epoch 1/10\n",
      "9377/9377 [==============================] - 10s 1ms/step - loss: 0.3096 - acc: 0.8860 - f1: 0.8843 - val_loss: 0.4082 - val_acc: 0.8369 - val_f1: 0.8290\n",
      "Epoch 2/10\n",
      "9377/9377 [==============================] - 10s 1ms/step - loss: 0.1528 - acc: 0.9469 - f1: 0.9464 - val_loss: 0.4552 - val_acc: 0.8340 - val_f1: 0.8333\n",
      "Epoch 3/10\n",
      "9377/9377 [==============================] - 11s 1ms/step - loss: 0.0749 - acc: 0.9752 - f1: 0.9760 - val_loss: 0.5621 - val_acc: 0.8215 - val_f1: 0.8221\n",
      "Epoch 4/10\n",
      "9377/9377 [==============================] - 10s 1ms/step - loss: 0.0491 - acc: 0.9841 - f1: 0.9843 - val_loss: 0.6701 - val_acc: 0.8148 - val_f1: 0.8139\n",
      "Epoch 5/10\n",
      "1152/9377 [==>...........................] - ETA: 8s - loss: 0.0415 - acc: 0.9887 - f1: 0.9878"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-44ab79a55924>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_es, y_train_es, \n",
    "                    epochs=10, \n",
    "                    batch_size=128, \n",
    "                    validation_split=0.1, \n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Mixed: \n",
      "x: 3062 \t y: 3062\n",
      "Not Found words = 0.000000\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.36      0.43      1313\n",
      "           1       0.50      0.58      0.53       963\n",
      "           2       0.45      0.60      0.52       786\n",
      "\n",
      "   micro avg       0.49      0.49      0.49      3062\n",
      "   macro avg       0.49      0.51      0.49      3062\n",
      "weighted avg       0.50      0.49      0.49      3062\n",
      "\n",
      "Confusion Matrix\n",
      "[[478 417 418]\n",
      " [256 554 153]\n",
      " [170 144 472]]\n",
      "\n",
      "English: \n",
      "x: 4241 \t y: 4241\n",
      "Not Found words = 0.000000\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91      1952\n",
      "           1       0.90      0.89      0.89      1340\n",
      "           2       0.89      0.85      0.87       949\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      4241\n",
      "   macro avg       0.89      0.89      0.89      4241\n",
      "weighted avg       0.89      0.89      0.89      4241\n",
      "\n",
      "Confusion Matrix\n",
      "[[1794   93   65]\n",
      " [ 105 1196   39]\n",
      " [ 101   45  803]]\n",
      "\n",
      "Spanish: 1\n",
      "x: 3202 \t y: 3202\n",
      "Not Found words = 0.000000\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      1192\n",
      "           1       0.98      0.94      0.96      1087\n",
      "           2       0.98      0.97      0.97       923\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      3202\n",
      "   macro avg       0.96      0.96      0.96      3202\n",
      "weighted avg       0.96      0.96      0.96      3202\n",
      "\n",
      "Confusion Matrix\n",
      "[[1163   14   15]\n",
      " [  55 1025    7]\n",
      " [  22   10  891]]\n",
      "\n",
      "Spanish: 2\n",
      "x: 7217 \t y: 7217\n",
      "Not Found words = 0.000000\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      2152\n",
      "           1       0.99      0.97      0.98      2883\n",
      "           2       0.98      0.98      0.98      2182\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      7217\n",
      "   macro avg       0.98      0.98      0.98      7217\n",
      "weighted avg       0.98      0.98      0.98      7217\n",
      "\n",
      "Confusion Matrix\n",
      "[[2109   21   22]\n",
      " [  61 2805   17]\n",
      " [  30   11 2141]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both English and Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 4241 \t y: 4241\n",
      "x: 3202 \t y: 3202\n",
      "x: 7217 \t y: 7217\n",
      "Not Found words = 0.000000\n"
     ]
    }
   ],
   "source": [
    "x_train_both, y_train_both = from_datas_to_x_y([en_twitter_data,es2_twitter_data,es_tass1_data],word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 3062 \t y: 3062\n",
      "Not Found words = 0.000000\n"
     ]
    }
   ],
   "source": [
    "x_test,y_test = from_datas_to_x_y([en_es_wssa_data],word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14660 samples, validate on 3062 samples\n",
      "Epoch 1/1\n",
      "14660/14660 [==============================] - 17s 1ms/step - loss: 0.1047 - acc: 0.9641 - f1: 0.9636 - val_loss: 2.3280 - val_acc: 0.4700 - val_f1: 0.4646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb6280de0b8>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_both,y_train_both,\n",
    "          validation_data=(x_test,y_test),\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Mixed: \n",
      "x: 3062 \t y: 3062\n",
      "Not Found words = 0.000000\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.31      0.38      1313\n",
      "           1       0.48      0.58      0.53       963\n",
      "           2       0.43      0.61      0.50       786\n",
      "\n",
      "   micro avg       0.47      0.47      0.47      3062\n",
      "   macro avg       0.47      0.50      0.47      3062\n",
      "weighted avg       0.48      0.47      0.46      3062\n",
      "\n",
      "Confusion Matrix\n",
      "[[403 445 465]\n",
      " [224 555 184]\n",
      " [156 149 481]]\n",
      "\n",
      "English: \n",
      "x: 4241 \t y: 4241\n",
      "Not Found words = 0.000000\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1952\n",
      "           1       1.00      1.00      1.00      1340\n",
      "           2       1.00      1.00      1.00       949\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      4241\n",
      "   macro avg       1.00      1.00      1.00      4241\n",
      "weighted avg       1.00      1.00      1.00      4241\n",
      "\n",
      "Confusion Matrix\n",
      "[[1949    3    0]\n",
      " [   4 1336    0]\n",
      " [   1    2  946]]\n",
      "\n",
      "Spanish: 1\n",
      "x: 3202 \t y: 3202\n",
      "Not Found words = 0.000000\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      1192\n",
      "           1       0.99      0.96      0.98      1087\n",
      "           2       1.00      0.98      0.99       923\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      3202\n",
      "   macro avg       0.98      0.98      0.98      3202\n",
      "weighted avg       0.98      0.98      0.98      3202\n",
      "\n",
      "Confusion Matrix\n",
      "[[1185    4    3]\n",
      " [  42 1045    0]\n",
      " [  12    3  908]]\n",
      "\n",
      "Spanish: 2\n",
      "x: 7217 \t y: 7217\n",
      "Not Found words = 0.000000\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2152\n",
      "           1       0.99      1.00      0.99      2883\n",
      "           2       1.00      1.00      1.00      2182\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      7217\n",
      "   macro avg       1.00      1.00      1.00      7217\n",
      "weighted avg       1.00      1.00      1.00      7217\n",
      "\n",
      "Confusion Matrix\n",
      "[[2137   14    1]\n",
      " [  13 2869    1]\n",
      " [   5    1 2176]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Lingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        Only computes a batch-wise average of recall.\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "def get_class_weight(y):\n",
    "    \"\"\"\n",
    "    Used from: https://stackoverflow.com/a/50695814\n",
    "    TODO: check validity and 'balanced' option\n",
    "    :param y: A list of one-hot-encoding labels [[0,0,1,0],[0,0,0,1],..]\n",
    "    :return: class-weights to be used by keras model.fit(.. class_weight=\"\") -> {0:0.52134, 1:1.adas..}\n",
    "    \"\"\"\n",
    "    y_integers = np.argmax(y, axis=1)\n",
    "    class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "    return d_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlblocks import text\n",
    "from dlblocks.pyutils import mapArrays , loadJson , saveJson , selectKeys , oneHotVec , padList\n",
    "from dlblocks.pyutils import int64Arr , floatArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = text.Vocabulary()\n",
    "\n",
    "for d in es_tass1_data + en_es_wssa_data + en_twitter_data + es2_twitter_data :\n",
    "    vocab.add_words( d['tokens']  )\n",
    "\n",
    "    \n",
    "vocab.keepTopK(25000)\n",
    "\n",
    "\n",
    "\n",
    "maxSentenceL = 150\n",
    "\n",
    "def vecc( d ):\n",
    "    ret = {}\n",
    "    words   = d['tokens']\n",
    "    wordids = map( vocab , words )\n",
    "    ret['sentence'] = int64Arr( padList( wordids , maxSentenceL , 0 , 'left') )\n",
    "    ret['sentiment_val'] =  floatArr( d['sentiment'] )\n",
    "    ret['sentiment_id'] =  int64Arr( d['sentiment'] + 1 )\n",
    "    ret['sentiment_onehot'] =  floatArr( oneHotVec( d['sentiment']+1 , 3  ) )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "en_es_wssa_data_train_arr = mapArrays( en_es_wssa_data_train , vecc )\n",
    "en_es_wssa_data_test_arr = mapArrays( en_es_wssa_data_test , vecc )\n",
    "\n",
    "en_twitter_data_train_arr = mapArrays( en_twitter_data , vecc )\n",
    "es_tass1_datatrain_arr = mapArrays( es_tass1_data , vecc )\n",
    "\n",
    "datasets = {\"en_es_wssa_data_train_arr\":en_es_wssa_data_train_arr ,\n",
    "           \"en_es_wssa_data_test_arr\":en_es_wssa_data_test_arr ,\n",
    "           \"en_twitter_data_train_arr\":en_twitter_data_train_arr ,\n",
    "           \"es_tass1_datatrain_arr\": es_tass1_datatrain_arr }\n",
    "\n",
    "\n",
    "\n",
    "outFNN = \"../data/senti_prepped.h5\"\n",
    "\n",
    "f = h5py.File(outFNN , \"w\")\n",
    "for kk in datasets.keys():\n",
    "    f.create_group( kk  )\n",
    "    for k in datasets[kk].keys():\n",
    "        f[ kk ].create_dataset( k , data=datasets[kk][k] )\n",
    "\n",
    "print \"HDF5 file created !\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
