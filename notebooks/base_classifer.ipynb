{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include useful folders\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../vendors/mtl_girnet/data_prep/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable or disable cuda\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "\n",
    "# tokenizer\n",
    "from twokenize import tokenizeRawTweetText as tokenize\n",
    "\n",
    "# for a particular dataset\n",
    "from xml.dom import minidom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trying differnet types of tokenizer\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# from  nltk.stem import SnowballStemmer\n",
    "# from tokensize_deepmoji import tokenize\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "# tokenizer = TweetTokenizer(a)\n",
    "# from tokenizer import tokenizer\n",
    "# T = tokenizer.TweetTokenizer(preserve_handles=False, preserve_hashes=False, preserve_case=False, preserve_url=False, regularize=True)\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = stopwords.words(\"english\")\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "# TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "# def preprocess(text, stem=False):\n",
    "#     # Remove link,user and special characters\n",
    "#     text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "#     tokens = []\n",
    "#     for token in text.split():\n",
    "#         if token not in stop_words:\n",
    "#             if stem:\n",
    "#                 tokens.append(stemmer.stem(token))\n",
    "#             else:\n",
    "#                 tokens.append(token)\n",
    "#     return tokenizer.tokenize(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment140 dataset \n",
    "https://www.kaggle.com/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essential functions/declarations\n",
    "decode_map = {0: -1, 2: 0, 4: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.6 millionss tweets dataset\n",
    "df = pd.read_csv('../data/'+'training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1' , names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is goning to take some time. chill \n",
    "df.target = df.target.apply(lambda x: decode_map[int(x)])\n",
    "df.text = df.text.apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = map( lambda x :{'sentiment': x[0] , 'tokens': x[-1] , } , df.to_numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentiment140 = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English-Spanish Code Mixed Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = {\"N\":-1 , \"P\" :1 , \"NONE\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/cs-corpus-with-tweets_train.txt\", encoding='utf-8').read().split(\"\\n\") \n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': sents[x[1]] , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "en_es_wssa_data_train = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/cs-corpus-with-tweets_test.txt\", encoding='utf-8').read().split(\"\\n\") \n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': sents[x[1]] , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "en_es_wssa_data_test = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_es_wssa_data = list(en_es_wssa_data_train) + list(en_es_wssa_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish Tweet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc = minidom.parse(\"../vendors/mtl_girnet/data_prep/data_cm_senti/general-tweets-train-tagged.xml\")\n",
    "tweets = xmldoc.getElementsByTagName('tweet')\n",
    "\n",
    "sents = {\"N\":-1 , \"P\" :1 , \"NEU\":0 , 'NONE':0 , \"P+\" : 1 , \"N+\":-1 }\n",
    "\n",
    "\n",
    "es_tass1_data = []\n",
    "\n",
    "for i in range( len(tweets)-1) :\n",
    "    if i == 6055:\n",
    "        continue # bad jogar\n",
    "    textt = tweets[i].getElementsByTagName('content')[0].childNodes[0].data\n",
    "    words = tokenize( textt )\n",
    "    sentiment = tweets[i].getElementsByTagName('polarity')[0].getElementsByTagName('value')[0].childNodes[0].data\n",
    "    assert len(tweets[i].getElementsByTagName('polarity')[0].getElementsByTagName('entity'))==0\n",
    "    es_tass1_data.append({'text':textt , 'tokens':words , 'sentiment': sents[sentiment] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some english tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/twitter4242.txt\", \"r\", encoding=\"utf-8\",errors='ignore').read().split(\"\\n\")[1:-1]\n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': int(np.sign(int(x[0])-int(x[1]))) , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "\n",
    "en_twitter_data = list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### es2_twitter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/1600_tweets_dev_complete.txt\", encoding=\"utf-8\").read().split(\"\\n\")[1:-1]\n",
    "data += open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/1600_tweets_test_average_complete.tsv\", encoding=\"utf-8\").read().split(\"\\n\")[1:-2]\n",
    "\n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': int(np.sign(int(x[0])-int(x[1]))) , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "\n",
    "es2_twitter_data = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code-Mixed: en_es_wssa_data: 3062\n",
      "Spanish: es2_twitter_data: 3202\n",
      "Spanish: es_tass1_data: 7217\n",
      "English: en_twitter_data: 4241\n",
      "English: en_sentiment140: 1600000\n"
     ]
    }
   ],
   "source": [
    "print(\"Code-Mixed: en_es_wssa_data: %d\" % len(en_es_wssa_data))\n",
    "print(\"Spanish: es2_twitter_data: %d\" % len(es2_twitter_data))\n",
    "print(\"Spanish: es_tass1_data: %d\" % len(es_tass1_data))\n",
    "print(\"English: en_twitter_data: %d\" % len(en_twitter_data))\n",
    "print(\"English: en_sentiment140: %d\" %len(en_sentiment140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## essential functions\n",
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIG\n",
    "## NEED TO RUN MUSE BEFORE THIS and to get this path\n",
    "src_path = '../vendors/MUSE/dumped/6pzywzu6yg/vectors-en.txt'\n",
    "# src_path = '../vendors/MUSE/data/wiki.en.vec'\n",
    "tgt_path = '../vendors/MUSE/dumped/6pzywzu6yg/vectors-es.txt'\n",
    "nmax = 100000  # maximum number of word embeddings to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n",
    "    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
    "    word2id = {v: k for k, v in src_id2word.items()}\n",
    "    word_emb = src_emb[word2id[word]]\n",
    "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    for i, idx in enumerate(k_best):\n",
    "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"cat\":\n",
      "1.0000 - cat\n",
      "0.7322 - cats\n",
      "0.6453 - kitten\n",
      "0.6381 - dog\n",
      "0.6218 - kittens\n"
     ]
    }
   ],
   "source": [
    "# printing nearest neighbors in the source space\n",
    "src_word = 'cat'\n",
    "get_nn(src_word, src_embeddings, src_id2word, src_embeddings, src_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"cat\":\n",
      "0.6239 - gato\n",
      "0.5517 - perro\n",
      "0.5393 - gatito\n",
      "0.4895 - conejo\n",
      "0.4846 - gorila\n"
     ]
    }
   ],
   "source": [
    "# printing nearest neighbors in the target space\n",
    "src_word = 'cat'\n",
    "get_nn(src_word, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 300)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_embeddings(src_embeddings, tgt_embeddings):\n",
    "    \n",
    "    # make combined embedding mattrix\n",
    "    embedding_matrix = src_embeddings.copy().tolist()\n",
    "    embedding_matrix.extend(tgt_embeddings.tolist())\n",
    "    embedding_matrix = np.array(embedding_matrix)\n",
    "    \n",
    "    # make combined id2word and word2id\n",
    "    id2word = src_id2word.copy()\n",
    "    word2id = src_word2id.copy()\n",
    "    \n",
    "    next_id = len(id2word.keys())\n",
    "    counter = len(id2word.keys())\n",
    "    \n",
    "    to_be_removed_id = []\n",
    "    common_words = []\n",
    "    \n",
    "    for key in tgt_id2word:\n",
    "        if tgt_id2word[key] in word2id:\n",
    "            to_be_removed_id.append(counter)\n",
    "            common_words.append(tgt_id2word[key])\n",
    "            embedding_matrix[word2id[tgt_id2word[key]]] =  (embedding_matrix[word2id[tgt_id2word[key]]] + embedding_matrix[counter])/2\n",
    "        else:\n",
    "            id2word[next_id] = tgt_id2word[key]\n",
    "            word2id[tgt_id2word[key]] = next_id\n",
    "            next_id += 1\n",
    "        counter += 1\n",
    "        \n",
    "    embedding_matrix = np.delete(embedding_matrix, to_be_removed_id, axis=0)\n",
    "        \n",
    "    return embedding_matrix, id2word, word2id, common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, id2word, word2id, common_words = merge_embeddings(src_embeddings, tgt_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size:  (161832, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"embedding size: \", str(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common words in both the embedding 38168\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of common words in both the embedding %d\" % len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD UNK\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_not_found = 0\n",
    "def from_datas_to_x_y(list_data, word2id, max_seq_len=20, max_classes=3, seed=0):\n",
    "    stemmer = SnowballStemmer(\"english\")  \n",
    "    words_not_found = 0\n",
    "    def to_x(sample):\n",
    "        global words_not_found\n",
    "        x = []\n",
    "        for word in sample['tokens']:\n",
    "            # lower the word \n",
    "            word = word.lower()\n",
    "            if word in word2id:\n",
    "                x.append(word2id[word]) \n",
    "            else:\n",
    "                stem = stemmer.stem(word) # find stem\n",
    "                if stem in word2id:\n",
    "                    x.append(word2id[stem])\n",
    "                else:\n",
    "                    words_not_found = words_not_found + 1\n",
    "                    pass\n",
    "        return x\n",
    "\n",
    "    def to_x_y(data):\n",
    "        temp =  np.array(list(map(lambda x : [to_x(x), x['sentiment']], data)))\n",
    "        x = list(sequence.pad_sequences(temp[:,0], maxlen=max_seq_len))\n",
    "        y = list(to_categorical(temp[:,1],num_classes=max_classes))\n",
    "        return x, y\n",
    "    \n",
    "    x,y = [],[]\n",
    "    for data in list_data:\n",
    "        x_, y_ = to_x_y(data)\n",
    "        print(\"x: %d \\t y: %d\" % (len(x_),len(y_)))\n",
    "        x.extend(x_)\n",
    "        y.extend(y_)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    x,y = shuffle(x, y, replace=True)\n",
    "    \n",
    "    print(\"Not Found words = %f\" % (float(words_not_found)/(x.shape[0])))\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 3062 \t y: 3062\n",
      "Not Found words = 0.000000\n"
     ]
    }
   ],
   "source": [
    "x_test,y_test = from_datas_to_x_y([en_es_wssa_data],word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    print(\"One-Shot Code Mixed\")\n",
    "    x_test,y_test = from_datas_to_x_y([en_es_wssa_data],word2id)\n",
    "    print(model.evaluate(x_test,y_test,batch_size=128,verbose=0))\n",
    "    \n",
    "    print(\"One-Shot Spanish: 1\")\n",
    "    x_test,y_test = from_datas_to_x_y([es2_twitter_data],word2id)\n",
    "    print(model.evaluate(x_test,y_test,batch_size=128,verbose=0))\n",
    "    \n",
    "    print(\"One-Shot Spanish: 2\")\n",
    "    x_test,y_test = from_datas_to_x_y([es_tass1_data],word2id)\n",
    "    print(model.evaluate(x_test,y_test,batch_size=128,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x.shape)\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding\n",
    "EMBEDDING_DIM = embedding_matrix.shape[1] \n",
    "MAX_NUM_WORDS = embedding_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cnn_lstm\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 1\n",
    "# lstm\n",
    "lstm_output_size = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NUM_WORDS,\n",
    "                     EMBEDDING_DIM,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=MAX_SEQUENCE_LENGTH,\n",
    "                     trainable=False))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(filters,\n",
    "#                  kernel_size,\n",
    "#                  activation='relu',\n",
    "#                  strides=1))\n",
    "# model.add(MaxPooling1D(pool_size=pool_size))\n",
    "# model.add(LSTM(lstm_output_size))\n",
    "model.add(Bidirectional(LSTM(512, dropout=0.1, recurrent_dropout=0.1)))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 300)           48549600  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1024)              3330048   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 3075      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 51,882,723\n",
      "Trainable params: 3,333,123\n",
      "Non-trainable params: 48,549,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "earlystop = EarlyStopping(monitor='val_f1', min_delta=0.01, patience=4, \\\n",
    "                          verbose=1, mode='auto', restore_best_weights=True)\n",
    "checkpointer = ModelCheckpoint(filepath='weights.{epoch:02d}-{val_f1:.2f}.hdf5', verbose=1, save_best_only=True, save_weights_only=True, monitor='val_f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 1600000 \t y: 1600000\n",
      "Not Found words = 0.000000\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = from_datas_to_x_y([en_sentiment140],word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0, 1: 1.0}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_class_weight(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1120000 samples, validate on 480000 samples\n",
      "Epoch 1/10\n",
      "1120000/1120000 [==============================] - 354s 316us/step - loss: 0.4892 - acc: 0.7599 - f1: 0.7594 - val_loss: 0.4386 - val_acc: 0.7944 - val_f1: 0.7944\n",
      "Epoch 2/10\n",
      "1120000/1120000 [==============================] - 350s 313us/step - loss: 0.4282 - acc: 0.8005 - f1: 0.8005 - val_loss: 0.4121 - val_acc: 0.8093 - val_f1: 0.8093\n",
      "Epoch 3/10\n",
      "1120000/1120000 [==============================] - 350s 313us/step - loss: 0.4071 - acc: 0.8127 - f1: 0.8127 - val_loss: 0.4032 - val_acc: 0.8141 - val_f1: 0.8141\n",
      "Epoch 4/10\n",
      "1120000/1120000 [==============================] - 350s 313us/step - loss: 0.3914 - acc: 0.8215 - f1: 0.8215 - val_loss: 0.3956 - val_acc: 0.8194 - val_f1: 0.8194\n",
      "Epoch 5/10\n",
      "1120000/1120000 [==============================] - 351s 314us/step - loss: 0.3778 - acc: 0.8288 - f1: 0.8288 - val_loss: 0.3930 - val_acc: 0.8215 - val_f1: 0.8215\n",
      "Epoch 6/10\n",
      "1120000/1120000 [==============================] - 352s 314us/step - loss: 0.3641 - acc: 0.8362 - f1: 0.8362 - val_loss: 0.3897 - val_acc: 0.8236 - val_f1: 0.8236\n",
      "Epoch 7/10\n",
      "1120000/1120000 [==============================] - 354s 316us/step - loss: 0.3493 - acc: 0.8442 - f1: 0.8442 - val_loss: 0.3954 - val_acc: 0.8227 - val_f1: 0.8227\n",
      "Epoch 8/10\n",
      "1120000/1120000 [==============================] - 501s 447us/step - loss: 0.3335 - acc: 0.8520 - f1: 0.8520 - val_loss: 0.4038 - val_acc: 0.8208 - val_f1: 0.8208\n",
      "Epoch 9/10\n",
      "1120000/1120000 [==============================] - 363s 324us/step - loss: 0.3168 - acc: 0.8606 - f1: 0.8606 - val_loss: 0.4029 - val_acc: 0.8205 - val_f1: 0.8205\n",
      "Epoch 10/10\n",
      "1120000/1120000 [==============================] - 353s 315us/step - loss: 0.2993 - acc: 0.8697 - f1: 0.8697 - val_loss: 0.4186 - val_acc: 0.8186 - val_f1: 0.8186\n"
     ]
    }
   ],
   "source": [
    "history = model.fit( x_train , y_train, epochs=10, batch_size=648, validation_split=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Shot Code Mixed\n",
      "x: 3062 \t y: 3062\n",
      "Not Found words = 0.000000\n",
      "[7.154420852038223, 0.39483997354236483, 0.39483991508620925]\n",
      "One-Shot Spanish: 1\n",
      "x: 3202 \t y: 3202\n",
      "Not Found words = 0.000000\n",
      "[6.33727825879306, 0.39631480324797, 0.39631474364332525]\n",
      "One-Shot Spanish: 2\n",
      "x: 7217 \t y: 7217\n",
      "Not Found words = 0.000000\n",
      "[5.248283325378726, 0.4486628792796981, 0.44866281967505334]\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 7217 \t y: 7217\n",
      "x: 3202 \t y: 3202\n",
      "Not Found words = 0.000000\n"
     ]
    }
   ],
   "source": [
    "x_train_es, y_train_es =  from_datas_to_x_y([es_tass1_data, es2_twitter_data], word2id=word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0385765550239234, 1: 0.8748110831234257, 2: 1.1185185185185185}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_class_weight(y_train_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9377 samples, validate on 1042 samples\n",
      "Epoch 1/5\n",
      "9377/9377 [==============================] - 7s 748us/step - loss: 1.1921 - acc: 0.5339 - f1: 0.4927 - val_loss: 0.8955 - val_acc: 0.5787 - val_f1: 0.5290\n",
      "Epoch 2/5\n",
      "9377/9377 [==============================] - 7s 746us/step - loss: 0.7719 - acc: 0.6519 - f1: 0.6201 - val_loss: 0.8919 - val_acc: 0.5873 - val_f1: 0.5541\n",
      "Epoch 3/5\n",
      "9377/9377 [==============================] - 7s 763us/step - loss: 0.6704 - acc: 0.7109 - f1: 0.6855 - val_loss: 0.9092 - val_acc: 0.5854 - val_f1: 0.5724\n",
      "Epoch 4/5\n",
      "9377/9377 [==============================] - 7s 723us/step - loss: 0.5606 - acc: 0.7699 - f1: 0.7561 - val_loss: 0.9780 - val_acc: 0.5950 - val_f1: 0.5790\n",
      "Epoch 5/5\n",
      "9377/9377 [==============================] - 7s 731us/step - loss: 0.4352 - acc: 0.8336 - f1: 0.8251 - val_loss: 1.0092 - val_acc: 0.5797 - val_f1: 0.5653\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_es, y_train_es, epochs=5, batch_size=128, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Shot Code Mixed\n",
      "x: 3062 \t y: 3062\n",
      "Not Found words = 0.000000\n",
      "[1.3436515315695574, 0.48367080386365496, 0.46996553982884803]\n",
      "One-Shot Spanish: 1\n",
      "x: 3202 \t y: 3202\n",
      "Not Found words = 0.000000\n",
      "[0.39362900129558376, 0.8800749531542785, 0.8721769977405174]\n",
      "One-Shot Spanish: 2\n",
      "x: 7217 \t y: 7217\n",
      "Not Found words = 0.000000\n",
      "[0.3046022879956782, 0.9095191907251708, 0.9075652555496355]\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both English and Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 4241 \t y: 4241\n",
      "x: 3202 \t y: 3202\n",
      "x: 7217 \t y: 7217\n",
      "Not Found words = 0.000000\n"
     ]
    }
   ],
   "source": [
    "x_train_both, y_train_both = from_datas_to_x_y([en_twitter_data,es2_twitter_data,es_tass1_data],word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14660 samples, validate on 3062 samples\n",
      "Epoch 1/5\n",
      "14660/14660 [==============================] - 43s 3ms/step - loss: 0.6641 - acc: 0.7177 - f1: 0.7072 - val_loss: 1.0251 - val_acc: 0.5274 - val_f1: 0.4941\n",
      "Epoch 2/5\n",
      "14660/14660 [==============================] - 44s 3ms/step - loss: 0.4075 - acc: 0.8402 - f1: 0.8348 - val_loss: 1.1320 - val_acc: 0.5333 - val_f1: 0.5072\n",
      "Epoch 3/5\n",
      "14660/14660 [==============================] - 44s 3ms/step - loss: 0.2508 - acc: 0.9138 - f1: 0.9111 - val_loss: 1.3085 - val_acc: 0.4948 - val_f1: 0.4783\n",
      "Epoch 4/5\n",
      "14660/14660 [==============================] - 44s 3ms/step - loss: 0.1562 - acc: 0.9504 - f1: 0.9498 - val_loss: 1.4588 - val_acc: 0.5007 - val_f1: 0.4905\n",
      "Epoch 5/5\n",
      "14660/14660 [==============================] - 44s 3ms/step - loss: 0.1083 - acc: 0.9690 - f1: 0.9687 - val_loss: 1.5450 - val_acc: 0.4954 - val_f1: 0.4893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2f1c3c3da0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_both,y_train_both,validation_data=(x_test,y_test),batch_size=32, epochs=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Shot Code Mixed\n",
      "x: 3062 \t y: 3062\n",
      "Not Found words = 0.000000\n",
      "[1.5450221322705588, 0.49542782588537654, 0.4895957799069948]\n",
      "One-Shot Spanish: 1\n",
      "x: 3202 \t y: 3202\n",
      "Not Found words = 0.000000\n",
      "[0.07692923541439652, 0.9787632729544035, 0.9784129307762971]\n",
      "One-Shot Spanish: 2\n",
      "x: 7217 \t y: 7217\n",
      "Not Found words = 0.000000\n",
      "[0.03395619294253209, 0.9941804073714839, 0.9939679122952291]\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Lingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        Only computes a batch-wise average of recall.\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "def get_class_weight(y):\n",
    "    \"\"\"\n",
    "    Used from: https://stackoverflow.com/a/50695814\n",
    "    TODO: check validity and 'balanced' option\n",
    "    :param y: A list of one-hot-encoding labels [[0,0,1,0],[0,0,0,1],..]\n",
    "    :return: class-weights to be used by keras model.fit(.. class_weight=\"\") -> {0:0.52134, 1:1.adas..}\n",
    "    \"\"\"\n",
    "    y_integers = np.argmax(y, axis=1)\n",
    "    class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "    return d_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlblocks import text\n",
    "from dlblocks.pyutils import mapArrays , loadJson , saveJson , selectKeys , oneHotVec , padList\n",
    "from dlblocks.pyutils import int64Arr , floatArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = text.Vocabulary()\n",
    "\n",
    "for d in es_tass1_data + en_es_wssa_data + en_twitter_data + es2_twitter_data :\n",
    "    vocab.add_words( d['tokens']  )\n",
    "\n",
    "    \n",
    "vocab.keepTopK(25000)\n",
    "\n",
    "\n",
    "\n",
    "maxSentenceL = 150\n",
    "\n",
    "def vecc( d ):\n",
    "    ret = {}\n",
    "    words   = d['tokens']\n",
    "    wordids = map( vocab , words )\n",
    "    ret['sentence'] = int64Arr( padList( wordids , maxSentenceL , 0 , 'left') )\n",
    "    ret['sentiment_val'] =  floatArr( d['sentiment'] )\n",
    "    ret['sentiment_id'] =  int64Arr( d['sentiment'] + 1 )\n",
    "    ret['sentiment_onehot'] =  floatArr( oneHotVec( d['sentiment']+1 , 3  ) )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "en_es_wssa_data_train_arr = mapArrays( en_es_wssa_data_train , vecc )\n",
    "en_es_wssa_data_test_arr = mapArrays( en_es_wssa_data_test , vecc )\n",
    "\n",
    "en_twitter_data_train_arr = mapArrays( en_twitter_data , vecc )\n",
    "es_tass1_datatrain_arr = mapArrays( es_tass1_data , vecc )\n",
    "\n",
    "datasets = {\"en_es_wssa_data_train_arr\":en_es_wssa_data_train_arr ,\n",
    "           \"en_es_wssa_data_test_arr\":en_es_wssa_data_test_arr ,\n",
    "           \"en_twitter_data_train_arr\":en_twitter_data_train_arr ,\n",
    "           \"es_tass1_datatrain_arr\": es_tass1_datatrain_arr }\n",
    "\n",
    "\n",
    "\n",
    "outFNN = \"../data/senti_prepped.h5\"\n",
    "\n",
    "f = h5py.File(outFNN , \"w\")\n",
    "for kk in datasets.keys():\n",
    "    f.create_group( kk  )\n",
    "    for k in datasets[kk].keys():\n",
    "        f[ kk ].create_dataset( k , data=datasets[kk][k] )\n",
    "\n",
    "print \"HDF5 file created !\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
