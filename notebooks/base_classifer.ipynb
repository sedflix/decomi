{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include useful folders\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../vendors/mtl_girnet/data_prep/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable or disable cuda\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "\n",
    "# tokenizer\n",
    "from twokenize import tokenizeRawTweetText as tokenize\n",
    "\n",
    "# for a particular dataset\n",
    "from xml.dom import minidom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trying differnet types of tokenizer\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# from  nltk.stem import SnowballStemmer\n",
    "# from tokensize_deepmoji import tokenize\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "# tokenizer = TweetTokenizer(a)\n",
    "# from tokenizer import tokenizer\n",
    "# T = tokenizer.TweetTokenizer(preserve_handles=False, preserve_hashes=False, preserve_case=False, preserve_url=False, regularize=True)\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = stopwords.words(\"english\")\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "# TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "# def preprocess(text, stem=False):\n",
    "#     # Remove link,user and special characters\n",
    "#     text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "#     tokens = []\n",
    "#     for token in text.split():\n",
    "#         if token not in stop_words:\n",
    "#             if stem:\n",
    "#                 tokens.append(stemmer.stem(token))\n",
    "#             else:\n",
    "#                 tokens.append(token)\n",
    "#     return tokenizer.tokenize(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment140 dataset \n",
    "https://www.kaggle.com/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essential functions/declarations\n",
    "decode_map = {0: -1, 2: 0, 4: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.6 millionss tweets dataset\n",
    "df = pd.read_csv('../data/'+'training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1' , names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is goning to take some time. chill \n",
    "df.target = df.target.apply(lambda x: decode_map[int(x)])\n",
    "df.text = df.text.apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = map( lambda x :{'sentiment': x[0] , 'tokens': x[-1] , } , df.to_numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentiment140 = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English-Spanish Code Mixed Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = {\"N\":-1 , \"P\" :1 , \"NONE\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/cs-corpus-with-tweets_train.txt\", encoding='utf-8').read().split(\"\\n\") \n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': sents[x[1]] , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "en_es_wssa_data_train = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/cs-corpus-with-tweets_test.txt\", encoding='utf-8').read().split(\"\\n\") \n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': sents[x[1]] , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "en_es_wssa_data_test = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_es_wssa_data = list(en_es_wssa_data_train) + list(en_es_wssa_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish Tweet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc = minidom.parse(\"../vendors/mtl_girnet/data_prep/data_cm_senti/general-tweets-train-tagged.xml\")\n",
    "tweets = xmldoc.getElementsByTagName('tweet')\n",
    "\n",
    "sents = {\"N\":-1 , \"P\" :1 , \"NEU\":0 , 'NONE':0 , \"P+\" : 1 , \"N+\":-1 }\n",
    "\n",
    "\n",
    "es_tass1_data = []\n",
    "\n",
    "for i in range( len(tweets)-1) :\n",
    "    if i == 6055:\n",
    "        continue # bad jogar\n",
    "    textt = tweets[i].getElementsByTagName('content')[0].childNodes[0].data\n",
    "    words = tokenize( textt )\n",
    "    sentiment = tweets[i].getElementsByTagName('polarity')[0].getElementsByTagName('value')[0].childNodes[0].data\n",
    "    assert len(tweets[i].getElementsByTagName('polarity')[0].getElementsByTagName('entity'))==0\n",
    "    es_tass1_data.append({'text':textt , 'tokens':words , 'sentiment': sents[sentiment] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some english tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/twitter4242.txt\", \"r\", encoding=\"utf-8\",errors='ignore').read().split(\"\\n\")[1:-1]\n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': int(np.sign(int(x[0])-int(x[1]))) , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "\n",
    "en_twitter_data = list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### es2_twitter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/1600_tweets_dev_complete.txt\", encoding=\"utf-8\").read().split(\"\\n\")[1:-1]\n",
    "data += open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/1600_tweets_test_average_complete.tsv\", encoding=\"utf-8\").read().split(\"\\n\")[1:-2]\n",
    "\n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': int(np.sign(int(x[0])-int(x[1]))) , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "\n",
    "es2_twitter_data = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Code-Mixed: en_es_wssa_data: %d\" % len(en_es_wssa_data))\n",
    "print(\"Spanish: es2_twitter_data: %d\" % len(es2_twitter_data))\n",
    "print(\"Spanish: es_tass1_data: %d\" % len(es_tass1_data))\n",
    "print(\"English: en_twitter_data: %d\" % len(en_twitter_data))\n",
    "print(\"English: en_sentiment140: %d\" %len(en_sentiment140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## essential functions\n",
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIG\n",
    "## NEED TO RUN MUSE BEFORE THIS and to get this path\n",
    "src_path = '../vendors/MUSE/dumped/6pzywzu6yg/vectors-en.txt'\n",
    "# src_path = '../vendors/MUSE/data/wiki.en.vec'\n",
    "tgt_path = '../vendors/MUSE/dumped/6pzywzu6yg/vectors-es.txt'\n",
    "nmax = 100000  # maximum number of word embeddings to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n",
    "    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
    "    word2id = {v: k for k, v in src_id2word.items()}\n",
    "    word_emb = src_emb[word2id[word]]\n",
    "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    for i, idx in enumerate(k_best):\n",
    "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing nearest neighbors in the source space\n",
    "src_word = 'cat'\n",
    "get_nn(src_word, src_embeddings, src_id2word, src_embeddings, src_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing nearest neighbors in the target space\n",
    "src_word = 'cat'\n",
    "get_nn(src_word, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_embeddings(src_embeddings, tgt_embeddings):\n",
    "    \n",
    "    # make combined embedding mattrix\n",
    "    embedding_matrix = src_embeddings.copy().tolist()\n",
    "    embedding_matrix.extend(tgt_embeddings.tolist())\n",
    "    embedding_matrix = np.array(embedding_matrix)\n",
    "    \n",
    "    # make combined id2word and word2id\n",
    "    id2word = src_id2word.copy()\n",
    "    word2id = src_word2id.copy()\n",
    "    \n",
    "    next_id = len(id2word.keys())\n",
    "    counter = len(id2word.keys())\n",
    "    \n",
    "    to_be_removed_id = []\n",
    "    common_words = []\n",
    "    \n",
    "    for key in tgt_id2word:\n",
    "        if tgt_id2word[key] in word2id:\n",
    "            to_be_removed_id.append(counter)\n",
    "            common_words.append(tgt_id2word[key])\n",
    "            embedding_matrix[word2id[tgt_id2word[key]]] =  (embedding_matrix[word2id[tgt_id2word[key]]] + embedding_matrix[counter])/2\n",
    "        else:\n",
    "            id2word[next_id] = tgt_id2word[key]\n",
    "            word2id[tgt_id2word[key]] = next_id\n",
    "            next_id += 1\n",
    "        counter += 1\n",
    "        \n",
    "    embedding_matrix = np.delete(embedding_matrix, to_be_removed_id, axis=0)\n",
    "        \n",
    "    return embedding_matrix, id2word, word2id, common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, id2word, word2id, common_words = merge_embeddings(src_embeddings, tgt_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"embedding size: \", str(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of common words in both the embedding %d\" % len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD UNK\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_not_found = 0\n",
    "def from_datas_to_x_y(list_data, word2id, max_seq_len=20, max_classes=3, seed=0):\n",
    "    stemmer = SnowballStemmer(\"english\")  \n",
    "    words_not_found = 0\n",
    "    def to_x(sample):\n",
    "        global words_not_found\n",
    "        x = []\n",
    "        for word in sample['tokens']:\n",
    "            # lower the word \n",
    "            word = word.lower()\n",
    "            if word in word2id:\n",
    "                x.append(word2id[word]) \n",
    "            else:\n",
    "                stem = stemmer.stem(word) # find stem\n",
    "                if stem in word2id:\n",
    "                    x.append(word2id[stem])\n",
    "                else:\n",
    "                    words_not_found = words_not_found + 1\n",
    "                    pass\n",
    "        return x\n",
    "\n",
    "    def to_x_y(data):\n",
    "        temp =  np.array(list(map(lambda x : [to_x(x), x['sentiment']], data)))\n",
    "        x = list(sequence.pad_sequences(temp[:,0], maxlen=max_seq_len))\n",
    "        y = list(to_categorical(temp[:,1],num_classes=max_classes))\n",
    "        return x, y\n",
    "    \n",
    "    x,y = [],[]\n",
    "    for data in list_data:\n",
    "        x_, y_ = to_x_y(data)\n",
    "        print(\"x: %d \\t y: %d\" % (len(x_),len(y_)))\n",
    "        x.extend(x_)\n",
    "        y.extend(y_)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    x,y = shuffle(x, y, replace=True)\n",
    "    \n",
    "    print(\"Not Found words = %f\" % (float(words_not_found)/(x.shape[0])))\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test,y_test = from_datas_to_x_y([en_es_wssa_data],word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    print(\"One-Shot Code Mixed\")\n",
    "    x_test,y_test = from_datas_to_x_y([en_es_wssa_data],word2id)\n",
    "    print(model.evaluate(x_test,x_test,batch_size=128,verbose=0))\n",
    "    \n",
    "    print(\"One-Shot Spanish: 1\")\n",
    "    x_test,y_test = from_datas_to_x_y([es2_twitter_data],word2id)\n",
    "    print(model.evaluate(x_test,x_test,batch_size=128,verbose=0))\n",
    "    \n",
    "    print(\"One-Shot Spanish: 2\")\n",
    "    x_test,y_test = from_datas_to_x_y([es_tass1_data],word2id)\n",
    "    print(model.evaluate(x_test,x_test,batch_size=128,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x.shape)\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding\n",
    "EMBEDDING_DIM = embedding_matrix.shape[1] \n",
    "MAX_NUM_WORDS = embedding_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cnn_lstm\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 1\n",
    "# lstm\n",
    "lstm_output_size = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NUM_WORDS,\n",
    "                     EMBEDDING_DIM,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=MAX_SEQUENCE_LENGTH,\n",
    "                     trainable=False))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(filters,\n",
    "#                  kernel_size,\n",
    "#                  activation='relu',\n",
    "#                  strides=1))\n",
    "# model.add(MaxPooling1D(pool_size=pool_size))\n",
    "# model.add(LSTM(lstm_output_size))\n",
    "model.add(Bidirectional(LSTM(512, dropout=0.1, recurrent_dropout=0.1)))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = from_datas_to_x_y([en_sentiment140],word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_class_weight(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit( x_train , y_train, epochs=10, batch_size=648, validation_split=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_es, y_train_es =  from_datas_to_x_y([es_tass1_data, es2_twitter_data], word2id=word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_class_weight(y_train_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train_es, y_train_es, epochs=5, batch_size=128, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both English and Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 4241 \t y: 4241\n",
      "x: 3202 \t y: 3202\n",
      "x: 7217 \t y: 7217\n",
      "Not Found words = 0.000000\n"
     ]
    }
   ],
   "source": [
    "x_train_both, y_train_both = from_datas_to_x_y([en_twitter_data,es2_twitter_data,es_tass1_data],word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14660 samples, validate on 3062 samples\n",
      "Epoch 1/10\n",
      "14660/14660 [==============================] - 9s 626us/step - loss: 0.4204 - acc: 0.8338 - f1: 0.8318 - val_loss: 1.1204 - val_acc: 0.5186 - val_f1: 0.4903\n",
      "Epoch 2/10\n",
      "14660/14660 [==============================] - 9s 636us/step - loss: 0.3663 - acc: 0.8561 - f1: 0.8538 - val_loss: 1.1732 - val_acc: 0.5091 - val_f1: 0.4918\n",
      "Epoch 3/10\n",
      "14660/14660 [==============================] - 9s 624us/step - loss: 0.3358 - acc: 0.8681 - f1: 0.8667 - val_loss: 1.2275 - val_acc: 0.5013 - val_f1: 0.4818\n",
      "Epoch 4/10\n",
      "14660/14660 [==============================] - 9s 623us/step - loss: 0.2840 - acc: 0.8911 - f1: 0.8911 - val_loss: 1.2524 - val_acc: 0.5173 - val_f1: 0.4952\n",
      "Epoch 5/10\n",
      "14660/14660 [==============================] - 9s 634us/step - loss: 0.2657 - acc: 0.8998 - f1: 0.8993 - val_loss: 1.3282 - val_acc: 0.5023 - val_f1: 0.4857\n",
      "Epoch 6/10\n",
      "14660/14660 [==============================] - 9s 646us/step - loss: 0.2357 - acc: 0.9127 - f1: 0.9111 - val_loss: 1.3468 - val_acc: 0.5101 - val_f1: 0.4961\n",
      "Epoch 7/10\n",
      "14660/14660 [==============================] - 9s 612us/step - loss: 0.2118 - acc: 0.9231 - f1: 0.9229 - val_loss: 1.3963 - val_acc: 0.4961 - val_f1: 0.4818\n",
      "Epoch 8/10\n",
      "14660/14660 [==============================] - 9s 627us/step - loss: 0.1920 - acc: 0.9318 - f1: 0.9323 - val_loss: 1.4331 - val_acc: 0.5052 - val_f1: 0.4970\n",
      "Epoch 9/10\n",
      "14660/14660 [==============================] - 9s 640us/step - loss: 0.1697 - acc: 0.9410 - f1: 0.9403 - val_loss: 1.4664 - val_acc: 0.5000 - val_f1: 0.4911\n",
      "Epoch 10/10\n",
      "14660/14660 [==============================] - 9s 620us/step - loss: 0.1568 - acc: 0.9460 - f1: 0.9446 - val_loss: 1.5860 - val_acc: 0.4922 - val_f1: 0.4850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f69bf78a6d8>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_both,y_train_both,validation_data=(x_test,y_test),batch_size=128, epochs=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Lingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 10, 300)           48549600  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 6, 64)             96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 70)                37800     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 213       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 48,683,677\n",
      "Trainable params: 134,077\n",
      "Non-trainable params: 48,549,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1614660 samples, validate on 3062 samples\n",
      "Epoch 1/40\n",
      "1614660/1614660 [==============================] - 16s 10us/step - loss: 0.6739 - acc: 0.6068 - f1: 0.5836 - val_loss: 2.5199 - val_acc: 0.3573 - val_f1: 0.3537\n",
      "Epoch 2/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.6339 - acc: 0.6414 - f1: 0.6399 - val_loss: 2.2754 - val_acc: 0.3632 - val_f1: 0.3296\n",
      "Epoch 3/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.6255 - acc: 0.6489 - f1: 0.6473 - val_loss: 2.1351 - val_acc: 0.3596 - val_f1: 0.3191\n",
      "Epoch 4/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.6201 - acc: 0.6530 - f1: 0.6517 - val_loss: 1.9748 - val_acc: 0.3658 - val_f1: 0.3069\n",
      "Epoch 5/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.6167 - acc: 0.6557 - f1: 0.6545 - val_loss: 1.8689 - val_acc: 0.3654 - val_f1: 0.2964\n",
      "Epoch 6/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.6140 - acc: 0.6580 - f1: 0.6568 - val_loss: 1.8219 - val_acc: 0.3772 - val_f1: 0.3001\n",
      "Epoch 7/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.6114 - acc: 0.6603 - f1: 0.6591 - val_loss: 1.8345 - val_acc: 0.3831 - val_f1: 0.3079\n",
      "Epoch 8/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.6098 - acc: 0.6612 - f1: 0.6602 - val_loss: 1.7682 - val_acc: 0.3883 - val_f1: 0.3046\n",
      "Epoch 9/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.6082 - acc: 0.6626 - f1: 0.6615 - val_loss: 1.6600 - val_acc: 0.4046 - val_f1: 0.3088\n",
      "Epoch 10/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.6061 - acc: 0.6643 - f1: 0.6634 - val_loss: 1.6841 - val_acc: 0.3961 - val_f1: 0.3183\n",
      "Epoch 11/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.6051 - acc: 0.6653 - f1: 0.6644 - val_loss: 1.6421 - val_acc: 0.4010 - val_f1: 0.3168\n",
      "Epoch 12/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.6038 - acc: 0.6665 - f1: 0.6656 - val_loss: 1.6653 - val_acc: 0.4007 - val_f1: 0.3355\n",
      "Epoch 13/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.6026 - acc: 0.6674 - f1: 0.6666 - val_loss: 1.6350 - val_acc: 0.4073 - val_f1: 0.3305\n",
      "Epoch 14/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.6017 - acc: 0.6679 - f1: 0.6671 - val_loss: 1.6069 - val_acc: 0.4014 - val_f1: 0.3341\n",
      "Epoch 15/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.6004 - acc: 0.6691 - f1: 0.6683 - val_loss: 1.6450 - val_acc: 0.4115 - val_f1: 0.3418\n",
      "Epoch 16/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5999 - acc: 0.6693 - f1: 0.6685 - val_loss: 1.6118 - val_acc: 0.4089 - val_f1: 0.3339\n",
      "Epoch 17/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5991 - acc: 0.6703 - f1: 0.6695 - val_loss: 1.5955 - val_acc: 0.4043 - val_f1: 0.3295\n",
      "Epoch 18/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5984 - acc: 0.6707 - f1: 0.6700 - val_loss: 1.5925 - val_acc: 0.4108 - val_f1: 0.3408\n",
      "Epoch 19/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5982 - acc: 0.6707 - f1: 0.6699 - val_loss: 1.5443 - val_acc: 0.4089 - val_f1: 0.3268\n",
      "Epoch 20/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5973 - acc: 0.6719 - f1: 0.6712 - val_loss: 1.6175 - val_acc: 0.4115 - val_f1: 0.3358\n",
      "Epoch 21/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5969 - acc: 0.6721 - f1: 0.6714 - val_loss: 1.6468 - val_acc: 0.4050 - val_f1: 0.3363\n",
      "Epoch 22/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5962 - acc: 0.6727 - f1: 0.6720 - val_loss: 1.5827 - val_acc: 0.4076 - val_f1: 0.3319\n",
      "Epoch 23/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5955 - acc: 0.6733 - f1: 0.6727 - val_loss: 1.6138 - val_acc: 0.4164 - val_f1: 0.3289\n",
      "Epoch 24/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5954 - acc: 0.6734 - f1: 0.6728 - val_loss: 1.5094 - val_acc: 0.4118 - val_f1: 0.3270\n",
      "Epoch 25/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5946 - acc: 0.6737 - f1: 0.6731 - val_loss: 1.6596 - val_acc: 0.4017 - val_f1: 0.3282\n",
      "Epoch 26/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5942 - acc: 0.6745 - f1: 0.6739 - val_loss: 1.6101 - val_acc: 0.4138 - val_f1: 0.3269\n",
      "Epoch 27/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5939 - acc: 0.6744 - f1: 0.6739 - val_loss: 1.6590 - val_acc: 0.4112 - val_f1: 0.3215\n",
      "Epoch 28/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5936 - acc: 0.6747 - f1: 0.6742 - val_loss: 1.6416 - val_acc: 0.4027 - val_f1: 0.3237\n",
      "Epoch 29/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5931 - acc: 0.6754 - f1: 0.6749 - val_loss: 1.5940 - val_acc: 0.4105 - val_f1: 0.3310\n",
      "Epoch 30/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5930 - acc: 0.6753 - f1: 0.6748 - val_loss: 1.6026 - val_acc: 0.4131 - val_f1: 0.3230\n",
      "Epoch 31/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5928 - acc: 0.6753 - f1: 0.6748 - val_loss: 1.6133 - val_acc: 0.4043 - val_f1: 0.3166\n",
      "Epoch 32/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5922 - acc: 0.6759 - f1: 0.6753 - val_loss: 1.6465 - val_acc: 0.4053 - val_f1: 0.3263\n",
      "Epoch 33/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5919 - acc: 0.6764 - f1: 0.6759 - val_loss: 1.5956 - val_acc: 0.4079 - val_f1: 0.3141\n",
      "Epoch 34/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5914 - acc: 0.6764 - f1: 0.6758 - val_loss: 1.5995 - val_acc: 0.4108 - val_f1: 0.3143\n",
      "Epoch 35/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5915 - acc: 0.6762 - f1: 0.6757 - val_loss: 1.6149 - val_acc: 0.4027 - val_f1: 0.3095\n",
      "Epoch 36/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5915 - acc: 0.6765 - f1: 0.6760 - val_loss: 1.6640 - val_acc: 0.4063 - val_f1: 0.3094\n",
      "Epoch 37/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5909 - acc: 0.6768 - f1: 0.6763 - val_loss: 1.6725 - val_acc: 0.4141 - val_f1: 0.3197\n",
      "Epoch 38/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5906 - acc: 0.6770 - f1: 0.6765 - val_loss: 1.6244 - val_acc: 0.4112 - val_f1: 0.3198\n",
      "Epoch 39/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5904 - acc: 0.6772 - f1: 0.6767 - val_loss: 1.6509 - val_acc: 0.4046 - val_f1: 0.3172\n",
      "Epoch 40/40\n",
      "1614660/1614660 [==============================] - 10s 6us/step - loss: 0.5903 - acc: 0.6771 - f1: 0.6766 - val_loss: 1.6385 - val_acc: 0.4059 - val_f1: 0.3079\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=40, batch_size=6000, validation_data=(x_test,y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        Only computes a batch-wise average of recall.\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "def get_class_weight(y):\n",
    "    \"\"\"\n",
    "    Used from: https://stackoverflow.com/a/50695814\n",
    "    TODO: check validity and 'balanced' option\n",
    "    :param y: A list of one-hot-encoding labels [[0,0,1,0],[0,0,0,1],..]\n",
    "    :return: class-weights to be used by keras model.fit(.. class_weight=\"\") -> {0:0.52134, 1:1.adas..}\n",
    "    \"\"\"\n",
    "    y_integers = np.argmax(y, axis=1)\n",
    "    class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "    return d_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlblocks import text\n",
    "from dlblocks.pyutils import mapArrays , loadJson , saveJson , selectKeys , oneHotVec , padList\n",
    "from dlblocks.pyutils import int64Arr , floatArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = text.Vocabulary()\n",
    "\n",
    "for d in es_tass1_data + en_es_wssa_data + en_twitter_data + es2_twitter_data :\n",
    "    vocab.add_words( d['tokens']  )\n",
    "\n",
    "    \n",
    "vocab.keepTopK(25000)\n",
    "\n",
    "\n",
    "\n",
    "maxSentenceL = 150\n",
    "\n",
    "def vecc( d ):\n",
    "    ret = {}\n",
    "    words   = d['tokens']\n",
    "    wordids = map( vocab , words )\n",
    "    ret['sentence'] = int64Arr( padList( wordids , maxSentenceL , 0 , 'left') )\n",
    "    ret['sentiment_val'] =  floatArr( d['sentiment'] )\n",
    "    ret['sentiment_id'] =  int64Arr( d['sentiment'] + 1 )\n",
    "    ret['sentiment_onehot'] =  floatArr( oneHotVec( d['sentiment']+1 , 3  ) )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "en_es_wssa_data_train_arr = mapArrays( en_es_wssa_data_train , vecc )\n",
    "en_es_wssa_data_test_arr = mapArrays( en_es_wssa_data_test , vecc )\n",
    "\n",
    "en_twitter_data_train_arr = mapArrays( en_twitter_data , vecc )\n",
    "es_tass1_datatrain_arr = mapArrays( es_tass1_data , vecc )\n",
    "\n",
    "datasets = {\"en_es_wssa_data_train_arr\":en_es_wssa_data_train_arr ,\n",
    "           \"en_es_wssa_data_test_arr\":en_es_wssa_data_test_arr ,\n",
    "           \"en_twitter_data_train_arr\":en_twitter_data_train_arr ,\n",
    "           \"es_tass1_datatrain_arr\": es_tass1_datatrain_arr }\n",
    "\n",
    "\n",
    "\n",
    "outFNN = \"../data/senti_prepped.h5\"\n",
    "\n",
    "f = h5py.File(outFNN , \"w\")\n",
    "for kk in datasets.keys():\n",
    "    f.create_group( kk  )\n",
    "    for k in datasets[kk].keys():\n",
    "        f[ kk ].create_dataset( k , data=datasets[kk][k] )\n",
    "\n",
    "print \"HDF5 file created !\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
