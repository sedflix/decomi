{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras.backend as K\n",
    "\n",
    "from elmo.lm_generator import LMDataGenerator, MTLLMDataGenerator\n",
    "from elmo.model_girnet import ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "- fasttext: sent_vector: unsup: [0.5040783036202435, 0.44986425566245447]\n",
    "- fasttext: sent_vector: sup: [0.5628058728541675, 0.5187983749157842]\n",
    "- fasttext: lstm: unsup [0.6182707995419401, 0.5768815131210775] \n",
    "- fasttext: lstm: sup [0.6705, 0.6624]\n",
    "- lstm 400,200: preplexity - 14 on cm_p_test_1000.txt\n",
    "- lstm - mtl preplexity: 14.398013495954116, loss: 1.6848554463386536\n",
    "- girnet: perplexity: 165.677334, losss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_DIR = '../twiter_scrapping/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'name': 'cm_girnet',\n",
    "    'multi_processing': True,\n",
    "    'n_threads': 10,\n",
    "    'cuDNN': True if len(K.tensorflow_backend._get_available_gpus()) else False,\n",
    "    'train_dataset': 'cm_p_train_small.txt',\n",
    "    'valid_dataset': 'cm_p_test_1000.txt',\n",
    "    'test_dataset': 'cm_test_main_1000.txt',\n",
    "    'vocab': 'vocab',\n",
    "    'vocab_size': 525133,\n",
    "    'num_sampled': 8000,\n",
    "    'charset_size': 262,\n",
    "    'sentence_maxlen': 32,\n",
    "    'token_maxlen': 50,\n",
    "    'token_encoding': 'word',\n",
    "    'epochs': 1,\n",
    "    'patience': 2,\n",
    "    'batch_size': 32,\n",
    "    'clip_value': 1,\n",
    "    'cell_clip': 5,\n",
    "    'proj_clip': 5,\n",
    "    'lr': 0.2,\n",
    "    'shuffle': True,\n",
    "    'n_lstm_layers': 1,\n",
    "    'n_highway_layers': 1,\n",
    "    'cnn_filters': [[1, 32],\n",
    "                    [2, 32],\n",
    "                    [3, 64],\n",
    "                    [4, 128],\n",
    "                    [5, 256],\n",
    "                    [6, 512],\n",
    "                    [7, 512]\n",
    "                    ],\n",
    "    'lstm_units_size': 400,\n",
    "    'hidden_units_size': 200,\n",
    "    'char_embedding_size': 16,\n",
    "    'dropout_rate': 0.1,\n",
    "    'word_dropout_rate': 0.05,\n",
    "    'weight_tying': True,\n",
    "    'unidirectional': True,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator = MTLLMDataGenerator([os.path.join(DATA_SET_DIR, 'en_p_train_small.txt'),os.path.join(DATA_SET_DIR, 'es_p_train_small.txt'),os.path.join(DATA_SET_DIR, 'cm_p_train_small.txt')],\n",
    "#                                   os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "#                                   sentence_maxlen=parameters['sentence_maxlen'],\n",
    "#                                   token_maxlen=parameters['token_maxlen'],\n",
    "#                                   batch_size=parameters['batch_size'],\n",
    "#                                   shuffle=parameters['shuffle'],\n",
    "#                                   token_encoding=parameters['token_encoding']\n",
    "# )\n",
    "# val_generator = MTLLMDataGenerator([os.path.join(DATA_SET_DIR, 'en_p_test_1000.txt'),os.path.join(DATA_SET_DIR, 'es_p_test_1000.txt'),os.path.join(DATA_SET_DIR, 'cm_p_test_1000.txt')],\n",
    "#                                   os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "#                                   sentence_maxlen=parameters['sentence_maxlen'],\n",
    "#                                   token_maxlen=parameters['token_maxlen'],\n",
    "#                                   batch_size=parameters['batch_size'],\n",
    "#                                   shuffle=parameters['shuffle'],\n",
    "#                                   token_encoding=parameters['token_encoding']\n",
    "# )\n",
    "test_generator = MTLLMDataGenerator([os.path.join(DATA_SET_DIR, parameters['test_dataset']),os.path.join(DATA_SET_DIR, parameters['test_dataset']),os.path.join(DATA_SET_DIR, parameters['test_dataset'])],\n",
    "                                  os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                  sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                  token_maxlen=parameters['token_maxlen'],\n",
    "                                  batch_size=parameters['batch_size'],\n",
    "                                  shuffle=parameters['shuffle'],\n",
    "                                  token_encoding=parameters['token_encoding']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_en_generator =  LMDataGenerator(os.path.join(DATA_SET_DIR, 'en_p_train_small.txt'),\n",
    "#                                   os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "#                                   sentence_maxlen=parameters['sentence_maxlen'],\n",
    "#                                   token_maxlen=parameters['token_maxlen'],\n",
    "#                                   batch_size=parameters['batch_size'],\n",
    "#                                   shuffle=parameters['shuffle'],\n",
    "#                                   token_encoding=parameters['token_encoding'])\n",
    "\n",
    "# train_es_generator =  LMDataGenerator(os.path.join(DATA_SET_DIR, 'es_p_train_small.txt'),\n",
    "#                                   os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "#                                   sentence_maxlen=parameters['sentence_maxlen'],\n",
    "#                                   token_maxlen=parameters['token_maxlen'],\n",
    "#                                   batch_size=parameters['batch_size'],\n",
    "#                                   shuffle=parameters['shuffle'],\n",
    "#                                   token_encoding=parameters['token_encoding'])\n",
    "\n",
    "# train_cm_generator =  LMDataGenerator(os.path.join(DATA_SET_DIR, 'cm_p_train_small.txt'),\n",
    "#                                   os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "#                                   sentence_maxlen=parameters['sentence_maxlen'],\n",
    "#                                   token_maxlen=parameters['token_maxlen'],\n",
    "#                                   batch_size=parameters['batch_size'],\n",
    "#                                   shuffle=parameters['shuffle'],\n",
    "#                                   token_encoding=parameters['token_encoding'])\n",
    "\n",
    "test_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['test_dataset']),\n",
    "                                os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                token_maxlen=parameters['token_maxlen'],\n",
    "                                batch_size=parameters['batch_size'],\n",
    "                                shuffle=parameters['shuffle'],\n",
    "                                token_encoding=parameters['token_encoding'])\n",
    "# val_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['valid_dataset']),\n",
    "#                                 os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "#                                 sentence_maxlen=parameters['sentence_maxlen'],\n",
    "#                                 token_maxlen=parameters['token_maxlen'],\n",
    "#                                 batch_size=parameters['batch_size'],\n",
    "#                                 shuffle=parameters['shuffle'],\n",
    "#                                 token_encoding=parameters['token_encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up Generators\n",
    "train_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['train_dataset']),\n",
    "                                  os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                  sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                  token_maxlen=parameters['token_maxlen'],\n",
    "                                  batch_size=parameters['batch_size'],\n",
    "                                  shuffle=parameters['shuffle'],\n",
    "                                  token_encoding=parameters['token_encoding'])\n",
    "\n",
    "val_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['valid_dataset']),\n",
    "                                os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                token_maxlen=parameters['token_maxlen'],\n",
    "                                batch_size=parameters['batch_size'],\n",
    "                                shuffle=parameters['shuffle'],\n",
    "                                token_encoding=parameters['token_encoding'])\n",
    "\n",
    "test_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['test_dataset']),\n",
    "                                os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                token_maxlen=parameters['token_maxlen'],\n",
    "                                batch_size=parameters['batch_size'],\n",
    "                                shuffle=parameters['shuffle'],\n",
    "                                token_encoding=parameters['token_encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile ELMo\n",
    "elmo_model = ELMo(parameters)\n",
    "elmo_model.compile_elmo(print_summary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tr in [train_en_generator, train_es_generator, train_cm_generator]:\n",
    "#     elmo_model.train(train_data=tr, valid_data=val_generator)\n",
    "#  5.5218\n",
    "elmo_model.train(train_data=train_generator, valid_data=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_model.load_temp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model saved successfully\n"
     ]
    }
   ],
   "source": [
    "elmo_model.save(sampled_softmax=False, temp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cm_p_test_1000.txt\n",
    "# vanilla :  46.75967671612614\n",
    "# mtl: 39.42567465448776\n",
    "# girnet: 165.677334\n",
    "# cm_main_test_1000.txt\n",
    "# vanilla :  73.61951568054178\n",
    "# mtl: 58.723475872121696\n",
    "# girnet: 132.512263\n",
    "elmo_model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ELMo meta-model to deploy for production and persist in disk\n",
    "# elmo_model.wrap_multi_elmo_encoder(print_summary=True, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elmo_model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ELMo embeddings to feed as inputs for downstream tasks\n",
    "# elmo_embeddings = elmo_model.get_outputs(test_generator, output_type='word', state='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sample_logits(preds, temperature=1.0):\n",
    "    \"\"\"\n",
    "\n",
    "    Sample an index from a logit vector.\n",
    "\n",
    "    :param preds:\n",
    "    :param temperature:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    from scipy.misc import logsumexp\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        return np.argmax(preds)\n",
    "\n",
    "    preds = preds / temperature\n",
    "    preds = preds - logsumexp(preds)\n",
    "\n",
    "    choice = np.random.choice(len(preds), 1, p=np.exp(preds))\n",
    "\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = test_generator[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.array([2,3]).reshape(2,1,1)\n",
    "elmo_model._model.predict([[[2,3]], seed, seed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = [2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_seed_forward = np.array(seed[:-1]+[0]).reshape(len(seed),1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(elmo_model._model.predict([np.array(seed).reshape(len(seed),1), temp_seed, temp_seed])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = seed.shape[0]\n",
    "\n",
    "        # Due to the way Keras RNNs work, we feed the model a complete sequence each time. At first it's just the seed,\n",
    "        # zero-padded to the right length. With each iteration we sample and set the next character.\n",
    "\n",
    "        # tokens = np.concatenate([seed, np.zeros(size - ls)])\n",
    "        tokens_all = []\n",
    "        for i in range(out_num):\n",
    "            np.array()\n",
    "            tokens_all.append(np.concatenate([seed, np.zeros(size - ls)]))\n",
    "\n",
    "        for i in range(ls, size):\n",
    "            tokens_to_predict = []\n",
    "            for j in range(out_num):\n",
    "                tokens_to_predict.append(tokens_all[j][None, :])\n",
    "\n",
    "            all_probs = self.model.predict(tokens_to_predict)\n",
    "\n",
    "            # Extract the i-th probability vector and sample an index from it\n",
    "            for j, probs in enumerate(all_probs):\n",
    "                next_token = sample_logits(probs[0, i - 1, :], temperature=temperature)\n",
    "                tokens_all[j][i] = next_token\n",
    "\n",
    "                return [tokens.astype('int') for tokens in tokens_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = test_generator.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = [2, 6, vocab[\"what\"], vocab[\"this\"]]\n",
    "size = 16\n",
    "for i in range(len(seed), size):\n",
    "    temp_seed_forward = np.array(seed[:-1]+[0]).reshape(len(seed),1,1)\n",
    "    x_seed = np.array(seed).reshape(len(seed),1)\n",
    "    pred = elmo_model._model.predict([x_seed, temp_seed_forward, temp_seed_forward])\n",
    "    seed.append(np.argmax(pred[i-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {}\n",
    "for key in test_generator.vocab:\n",
    "    id2word[test_generator.vocab[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sents_index):\n",
    "    ans = \"\"\n",
    "    for index in sents_index:\n",
    "        ans = ans +  id2word[index] + \" \"\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos> <user> what this is the sea <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> '"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(seed, size=32):\n",
    "    for i in range(len(seed), size):\n",
    "        temp_seed_forward = np.array(seed[:-1]+[0]).reshape(len(seed),1,1)\n",
    "        x_seed = np.array(seed).reshape(len(seed),1)\n",
    "        pred = elmo_model._model.predict([x_seed, temp_seed_forward, temp_seed_forward])\n",
    "        seed.append(np.argmax(pred[i-1]))\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = [2,324]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for i in range(len(seed), size):\n",
    "            temp_seed_forward = np.array(seed[:-1] + [0]).reshape(len(seed), 1, 1)\n",
    "            x_seed = np.array(seed).reshape(len(seed), 1)\n",
    "            x = [x_seed, temp_seed_forward, x_seed, temp_seed_forward, x_seed, temp_seed_forward]\n",
    "            pred = elmo_model._model.predict(x)\n",
    "            seed.append(np.argmax(pred[2][i - 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos> alguien <user> i am <user> i am <user> i am <user> i am <user> i '"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
