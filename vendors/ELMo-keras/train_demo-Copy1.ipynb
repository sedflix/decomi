{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras.backend as K\n",
    "\n",
    "from elmo.lm_generator import LMDataGenerator, MTLLMDataGenerator\n",
    "from elmo.model_girnet import ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "- fasttext: sent_vector: unsup: [0.5040783036202435, 0.44986425566245447]\n",
    "- fasttext: sent_vector: sup: [0.5628058728541675, 0.5187983749157842]\n",
    "- fasttext: lstm: unsup [0.6182707995419401, 0.5768815131210775] \n",
    "- fasttext: lstm: sup [0.6705, 0.6624]\n",
    "- lstm 400,200: preplexity - 14 on cm_p_test_1000.txt\n",
    "- lstm - mtl preplexity: 14.398013495954116, loss: 1.6848554463386536\n",
    "- girnet: perplexity: 165.677334, losss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_DIR = '../twiter_scrapping/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'name': 'cm_girnet_4',\n",
    "    'multi_processing': True,\n",
    "    'n_threads': 10,\n",
    "    'cuDNN': True if len(K.tensorflow_backend._get_available_gpus()) else False,\n",
    "    'train_dataset': 'cm_p_train_small.txt',\n",
    "    'valid_dataset': 'cm_p_test_1000.txt',\n",
    "    'test_dataset': 'cm_test_main_1000.txt',\n",
    "    'vocab': 'vocab',\n",
    "    'vocab_size': 525133,\n",
    "    'num_sampled': 8000,\n",
    "    'charset_size': 262,\n",
    "    'sentence_maxlen': 32,\n",
    "    'token_maxlen': 50,\n",
    "    'token_encoding': 'word',\n",
    "    'epochs': 1,\n",
    "    'patience': 2,\n",
    "    'batch_size': 32,\n",
    "    'clip_value': 1,\n",
    "    'cell_clip': 5,\n",
    "    'proj_clip': 5,\n",
    "    'lr': 0.2,\n",
    "    'shuffle': True,\n",
    "    'n_lstm_layers': 1,\n",
    "    'n_highway_layers': 1,\n",
    "    'cnn_filters': [[1, 32],\n",
    "                    [2, 32],\n",
    "                    [3, 64],\n",
    "                    [4, 128],\n",
    "                    [5, 256],\n",
    "                    [6, 512],\n",
    "                    [7, 512]\n",
    "                    ],\n",
    "    'lstm_units_size': 400,\n",
    "    'hidden_units_size': 200,\n",
    "    'char_embedding_size': 16,\n",
    "    'dropout_rate': 0.1,\n",
    "    'word_dropout_rate': 0.05,\n",
    "    'weight_tying': True,\n",
    "    'unidirectional': True,\n",
    "    'subwords': False,\n",
    "    'subword_weight': None,\n",
    "    'filters': 200,\n",
    "    'kernel_size': 3,\n",
    "    'pool_length': 1\n",
    "    \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator = MTLLMDataGenerator([os.path.join(DATA_SET_DIR, 'en_p_train_small.txt'),os.path.join(DATA_SET_DIR, 'es_p_train_small.txt'),os.path.join(DATA_SET_DIR, 'cm_p_train_small.txt')],\n",
    "#                                   os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "#                                   sentence_maxlen=parameters['sentence_maxlen'],\n",
    "#                                   token_maxlen=parameters['token_maxlen'],\n",
    "#                                   batch_size=parameters['batch_size'],\n",
    "#                                   shuffle=parameters['shuffle'],\n",
    "#                                   token_encoding=parameters['token_encoding']\n",
    "# )\n",
    "# val_generator = MTLLMDataGenerator([os.path.join(DATA_SET_DIR, 'en_p_test_1000.txt'),os.path.join(DATA_SET_DIR, 'es_p_test_1000.txt'),os.path.join(DATA_SET_DIR, 'cm_p_test_1000.txt')],\n",
    "#                                   os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "#                                   sentence_maxlen=parameters['sentence_maxlen'],\n",
    "#                                   token_maxlen=parameters['token_maxlen'],\n",
    "#                                   batch_size=parameters['batch_size'],\n",
    "#                                   shuffle=parameters['shuffle'],\n",
    "#                                   token_encoding=parameters['token_encoding']\n",
    "# )\n",
    "test_generator = MTLLMDataGenerator([os.path.join(DATA_SET_DIR, parameters['test_dataset']),os.path.join(DATA_SET_DIR, parameters['test_dataset']),os.path.join(DATA_SET_DIR, parameters['test_dataset'])],\n",
    "                                  os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                  sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                  token_maxlen=parameters['token_maxlen'],\n",
    "                                  batch_size=parameters['batch_size'],\n",
    "                                  shuffle=parameters['shuffle'],\n",
    "                                  token_encoding=parameters['token_encoding']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_en_generator =  LMDataGenerator(os.path.join(DATA_SET_DIR, 'en_p_train_small.txt'),\n",
    "#                                   os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "#                                   sentence_maxlen=parameters['sentence_maxlen'],\n",
    "#                                   token_maxlen=parameters['token_maxlen'],\n",
    "#                                   batch_size=parameters['batch_size'],\n",
    "#                                   shuffle=parameters['shuffle'],\n",
    "#                                   token_encoding=parameters['token_encoding'])\n",
    "\n",
    "# train_es_generator =  LMDataGenerator(os.path.join(DATA_SET_DIR, 'es_p_train_small.txt'),\n",
    "#                                   os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "#                                   sentence_maxlen=parameters['sentence_maxlen'],\n",
    "#                                   token_maxlen=parameters['token_maxlen'],\n",
    "#                                   batch_size=parameters['batch_size'],\n",
    "#                                   shuffle=parameters['shuffle'],\n",
    "#                                   token_encoding=parameters['token_encoding'])\n",
    "\n",
    "# train_cm_generator =  LMDataGenerator(os.path.join(DATA_SET_DIR, 'cm_p_train_small.txt'),\n",
    "#                                   os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "#                                   sentence_maxlen=parameters['sentence_maxlen'],\n",
    "#                                   token_maxlen=parameters['token_maxlen'],\n",
    "#                                   batch_size=parameters['batch_size'],\n",
    "#                                   shuffle=parameters['shuffle'],\n",
    "#                                   token_encoding=parameters['token_encoding'])\n",
    "\n",
    "test_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['test_dataset']),\n",
    "                                os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                token_maxlen=parameters['token_maxlen'],\n",
    "                                batch_size=parameters['batch_size'],\n",
    "                                shuffle=parameters['shuffle'],\n",
    "                                token_encoding=parameters['token_encoding'])\n",
    "# val_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['valid_dataset']),\n",
    "#                                 os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "#                                 sentence_maxlen=parameters['sentence_maxlen'],\n",
    "#                                 token_maxlen=parameters['token_maxlen'],\n",
    "#                                 batch_size=parameters['batch_size'],\n",
    "#                                 shuffle=parameters['shuffle'],\n",
    "#                                 token_encoding=parameters['token_encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up Generators\n",
    "train_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['train_dataset']),\n",
    "                                  os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                  sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                  token_maxlen=parameters['token_maxlen'],\n",
    "                                  batch_size=parameters['batch_size'],\n",
    "                                  shuffle=parameters['shuffle'],\n",
    "                                  token_encoding=parameters['token_encoding'])\n",
    "\n",
    "val_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['valid_dataset']),\n",
    "                                os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                token_maxlen=parameters['token_maxlen'],\n",
    "                                batch_size=parameters['batch_size'],\n",
    "                                shuffle=parameters['shuffle'],\n",
    "                                token_encoding=parameters['token_encoding'])\n",
    "\n",
    "test_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['test_dataset']),\n",
    "                                os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                token_maxlen=parameters['token_maxlen'],\n",
    "                                batch_size=parameters['batch_size'],\n",
    "                                shuffle=parameters['shuffle'],\n",
    "                                token_encoding=parameters['token_encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile ELMo\n",
    "elmo_model = ELMo(parameters)\n",
    "elmo_model.compile_elmo(print_summary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(elmo_model._model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tr in [train_en_generator, train_es_generator, train_cm_generator]:\n",
    "#     elmo_model.train(train_data=tr, valid_data=val_generator)\n",
    "#  5.5218\n",
    "# elmo_model.train(train_data=train_generator, valid_data=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_model.load_temp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model saved successfully\n"
     ]
    }
   ],
   "source": [
    "elmo_model.save(sampled_softmax=False, temp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Langauge Model Perplexity: 46.54720741329034\n"
     ]
    }
   ],
   "source": [
    "# cm_p_test_1000.txt\n",
    "# vanilla :  46.75967671612614\n",
    "# mtl: 39.42567465448776\n",
    "# girnet: 165.677334\n",
    "# cm_main_test_1000.txt\n",
    "# vanilla :  73.61951568054178\n",
    "# mtl: 58.723475872121696\n",
    "# girnet: 132.512263\n",
    "elmo_model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ELMo meta-model to deploy for production and persist in disk\n",
    "# elmo_model.wrap_multi_elmo_encoder(print_summary=True, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elmo_model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ELMo embeddings to feed as inputs for downstream tasks\n",
    "# elmo_embeddings = elmo_model.get_outputs(test_generator, output_type='word', state='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sample_logits(preds, temperature=0.1):\n",
    "    \"\"\"\n",
    "\n",
    "    Sample an index from a logit vector.\n",
    "\n",
    "    :param preds:\n",
    "    :param temperature:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    from scipy.misc import logsumexp\n",
    "    preds = np.asarray(preds[0]).astype('float64')\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        return np.argmax(preds)\n",
    "    \n",
    "    \n",
    "    preds = preds / temperature\n",
    "    preds = preds - logsumexp(preds)\n",
    "\n",
    "    choice = np.random.choice(len(preds), 1, p=np.exp(preds))[0]\n",
    "\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = test_generator.data2.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {}\n",
    "for key in vocab:\n",
    "    id2word[vocab[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sents_index):\n",
    "    ans = \"\"\n",
    "    for index in sents_index:\n",
    "        ans = ans +  id2word[index] + \" \"\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(seed, size=32):\n",
    "    for i in range(len(seed), size):\n",
    "        temp_seed_forward = np.array(seed[:-1]+[0]).reshape(len(seed),1,1)\n",
    "        x_seed = np.array(seed).reshape(len(seed),1)\n",
    "        pred = elmo_model._model.predict([x_seed, temp_seed_forward, temp_seed_forward])\n",
    "        seed.append(sample_logits((pred[i-1])))\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = [2, ]\n",
    "# ans = generate(seed)\n",
    "# decode(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: `logsumexp` is deprecated!\n",
      "Importing `logsumexp` from scipy.misc is deprecated in scipy 1.0.0. Use `scipy.special.logsumexp` instead.\n"
     ]
    }
   ],
   "source": [
    "seed = [24]\n",
    "for i in range(len(seed), size):\n",
    "    temp_seed_forward = np.array(seed[:-1] + [0]).reshape(len(seed), 1, 1)\n",
    "    x_seed = np.array(seed).reshape(len(seed), 1)\n",
    "    x = [x_seed, temp_seed_forward, x_seed, temp_seed_forward, x_seed, temp_seed_forward]\n",
    "    pred = elmo_model._model.predict(x)\n",
    "    seed.append(sample_logits(pred[0][i - 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i agru gasulla sincronica disfrutarais ungu malandra apaan reivindicaciã³n recuerdamelo restocked martinson celebrare clee commet mainframes '"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
