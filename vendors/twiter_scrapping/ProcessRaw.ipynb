{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install git+https://github.com/erikavaris/tokenizer.git\n",
    "# ! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import tokenizer\n",
    "from tqdm import tqdm\n",
    "T = tokenizer.TweetTokenizer(preserve_handles=False, preserve_hashes=False, preserve_case=True, preserve_url=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23543676"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./result.txt') as f:\n",
    "    sents = f.readlines()\n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14193880"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicates\n",
    "sents = list(set(sents))\n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary \n",
    "import numpy as np\n",
    "import io\n",
    "def load_vec(emb_path, nmax=50000):\n",
    "    word2id = {'<pad>':0, '<unk>':1, '<sos>':2, '<eos>':3}\n",
    "    vectors = [np.zeros(300) for _ in range(len(word2id))]\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = '../MUSE/dumped/6pzywzu6yg/vectors-en.txt'\n",
    "tgt_path = '..//MUSE/dumped/6pzywzu6yg/vectors-es.txt'\n",
    "nmax = 1000000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "len(src_id2word)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)\n",
    "len(tgt_id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token counts in the document\n",
    "def batch_process(sents):\n",
    "    unk_count = 0\n",
    "    en_count = 0\n",
    "    es_count = 0\n",
    "    word_count = 0\n",
    "    common_count = 0\n",
    "    for sent in sents:\n",
    "        sent = T.tokenize(sent)\n",
    "        for word in sent:\n",
    "            temp = False\n",
    "            if word in src_word2id:\n",
    "                en_count += 1\n",
    "                temp = True\n",
    "            if word in tgt_word2id:\n",
    "                es_count += 1\n",
    "                if temp:\n",
    "                    common_count += 1\n",
    "            if not temp:\n",
    "                unk_count += 1\n",
    "            \n",
    "            word_count += 1\n",
    "    \n",
    "    return en_count, es_count, common_count, unk_count, word_count\n",
    "batch_size = len(sents)//40\n",
    "batched_sents = [ sents[i:i+batch_size] for i in range(0,len(sents),batch_size) ]\n",
    "import multiprocessing\n",
    "pool = multiprocessing.Pool(processes=40)\n",
    "answer = pool.map(batch_process, (a_batch for a_batch in batched_sents))\n",
    "pool.close()\n",
    "answer = np.array(answer)\n",
    "print(\"en tokens: \", sum(answer[:,0]))\n",
    "print(\"es tokens: \", sum(answer[:,1]))\n",
    "print(\"common tokens: \", sum(answer[:,2]))\n",
    "print(\"unk tokens\", sum(answer[:,3]))\n",
    "print(\"total tokens\", sum(answer[:,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def each(sent):\n",
    "    sent = T.tokenize(sent)\n",
    "        \n",
    "    unk_count = 0\n",
    "    en_count = 0\n",
    "    es_count = 0\n",
    "    word_count = len(sent)\n",
    "    \n",
    "    if word_count <= 0:\n",
    "        return None\n",
    "    \n",
    "    for word in sent:\n",
    "        temp = False\n",
    "        if word in src_word2id:\n",
    "            en_count += 1\n",
    "            temp = True\n",
    "        if word in tgt_word2id:\n",
    "            es_count += 1\n",
    "            temp = True\n",
    "            \n",
    "        if not temp:\n",
    "            unk_count += 1\n",
    "            \n",
    "    if en_count / word_count >= 0.3 and  es_count / word_count >= 0.3:\n",
    "        return sent \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process(sents):\n",
    "    new_sents = []\n",
    "    for sent in sents:\n",
    "        x = each(sent)\n",
    "        if x is not None:\n",
    "            new_sents.append(x)\n",
    "    return new_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(sents)//40\n",
    "batched_sents = [ sents[i:i+batch_size] for i in range(0,len(sents),batch_size) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "pool = multiprocessing.Pool(processes=40)\n",
    "answer = pool.map(batch_process, (a_batch for a_batch in batched_sents))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"code-mixed.txt\", \"w\") as f:\n",
    "    for batch in answer:\n",
    "        for sent in batch:\n",
    "            f.write(' '.joint(snt) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
