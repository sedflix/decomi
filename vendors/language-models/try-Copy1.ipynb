{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "from words import *\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.corpora import Dictionary\n",
    "from keras.layers import *\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "vocab_size = 100000\n",
    "max_length = 64\n",
    "batch_size = 64\n",
    "input_file = './datasets/code_mixed_es_en_tweets.txt' # wiki_es.txt is the other file\n",
    "embedding_size = 100\n",
    "hidden = 100\n",
    "\n",
    "# loging info\n",
    "data_dir = './dumps/'\n",
    "experiment_name ='en_es'\n",
    "extra_tokens = {'<PAD>':4, '<START>':2, '<UNK>':1, '<EOS>':3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = data_dir+experiment_name+\"./\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011141\n",
      "2013873\n"
     ]
    }
   ],
   "source": [
    "en_dict = Dictionary.load('./datasets/wiki_dataset/wiki_en.vocab')\n",
    "print(len(en_dict.token2id))\n",
    "es_dict = Dictionary.load('./datasets/wiki_dataset/wiki_es.vocab')\n",
    "print(len(es_dict.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.VocabTransform at 0x7fdbcc031710>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_dict.merge_with(es_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3194833\n",
      "2013873\n"
     ]
    }
   ],
   "source": [
    "print(len(en_dict.token2id))\n",
    "print(len(es_dict.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54057"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dict = en_dict\n",
    "combined_dict.filter_extremes(keep_n=vocab_size, keep_tokens=None)\n",
    "combined_dict.patch_with_special_tokens(extra_tokens)\n",
    "print(len(combined_dict.token2id))\n",
    "combined_dict.token2id['lol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict.save('datasets/wiki_dataset/combined_vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36866"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(input_file) as f:\n",
    "    sentences = f.read().split(\"\\n\")\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install git+https://github.com/erikavaris/tokenizer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey', '!', 'this', 'is', 'a', 'nlproc', 'tweet', ':-D']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizer import tokenizer\n",
    "T = tokenizer.TweetTokenizer(preserve_case=False, preserve_handles=False, preserve_hashes=False, regularize=True, preserve_len=False, preserve_emoji=False, preserve_url=False)\n",
    "\n",
    "tweet = \"Hey @NLPer! This is a #NLProc tweet :-D http://www.somelink.com\"\n",
    "tokens = T.tokenize(tweet)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1, 43740,  5827, 79058, 27455, 13075,     1, 21502, 27455,\n",
       "        5778,     1, 47109, 76985,     1,     1,     1,     1,     1,\n",
       "           3,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "           4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "           4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "           4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "           4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "           4], dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def each(sentence):\n",
    "    x_ = combined_dict.doc2idx(text_to_word_sequence(' '.join(T.tokenize(sentence))), unknown_word_index=combined_dict.token2id['<UNK>'])\n",
    "    x_.append(combined_dict.token2id['<EOS>'])\n",
    "    return sequence.pad_sequences([x_], maxlen=max_length, dtype='int32', padding='post', truncating='post',value=combined_dict.token2id['<PAD>'])[0]\n",
    "each(sentences[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why can't it share its memory. Use joblib or something thread base if RAM is a bottle-neck\n",
    "pool = multiprocessing.Pool(processes=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pool.map(each, (sentence for sentence in sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sequence length  64\n",
      "100004 distinct words\n"
     ]
    }
   ],
   "source": [
    "# confirm stats\n",
    "x_max_len = max([len(sentence) for sentence in x])\n",
    "numwords = len(combined_dict.token2id)\n",
    "print('max sequence length ', x_max_len)\n",
    "print(numwords, 'distinct words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 64, 64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = []\n",
    "for i in range(1,len(x)-batch_size,batch_size):\n",
    "    batches.append(x[i:i+batch_size])\n",
    "batches = np.array(batches)\n",
    "batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading.  36864  sentences\n"
     ]
    }
   ],
   "source": [
    "def decode(seq):\n",
    "    return ' '.join(combined_dict[id_] for id_ in seq)\n",
    "print('Finished loading. ', sum([b.shape[0] for b in batches]), ' sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 8 ms, total: 12 ms\n",
      "Wall time: 12.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.save('datasets/wiki_dataset/twitter_en_es_100004_vocab.npy', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 100)         10000400  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 100)         80400     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 100004)      10100404  \n",
      "=================================================================\n",
      "Total params: 20,181,204\n",
      "Trainable params: 20,181,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_ = Input(shape=(None, ))\n",
    "embedding = Embedding(numwords, 100, input_length=None)\n",
    "embedded = embedding(input_)\n",
    "\n",
    "decoder_lstm = LSTM(100, return_sequences=True)\n",
    "h = decoder_lstm(embedded)\n",
    "\n",
    "fromhidden = Dense(numwords, activation='linear')\n",
    "out = TimeDistributed(fromhidden)(h)\n",
    "\n",
    "model = Model(input_, out)\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "lss = sparse_loss\n",
    "\n",
    "model.compile(opt, lss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/576 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-3499a8aa713a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*** ['\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'] '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-61-3499a8aa713a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mbatch_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# append pad symbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shifted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0minstances_seen\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m                 raise RuntimeError('You must compile a model before '\n\u001b[0m\u001b[1;32m    682\u001b[0m                                    \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                                    'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "def train(epochs = 10):\n",
    "    epoch = 0\n",
    "    instances_seen = 0\n",
    "    while epoch < epochs:\n",
    "\n",
    "        for batch in tqdm(batches):\n",
    "            n, l = batch.shape\n",
    "\n",
    "            batch_shifted = np.concatenate([np.ones((n, 1)), batch], axis=1)  # prepend start symbol\n",
    "            batch_out = np.concatenate([batch, np.zeros((n, 1))], axis=1)     # append pad symbol\n",
    "\n",
    "            loss = model.train_on_batch(batch_shifted, batch_out[:, :, None],)\n",
    " \n",
    "            instances_seen += n\n",
    "#             tbw.add_scalar('lm/batch-loss', float(loss), instances_seen)\n",
    "\n",
    "        epoch += 1\n",
    "    \n",
    "        # Show samples for some sentences from random batches\n",
    "        for temp in [0.0, 0.9, 1, 1.1, 1.2]:\n",
    "            print('### TEMP ', temp)\n",
    "            for i in range(CHECK):\n",
    "                b = random.choice(batches)\n",
    "\n",
    "                if b.shape[1] > 20:\n",
    "                    seed = b[0,:20]\n",
    "                else:\n",
    "                    seed = b[0, :]\n",
    "\n",
    "                seed = np.insert(seed, 0, 1)\n",
    "                gen = generate_seq(model, seed,  60, temperature=temp)\n",
    "\n",
    "                print('*** [', decode(seed), '] ', decode(gen[len(seed):]))\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(folder_path+\"weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_path+\"model.json\",'w') as f:\n",
    "    f.write(str(model.to_json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## Data Pre-processing Note\n",
    "\n",
    "### Genism\n",
    "https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html uses [WikiCorpus of gensim](https://radimrehurek.com/gensim/corpora/wikicorpus.html). I wasn't able to find a way to preseve line ending and that sucks.\n",
    "\n",
    "### wikiextractor\n",
    "Using a modified version of https://tiefenauer.github.io/blog/wiki-n-gram-lm/. Uses https://github.com/attardi/wikiextractor in its first step followed by a bash and weird script.\n",
    "The script being\n",
    "\n",
    "```\n",
    "result=$(find ./cleaned_wiki/ -name '*bz2' -exec bzcat {} \\+ \\\n",
    "        | pv \\\n",
    "        | tee >(    sed 's/<[^>]*>//g' \\\n",
    "                  | sed 's|[\"'\\''„“‚‘]||g' \\\n",
    "                  | python3 ./wiki_cleaner2.py es >> wiki_es2.txt \\\n",
    "               ) \\\n",
    "        | grep -e \"<doc\" \\\n",
    "        | wc -l);\n",
    "\n",
    "```\n",
    "\n",
    "## news\n",
    "\n",
    "cat news.es.all | ../normalize-punctuation.perl -l es | ../scripts/tokenizer.perl -l es -no-escape -threads 40 > new.es.all.tok\n",
    "cat news*es.shuffled > news.es.all \n",
    "\n",
    "\n",
    "## Making Vocab\n",
    "\n",
    "```\n",
    "from gensim.corpora import WikiCorpus\n",
    "wiki = WikiCorpus('datasets/wiki_dataset/raw/eswiki-latest-pages-articles-multistream.xml.bz2')\n",
    "from gensim.corpora import MmCorpus\n",
    "MmCorpus.serialize('datasets/wiki_dataset/wiki_es.mm', wiki)\n",
    "wiki.dictionary.save('datasets/wiki_dataset/wiki_es.vocab')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GiretTwoCell(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, cell_1 , cell_2 , nHidden , **kwargs):\n",
    "        self.cell_1 = cell_1\n",
    "        self.cell_2 = cell_2\n",
    "        self.nHidden = nHidden\n",
    "        self.state_size = [nHidden,nHidden]\n",
    "        super(GiretTwoCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        nHidden = self.nHidden\n",
    "        \n",
    "        input_shape_n = ( input_shape[0] , input_shape[1]- 2 )\n",
    "#         print \"pp\", input_shape_n\n",
    "        \n",
    "#         self.cell_1.build(input_shape_n)\n",
    "#         self.cell_2.build(input_shape_n)\n",
    "        \n",
    "        self._trainable_weights += ( self.cell_1.trainable_weights )\n",
    "        self._trainable_weights += ( self.cell_2.trainable_weights )\n",
    "        \n",
    "        self._non_trainable_weights += (  self.cell_1.non_trainable_weights )\n",
    "        self._non_trainable_weights += (  self.cell_2.non_trainable_weights )\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        \n",
    "        nHidden = self.nHidden\n",
    "        \n",
    "        gate_val_1 = inputs[ : , 0:1]\n",
    "        gate_val_2 = inputs[ : , 1:2]\n",
    "        \n",
    "        inputs  = inputs[ : , 2: ]\n",
    "                \n",
    "        gate_val_1 = K.repeat_elements(gate_val_1 , nHidden , -1 ) # shape # bs , hidden\n",
    "        gate_val_2 = K.repeat_elements(gate_val_2 , nHidden , -1 ) # shape # bs , hidden\n",
    "        \n",
    "        _ , [h1 , c1 ]  = self.cell_1.call( inputs , states )\n",
    "        _ , [h2 , c2 ]  = self.cell_2.call( inputs , states )\n",
    "        \n",
    "        h = gate_val_1*h1 + gate_val_2*h2  + (1 - gate_val_1 -  gate_val_2 )*states[0]\n",
    "        c = gate_val_1*c1 + gate_val_2*c2  + (1 - gate_val_1 -  gate_val_2 )*states[1]\n",
    "        \n",
    "        return h, [h , c ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size, embedding_size, mask_zero=True)\n",
    "\n",
    "rnn_en = LSTM(hidden, return_sequences=True)\n",
    "rnn_hi = LSTM(hidden , return_sequences=True)\n",
    "\n",
    "       \n",
    "# en\n",
    "inp_en = Input((None , ))\n",
    "x = embed(inp_en)\n",
    "x = rnn_en(x)\n",
    "out_en = TimeDistributed(Dense(vocab_size, activation='softmax'))(x)\n",
    "\n",
    "\n",
    "# hi\n",
    "inp_hi = Input((None, ))\n",
    "x = embed(inp_hi)\n",
    "x = rnn_hi( x )\n",
    "out_hi = TimeDistributed(Dense(vocab_size, activation='softmax'))(x)\n",
    "\n",
    "\n",
    "cell_combined = GiretTwoCell(rnn_hi.cell , rnn_en.cell , hidden)\n",
    "\n",
    "        \n",
    "inp_enhi = Input((None, ))\n",
    "x = embed(inp_enhi )\n",
    "\n",
    "x_att = x\n",
    "x_att = Bidirectional(LSTM(32 , return_sequences=True))( x )\n",
    "bider_h = x_att \n",
    "x_att = TimeDistributed( Dense(3, activation='softmax') )(x_att)\n",
    "x_att = Lambda(lambda x : x[... , 1: ])(x_att)\n",
    "\n",
    "x = Concatenate(-1)([x_att , x ])\n",
    "\n",
    "x =  RNN(cell_combined , return_sequences=True )( x )\n",
    "out_enhi = TimeDistributed(Dense( vocab_size , activation='softmax'))(x)\n",
    "        \n",
    "model = Model( [inp_hi , inp_en , inp_enhi  ] , [ out_hi , out_en , out_enhi ] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
