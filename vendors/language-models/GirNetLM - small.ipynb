{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/2f/168da118beb6eef637e5f5af955a017a0bf83cff496832aa5a6b24bb01c5/sentencepiece-0.1.81-cp35-cp35m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.0MB 1.9MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.81\n"
     ]
    }
   ],
   "source": [
    "# installation\n",
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab Size choice\n",
    "\n",
    "- BERT uses a vocab of 30k wordpieces for English and 110k for 102 language model\n",
    "- FB models use a vocab size of 40k-50k BPE (FB paper: https://arxiv.org/pdf/1811.01136.pdf) \n",
    "- From sentence piece repo, I'm gonna use: https://github.com/google/sentencepiece/blob/master/doc/experiments.md, 8k to accomodate for two differnet languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "# spm.SentencePieceTrainer.Train(\n",
    "#     '--input=./datasets/wmt11/en_es.txt \\\n",
    "#     --model_prefix=en_es_txt_word_piece \\\n",
    "#      --bos_id=2 --eos_id=3 --unk_id=4 --pad_id=0 \\\n",
    "#     --vocab_size=8000')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input=./datasets/wiki_dataset/en_es.txt \\\n",
    "    --model_prefix=wiki_en_es_txt_word_piece \\\n",
    "     --bos_id=2 --eos_id=3 --unk_id=4 --pad_id=0 \\\n",
    "    --vocab_size=20000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"wiki_en_es_txt_word_piece.model\")\n",
    "sp.SetEncodeExtraOptions(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for deatils take a look at: https://github.com/google/sentencepiece/blob/master/python/README.md\n",
    "print(sp.EncodeAsPieces(\"This is a test, what?.... lolol lolol\"))\n",
    "print(sp.EncodeAsIds(\"This is a test, what?.... lolol lolol\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(sent): \n",
    "    return sp.EncodeAsIds(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_name, maxlen, tokenise=tokenise, pool=None):\n",
    "    with open(file_name) as f:\n",
    "        sentences = f.read().split(\"\\n\")\n",
    "    print(\"no of sentencecs: \", len(sentences))\n",
    "    answer = None\n",
    "    if pool is None:\n",
    "        pool = multiprocessing.Pool(processes=40)\n",
    "        answer = pool.map(tokenise, (sentence for sentence in sentences))\n",
    "        pool.close()\n",
    "    else:\n",
    "        answer = pool.map(tokenise, (sentence for sentence in sentences))\n",
    "    answer = pad_sequences(answer, maxlen=maxlen, dtype='int32', padding='post', truncating='post', value=0)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of sentencecs:  2015441\n",
      "no of sentencecs:  1927758\n",
      "no of sentencecs:  39317\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool(processes=40)\n",
    "sentence_en = process_file('./datasets/wmt11/training-monolingual/europarl-v6.en', max_len, pool=pool)\n",
    "sentence_es = process_file('./datasets/wmt11/training-monolingual/europarl-v6.es', max_len, pool=pool)\n",
    "sentence_es_en = process_file('./datasets/wmt11/code_mixed_es_en.txt.tok', max_len, pool=pool)\n",
    "pool.close()\n",
    "del pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 128\n",
    "numwords = len(sp)\n",
    "hidden_emd_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_loss(y_true, y_pred):\n",
    "    return K.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "class GiretTwoCell(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, cell_1 , cell_2 , nHidden , **kwargs):\n",
    "        self.cell_1 = cell_1\n",
    "        self.cell_2 = cell_2\n",
    "        self.nHidden = nHidden\n",
    "        self.state_size = [nHidden,nHidden]\n",
    "        super(GiretTwoCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        nHidden = self.nHidden\n",
    "        \n",
    "        input_shape_n = ( input_shape[0] , input_shape[1]- 2 )\n",
    "#         print \"pp\", input_shape_n\n",
    "        \n",
    "#         self.cell_1.build(input_shape_n)\n",
    "#         self.cell_2.build(input_shape_n)\n",
    "        \n",
    "        self._trainable_weights += ( self.cell_1.trainable_weights )\n",
    "        self._trainable_weights += ( self.cell_2.trainable_weights )\n",
    "        \n",
    "        self._non_trainable_weights += (  self.cell_1.non_trainable_weights )\n",
    "        self._non_trainable_weights += (  self.cell_2.non_trainable_weights )\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        \n",
    "        nHidden = self.nHidden\n",
    "        \n",
    "        gate_val_1 = inputs[ : , 0:1]\n",
    "        gate_val_2 = inputs[ : , 1:2]\n",
    "        \n",
    "        inputs  = inputs[ : , 2: ]\n",
    "                \n",
    "        gate_val_1 = K.repeat_elements(gate_val_1 , nHidden , -1 ) # shape # bs , hidden\n",
    "        gate_val_2 = K.repeat_elements(gate_val_2 , nHidden , -1 ) # shape # bs , hidden\n",
    "        \n",
    "        _ , [h1 , c1 ]  = self.cell_1.call( inputs , states )\n",
    "        _ , [h2 , c2 ]  = self.cell_2.call( inputs , states )\n",
    "        \n",
    "        h = gate_val_1*h1 + gate_val_2*h2  + (1 - gate_val_1 -  gate_val_2 )*states[0]\n",
    "        c = gate_val_1*c1 + gate_val_2*c2  + (1 - gate_val_1 -  gate_val_2 )*states[1]\n",
    "        \n",
    "        return h, [h , c ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(numwords, hidden_emd_dim)\n",
    "\n",
    "rnn_en = LSTM(hidden, return_sequences=True)\n",
    "rnn_hi = LSTM(hidden , return_sequences=True)\n",
    "\n",
    "       \n",
    "# en\n",
    "inp_en = Input((None, ))\n",
    "x = embed(inp_en)\n",
    "x = rnn_en(x)\n",
    "out_en = TimeDistributed(Dense(numwords, activation='linear'), name='en')(x)\n",
    "\n",
    "\n",
    "# es\n",
    "inp_hi = Input((None, ))\n",
    "x = embed(inp_hi)\n",
    "x = rnn_hi( x )\n",
    "out_hi = TimeDistributed(Dense(numwords, activation='linear'), name='es')(x)\n",
    "\n",
    "\n",
    "cell_combined = GiretTwoCell(rnn_hi.cell , rnn_en.cell , hidden)\n",
    "\n",
    "        \n",
    "inp_enhi = Input((None, ))\n",
    "x = embed(inp_enhi )\n",
    "\n",
    "x_att = x\n",
    "x_att = Bidirectional(LSTM(32 , return_sequences=True))( x )\n",
    "bider_h = x_att \n",
    "x_att = TimeDistributed(Dense(3, activation='softmax') )(x_att)\n",
    "x_att = Lambda(lambda x : x[... , 1: ])(x_att)\n",
    "\n",
    "x = Concatenate(-1)([x_att , x ])\n",
    "\n",
    "x =  RNN(cell_combined , return_sequences=True)(x)\n",
    "out_enhi = TimeDistributed(Dense(numwords , activation='linear'), name='en_es')(x)\n",
    "        \n",
    "model = Model( [inp_hi , inp_en , inp_enhi  ] , [ out_hi , out_en , out_enhi ] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.01, clipvalue=0.4)\n",
    "lss = sparse_loss\n",
    "\n",
    "model.compile(loss=sparse_loss, optimizer=opt)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit( \n",
    "    [sentence_en[:n,:-1], sentence_en[:n,:-1], es_en_c[:n,:-1]],\n",
    "    [sentence_en[0:n,1:], sentence_es[:n,1:], es_en_c[:n,1:]], \n",
    "    batch_size=8, \n",
    "    epochs=1, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[tb],\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_logits(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        return np.argmax(preds)\n",
    "\n",
    "    preds = preds / temperature\n",
    "    preds = preds - logsumexp(preds)\n",
    "\n",
    "    choice = np.random.choice(len(preds), 1, p=np.exp(preds))\n",
    "\n",
    "    return choice\n",
    "def generate_seq(model : Model, seed, size, out_num=3, temperature=1.0):\n",
    "\n",
    "    ls = seed.shape[0]\n",
    "\n",
    "    # Due to the way Keras RNNs work, we feed the model a complete sequence each time. At first it's just the seed,\n",
    "    # zero-padded to the right length. With each iteration we sample and set the next character.\n",
    "    \n",
    "    # tokens = np.concatenate([seed, np.zeros(size - ls)])\n",
    "    tokens_all = []\n",
    "    for i in range(out_num):\n",
    "        tokens_all.append(np.concatenate([seed, np.zeros(size - ls)]))\n",
    "\n",
    "    for i in range(ls, size):\n",
    "        \n",
    "        tokens_to_predict = []\n",
    "        for j in range(out_num):\n",
    "            tokens_to_predict.append(tokens_all[j][None,:])\n",
    "        \n",
    "        all_probs = model.predict(tokens_to_predict)\n",
    "\n",
    "        # Extract the i-th probability vector and sample an index from it\n",
    "        for j, probs in enumerate(all_probs):\n",
    "            next_token = util.sample_logits(probs[0, i-1, :], temperature=temperature)\n",
    "            tokens_all[j][i] = next_token\n",
    "\n",
    "    return [tokens.astype('int') for tokens in tokens_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = sentence_en[122][:4]\n",
    "a = generate_seq(model, seed, 50, out_num=3, temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2id = {'contradiction':0, 'entailment':1, 'neutral':2}\n",
    "df = pd.read_json(\"./MultiNLO/XNLI-1.0/xnli.dev.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [df['sentence1'].to_numpy(), df['sentence2'].to_numpy()]\n",
    "y = df['gold_label'].to_numpy()\n",
    "for i in range(len(y)):\n",
    "    y[i] = class2id[y[i]]\n",
    "y = to_categorical(y, num_classes=3, dtype='int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(3, input_shape=(256*2,), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
