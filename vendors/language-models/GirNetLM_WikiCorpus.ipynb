{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "from words import *\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import *\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "vocab_size = 100000\n",
    "max_length = 64\n",
    "batch_size = 8\n",
    "embedding_size = 50\n",
    "hidden = 50\n",
    "input_file = './datasets/wiki_dataset/wiki_en.txt' # wiki_es.txt is the other file\n",
    "\n",
    "# loging info\n",
    "data_dir = './dumps/'\n",
    "experiment_name ='en_en_MUSE2'\n",
    "extra_tokens = {'<PAD>':4, '<START>':2, '<UNK>':1, '<EOS>':3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = data_dir+experiment_name+\"./\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011141\n",
      "2013873\n"
     ]
    }
   ],
   "source": [
    "en_dict = Dictionary.load('./datasets/wiki_dataset/wiki_en.vocab')\n",
    "print(len(en_dict.token2id))\n",
    "es_dict = Dictionary.load('./datasets/wiki_dataset/wiki_es.vocab')\n",
    "print(len(es_dict.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict.merge_with(es_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3194833\n",
      "2013873\n"
     ]
    }
   ],
   "source": [
    "print(len(en_dict.token2id))\n",
    "print(len(es_dict.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54058"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dict = en_dict\n",
    "combined_dict.filter_extremes(keep_n=vocab_size, keep_tokens=None)\n",
    "combined_dict.patch_with_special_tokens(extra_tokens)\n",
    "print(len(combined_dict.token2id))\n",
    "combined_dict.token2id['lol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict.save('datasets/wiki_dataset/combined_vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict = Dictionary.load('datasets/wiki_dataset/combined_vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file) as f:\n",
    "    sentences = f.read().split(\"\\n\")\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def each(sentence):\n",
    "    x_ = combined_dict.doc2idx(text_to_word_sequence(sentence), unknown_word_index=combined_dict.token2id['<UNK>'])\n",
    "    x_.append(combined_dict.token2id['<EOS>'])\n",
    "    return sequence.pad_sequences([x_], maxlen=max_length, dtype='int32', padding='post', truncating='post',value=combined_dict.token2id['<PAD>'])[0]\n",
    "each(sentences[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why can't it share its memory. Use joblib or something thread base if RAM is a bottle-neck\n",
    "pool = multiprocessing.Pool(processes=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pool.map(each, (sentence for sentence in sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 28.6 s, total: 28.6 s\n",
      "Wall time: 31.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.save('datasets/wiki_dataset/wiki_en_100004_vocab.npy', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102435443, 64)\n",
      "(22362530, 64)\n",
      "(36866, 64)\n"
     ]
    }
   ],
   "source": [
    "x_en = np.load('datasets/wiki_dataset/wiki_en_100004_vocab.npy')\n",
    "print(x_en.shape)\n",
    "x_es = np.load('datasets/wiki_dataset/wiki_es_100004_vocab.npy')\n",
    "print(x_es.shape)\n",
    "x_en_es = np.load('datasets/wiki_dataset/twitter_en_es_100004_vocab.npy')\n",
    "print(x_en_es.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_en_es_combined = np.concatenate([x_en_es for _ in range(min(x_es.shape[0],x_en.shape[0])//x_en_es.shape[0] -1)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numwords = len(combined_dict.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_x(x):\n",
    "    batches = []\n",
    "    for i in range(1,len(x)-batch_size,batch_size):\n",
    "        batches.append(x[i:i+batch_size])\n",
    "    batches = np.array(batches)\n",
    "    print(batches.shape)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(x_en)\n",
    "np.random.shuffle(x_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(x_en_es_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_en_batched = batch_x(x_en)\n",
    "# x_es_batched = batch_x(x_es)\n",
    "# x_en_es_batched = batch_x(x_en_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(seq):\n",
    "    return ' '.join(combined_dict[id_] for id_ in seq)\n",
    "# print('Finished loading. ', sum([b.shape[0] for b in batches]), ' sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MUSE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## essential functions\n",
    "def load_vec(emb_path, nmax=50000):\n",
    "    word2id = {'<pad>':0, '<unk>':1, '<sos>':2, '<eos>':3}\n",
    "    vectors = [np.zeros(300) for _ in range(len(word2id))]\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = '../MUSE/dumped/6pzywzu6yg/vectors-en.txt'\n",
    "tgt_path = '..//MUSE/dumped/6pzywzu6yg/vectors-es.txt'\n",
    "nmax = 100000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_embeddings(src_embeddings, tgt_embeddings):\n",
    "    \n",
    "    # make combined embedding mattrix\n",
    "    embedding_matrix = src_embeddings.copy().tolist()\n",
    "    embedding_matrix.extend(tgt_embeddings.tolist())\n",
    "    embedding_matrix = np.array(embedding_matrix)\n",
    "    \n",
    "    # make combined id2word and word2id\n",
    "    id2word = src_id2word.copy()\n",
    "    word2id = src_word2id.copy()\n",
    "    \n",
    "    next_id = len(id2word.keys())\n",
    "    counter = len(id2word.keys())\n",
    "    \n",
    "    to_be_removed_id = []\n",
    "    common_words = []\n",
    "    \n",
    "    for key in tgt_id2word:\n",
    "        if tgt_id2word[key] in word2id:\n",
    "            to_be_removed_id.append(counter)\n",
    "            common_words.append(tgt_id2word[key])\n",
    "            embedding_matrix[word2id[tgt_id2word[key]]] =  (embedding_matrix[word2id[tgt_id2word[key]]] + embedding_matrix[counter])/2\n",
    "        else:\n",
    "            id2word[next_id] = tgt_id2word[key]\n",
    "            word2id[tgt_id2word[key]] = next_id\n",
    "            next_id += 1\n",
    "        counter += 1\n",
    "        \n",
    "    embedding_matrix = np.delete(embedding_matrix, to_be_removed_id, axis=0)\n",
    "        \n",
    "    return embedding_matrix, id2word, word2id, common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, id2word, word2id, common_words = merge_embeddings(src_embeddings, tgt_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size:  (161829, 300)\n",
      "Number of common words in both the embedding 38171\n"
     ]
    }
   ],
   "source": [
    "print(\"embedding size: \", str(embedding_matrix.shape))\n",
    "print(\"Number of common words in both the embedding %d\" % len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(folder_path+\"vocab\", \"w\") as f:\n",
    "        json.dump(word2id,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(seq):\n",
    "    return ' '.join(id2word[id_] for id_ in seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 13989, 30, 17, 36, 3829, 17, 25, 1, 123, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def each(sentence, max_length=32, tokeniser=text_to_word_sequence):\n",
    "    sentence =  tokeniser(sentence)\n",
    "    new_sent = [word2id['<sos>']] \n",
    "    for word in sentence:\n",
    "        try:\n",
    "            new_sent.append(word2id[word])\n",
    "        except:\n",
    "            new_sent.append(word2id['<unk>'])\n",
    "    \n",
    "    new_sent.append(word2id['<eos>']) \n",
    "    if len(new_sent) > max_length:\n",
    "        new_sent = new_sent[:max_length]\n",
    "    else:\n",
    "        while len(new_sent) < max_length:\n",
    "            new_sent.append(word2id['<pad>'])\n",
    "    return new_sent\n",
    "each(\"Lol this is not fun is it ? hahh what\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_name, doc2ids=each, pool=None):\n",
    "    with open(file_name) as f:\n",
    "        sentences = f.read().split(\"\\n\")\n",
    "    print(\"no of sentencecs: \", len(sentences))\n",
    "    answer = None\n",
    "    if pool is None:\n",
    "        pool = multiprocessing.Pool(processes=40)\n",
    "        answer = pool.map(doc2ids, (sentence for sentence in sentences))\n",
    "        pool.close()\n",
    "    else:\n",
    "        answer = pool.map(doc2ids, (sentence for sentence in sentences))\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EUROPAL FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of sentencecs:  2015441\n",
      "no of sentencecs:  1927758\n",
      "no of sentencecs:  39317\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool(processes=40)\n",
    "sentence_en = process_file('./datasets/wmt11/training-monolingual/europarl-v6.en',pool=pool)\n",
    "sentence_es = process_file('./datasets/wmt11/training-monolingual/europarl-v6.es',pool=pool)\n",
    "sentence_es_en = process_file('./datasets/wmt11/code_mixed_es_en.txt.tok',pool=pool)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WiKi Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2015441, 32)\n",
      "(1927758, 32)\n",
      "(39317, 32)\n"
     ]
    }
   ],
   "source": [
    "sentence_en = np.array(sentence_en)\n",
    "print(sentence_en.shape)\n",
    "sentence_es = np.array(sentence_es)\n",
    "print(sentence_es.shape)\n",
    "sentence_es_en = np.array(sentence_es_en)\n",
    "print(sentence_es_en.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1887216, 32)\n"
     ]
    }
   ],
   "source": [
    "es_en_c = np.concatenate([sentence_es_en for _ in range(min(sentence_en.shape[0],sentence_es.shape[0])//sentence_es_en.shape[0] -1)], axis=0)\n",
    "print(es_en_c.shape)\n",
    "n = es_en_c.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ = Input(shape=(None, ))\n",
    "# embedding = Embedding(numwords, 100, input_length=None)\n",
    "# embedded = embedding(input_)\n",
    "\n",
    "# decoder_lstm = LSTM(100, return_sequences=True)\n",
    "# h = decoder_lstm(embedded)\n",
    "\n",
    "# fromhidden = Dense(numwords, activation='linear')\n",
    "# out = TimeDistributed(fromhidden)(h)\n",
    "\n",
    "# model = Model(input_, out)\n",
    "\n",
    "# opt = keras.optimizers.Adam()\n",
    "# lss = sparse_loss\n",
    "\n",
    "# model.compile(opt, lss)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GiretTwoCell(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, cell_1 , cell_2 , nHidden , **kwargs):\n",
    "        self.cell_1 = cell_1\n",
    "        self.cell_2 = cell_2\n",
    "        self.nHidden = nHidden\n",
    "        self.state_size = [nHidden,nHidden]\n",
    "        super(GiretTwoCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        nHidden = self.nHidden\n",
    "        \n",
    "        input_shape_n = ( input_shape[0] , input_shape[1]- 2 )\n",
    "#         print \"pp\", input_shape_n\n",
    "        \n",
    "#         self.cell_1.build(input_shape_n)\n",
    "#         self.cell_2.build(input_shape_n)\n",
    "        \n",
    "        self._trainable_weights += ( self.cell_1.trainable_weights )\n",
    "        self._trainable_weights += ( self.cell_2.trainable_weights )\n",
    "        \n",
    "        self._non_trainable_weights += (  self.cell_1.non_trainable_weights )\n",
    "        self._non_trainable_weights += (  self.cell_2.non_trainable_weights )\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        \n",
    "        nHidden = self.nHidden\n",
    "        \n",
    "        gate_val_1 = inputs[ : , 0:1]\n",
    "        gate_val_2 = inputs[ : , 1:2]\n",
    "        \n",
    "        inputs  = inputs[ : , 2: ]\n",
    "                \n",
    "        gate_val_1 = K.repeat_elements(gate_val_1 , nHidden , -1 ) # shape # bs , hidden\n",
    "        gate_val_2 = K.repeat_elements(gate_val_2 , nHidden , -1 ) # shape # bs , hidden\n",
    "        \n",
    "        _ , [h1 , c1 ]  = self.cell_1.call( inputs , states )\n",
    "        _ , [h2 , c2 ]  = self.cell_2.call( inputs , states )\n",
    "        \n",
    "        h = gate_val_1*h1 + gate_val_2*h2  + (1 - gate_val_1 -  gate_val_2 )*states[0]\n",
    "        c = gate_val_1*c1 + gate_val_2*c2  + (1 - gate_val_1 -  gate_val_2 )*states[1]\n",
    "        \n",
    "        return h, [h , c ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "hidden = 256\n",
    "numwords = len(word2id)\n",
    "hidden_emd_dim = 300\n",
    "embed = Embedding(numwords,\n",
    "                     hidden_emd_dim,\n",
    "                     weights=[embedding_matrix],trainable=True)\n",
    "\n",
    "rnn_en = LSTM(hidden, return_sequences=True, name='en_lstm', recurrent_dropout=0.3, dropout=0.3)\n",
    "rnn_hi = LSTM(hidden , return_sequences=True, name='es_lstm', recurrent_dropout=0.3, dropout=0.3)\n",
    "\n",
    "       \n",
    "# en\n",
    "inp_en = Input((None, ))\n",
    "x = embed(inp_en)\n",
    "x = rnn_en(x)\n",
    "out_en = TimeDistributed(Dense(numwords, activation='linear'), name='en')(x)\n",
    "\n",
    "\n",
    "# es\n",
    "inp_hi = Input((None, ))\n",
    "x = embed(inp_hi)\n",
    "x = rnn_hi( x )\n",
    "out_hi = TimeDistributed(Dense(numwords, activation='linear'), name='es')(x)\n",
    "\n",
    "\n",
    "cell_combined = GiretTwoCell(rnn_hi.cell , rnn_en.cell , hidden)\n",
    "\n",
    "        \n",
    "inp_enhi = Input((None, ))\n",
    "x = embed(inp_enhi )\n",
    "\n",
    "x_att = x\n",
    "x_att = Bidirectional(LSTM(32 , return_sequences=True, recurrent_dropout=0.3, dropout=0.3))( x )\n",
    "bider_h = x_att \n",
    "x_att = TimeDistributed(Dense(3, activation='softmax') )(x_att)\n",
    "x_att = Lambda(lambda x : x[... , 1: ])(x_att)\n",
    "\n",
    "x = Concatenate(-1)([x_att , x ])\n",
    "\n",
    "x =  RNN(cell_combined , return_sequences=True)(x)\n",
    "out_enhi = TimeDistributed(Dense(numwords , activation='linear'), name='en_es')(x)\n",
    "        \n",
    "model = Model( [inp_hi , inp_en , inp_enhi  ] , [ out_hi , out_en , out_enhi ] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    48548700    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 64)     85248       embedding_1[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 3)      195         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 2)      0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 302)    0           lambda_1[0][0]                   \n",
      "                                                                 embedding_1[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "es_lstm (LSTM)                  (None, None, 256)    570368      embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "en_lstm (LSTM)                  (None, None, 256)    570368      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "rnn_1 (RNN)                     (None, None, 256)    1140736     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "es (TimeDistributed)            (None, None, 161829) 41590053    es_lstm[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "en (TimeDistributed)            (None, None, 161829) 41590053    en_lstm[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "en_es (TimeDistributed)         (None, None, 161829) 41590053    rnn_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 174,545,038\n",
      "Trainable params: 174,545,038\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam()\n",
    "lss = sparse_loss\n",
    "\n",
    "model.compile(loss=sparse_loss, optimizer=opt)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = keras.callbacks.TensorBoard(log_dir='./logsMUSE2', histogram_freq=0, batch_size=8, write_graph=True, write_grads=True, write_images=True, update_freq='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(folder_path+\"weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(batch_size=8):\n",
    "    b = batch_size\n",
    "    while True:\n",
    "        n1 = np.random.randint(0, sentence_en.shape[0] - batch_size, batch_size)\n",
    "        n2 = np.random.randint(0, sentence_es.shape[0] - batch_size, batch_size)\n",
    "    \n",
    "        es_en_temp = None\n",
    "        if random.random() <= 0.6:\n",
    "            es_en_temp = sentence_es_en\n",
    "        elif random.random() <= 0.5:\n",
    "            es_en_temp = sentence_en\n",
    "        else:\n",
    "            es_en_temp = sentence_es\n",
    "            \n",
    "        n3 = np.random.randint(0, es_en_temp.shape[0] - batch_size, batch_size)\n",
    "            \n",
    "    \n",
    "        x = [sentence_en[n1,:-1], sentence_es[n2,:-1], es_en_temp[n3,:-1]]\n",
    "        y = [sentence_en[n1,1:], sentence_es[n2,1:], es_en_temp[n3,1:]]\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 16/20\n",
      "4914/4914 [==============================] - 2817s 573ms/step - loss: 7.7762 - es_loss: 3.1307 - en_loss: 3.1079 - en_es_loss: 1.5375\n",
      "Epoch 17/20\n",
      "4914/4914 [==============================] - 2822s 574ms/step - loss: 7.7154 - es_loss: 3.1216 - en_loss: 3.1064 - en_es_loss: 1.4874\n",
      "Epoch 18/20\n",
      "1686/4914 [=========>....................] - ETA: 30:50 - loss: 7.6661 - es_loss: 3.1101 - en_loss: 3.1007 - en_es_loss: 1.4553"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4914/4914 [==============================] - 2813s 573ms/step - loss: 7.6693 - es_loss: 3.1122 - en_loss: 3.0984 - en_es_loss: 1.4587\n",
      "Epoch 19/20\n",
      "4914/4914 [==============================] - 2845s 579ms/step - loss: 7.6028 - es_loss: 3.0870 - en_loss: 3.0658 - en_es_loss: 1.4499\n",
      "Epoch 20/20\n",
      "4914/4914 [==============================] - 2850s 580ms/step - loss: 7.5909 - es_loss: 3.0842 - en_loss: 3.0723 - en_es_loss: 1.4344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed9420e4a8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen = batch_generator(8)\n",
    "model.fit_generator(\n",
    "    generator=train_gen,\n",
    "    steps_per_epoch=sentence_es_en.shape[0] // 8, \n",
    "    epochs=20, \n",
    "    callbacks=[tb], \n",
    "    initial_epoch=15\n",
    ")\n",
    "\n",
    "# model.fit( \n",
    "#     [sentence_en[:n,:-1], sentence_en[:n,:-1], es_en_c[:n,:-1]],\n",
    "#     [sentence_en[0:n,1:], sentence_es[:n,1:], es_en_c[:n,1:]], \n",
    "#     batch_size=8, \n",
    "#     epochs=1, \n",
    "#     callbacks=[tb],\n",
    "#     shuffle=True\n",
    "# )\n",
    "# model.fit(\n",
    "#     [x_en[:n,:-1], x_es[:n,:-1]],\n",
    "#     [x_en[0:n,1:], x_es[:n,1:]]1, \n",
    "#     batch_size=16, \n",
    "#     epochs=1, \n",
    "#     validation_split=0.1,\n",
    "#     callbacks=[tb],\n",
    "#     shuffle=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(folder_path+\"weights\")\n",
    "with open(folder_path+\"model.json\",'w') as f:\n",
    "    f.write(str(model.to_json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "- a general statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentEval\n",
    "https://github.com/facebookresearch/SentEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install torchvision\n",
    "# ! pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def each(sentence, max_length=32, tokeniser=text_to_word_sequence):\n",
    "#     sentence =  tokeniser(sentence)\n",
    "    new_sent = [word2id['<sos>']] \n",
    "    for word in sentence:\n",
    "        try:\n",
    "            new_sent.append(word2id[word])\n",
    "        except:\n",
    "            new_sent.append(word2id['<unk>'])\n",
    "    \n",
    "    new_sent.append(word2id['<eos>']) \n",
    "    if len(new_sent) > max_length:\n",
    "        new_sent = new_sent[:max_length]\n",
    "    else:\n",
    "        while len(new_sent) < max_length:\n",
    "            new_sent.append(word2id['<pad>'])\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_embedding_model = Model(inputs=model.input, outputs=model.get_layer('rnn_1').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_embeddings(sents):\n",
    "    sents = [each(sent) for sent in sents] \n",
    "    return np.mean(sent_embedding_model.predict([sents,sents,sents], batch_size=64),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set PATHs\n",
    "PATH_TO_SENTEVAL = './SentEval/'\n",
    "PATH_TO_DATA = './SentEval/data/'\n",
    "\n",
    "# import SentEval\n",
    "import sys\n",
    "sys.path.insert(0, PATH_TO_SENTEVAL)\n",
    "import senteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentEval prepare and batcher\n",
    "def prepare(params, samples):\n",
    "#     print(samples[0])\n",
    "    return\n",
    "\n",
    "def batcher(params, batch):\n",
    "    return get_sent_embeddings(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set params for SentEval\n",
    "params_senteval = {'task_path': PATH_TO_DATA, 'usepytorch': False}\n",
    "params_senteval['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 32,\n",
    "                                 'tenacity': 3, 'epoch_size': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = senteval.engine.SE(params_senteval, batcher, prepare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer_tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16',\n",
    "#                       'MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'SST5', 'TREC', 'MRPC',\n",
    "#                       'SICKEntailment', 'SICKRelatedness', 'STSBenchmark',\n",
    "#                       'Length', 'WordContent', 'Depth', 'TopConstituents',\n",
    "#                       'BigramShift', 'Tense', 'SubjNumber', 'ObjNumber',\n",
    "#                       'OddManOut', 'CoordinationInversion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_tasks = ['TREC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'TREC': {'acc': 67.8, 'devacc': 58.73, 'ndev': 5452, 'ntest': 500}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = se.eval(transfer_tasks)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MR': {'acc': 65.61, 'devacc': 65.59, 'ndev': 10662, 'ntest': 10662},\n",
       " 'SICKEntailment': {'acc': 72.74, 'devacc': 73.8, 'ndev': 500, 'ntest': 4927},\n",
       " 'SICKRelatedness': {'devpearson': 0.6288833057276259,\n",
       "  'mse': 0.6256310523376608,\n",
       "  'ndev': 500,\n",
       "  'ntest': 4927,\n",
       "  'pearson': 0.6283092600845445,\n",
       "  'spearman': 0.5837184862936838,\n",
       "  'yhat': array([3.67965652, 3.8552741 , 1.12607261, ..., 3.38653953, 3.89661515,\n",
       "         4.70715554])},\n",
       " 'STS12': {'MSRpar': {'nsamples': 750,\n",
       "   'pearson': (0.14258749810529842, 8.912221970582458e-05),\n",
       "   'spearman': SpearmanrResult(correlation=0.15110999162593566, pvalue=3.248632461976632e-05)},\n",
       "  'MSRvid': {'nsamples': 750,\n",
       "   'pearson': (0.4007830218587774, 2.6189184388944674e-30),\n",
       "   'spearman': SpearmanrResult(correlation=0.42664203634904674, pvalue=1.5750695641919273e-34)},\n",
       "  'SMTeuroparl': {'nsamples': 459,\n",
       "   'pearson': (0.3271387330066187, 6.558578218961659e-13),\n",
       "   'spearman': SpearmanrResult(correlation=0.44585313264930493, pvalue=8.423250166894389e-24)},\n",
       "  'all': {'pearson': {'mean': 0.2958716812971679, 'wmean': 0.3028944887220307},\n",
       "   'spearman': {'mean': 0.3631602533857541, 'wmean': 0.36154137754353355}},\n",
       "  'surprise.OnWN': {'nsamples': 750,\n",
       "   'pearson': (0.4010845876707513, 2.349634411380406e-30),\n",
       "   'spearman': SpearmanrResult(correlation=0.483258535961669, pvalue=3.785359493165831e-45)},\n",
       "  'surprise.SMTnews': {'nsamples': 399,\n",
       "   'pearson': (0.2077645658443937, 2.8793575626173877e-05),\n",
       "   'spearman': SpearmanrResult(correlation=0.30893757034281394, pvalue=2.852474352467182e-10)}},\n",
       " 'STS13': {'FNWN': {'nsamples': 189,\n",
       "   'pearson': (0.1796998687694542, 0.013351592089388827),\n",
       "   'spearman': SpearmanrResult(correlation=0.17312087397497752, pvalue=0.0172074164275323)},\n",
       "  'OnWN': {'nsamples': 561,\n",
       "   'pearson': (-0.036391665423154564, 0.38961542391537196),\n",
       "   'spearman': SpearmanrResult(correlation=-0.0064082359237449225, pvalue=0.8796247798086678)},\n",
       "  'all': {'pearson': {'mean': 0.1506315417021541,\n",
       "    'wmean': 0.16332491147677275},\n",
       "   'spearman': {'mean': 0.15573074065761325, 'wmean': 0.1696563418461701}},\n",
       "  'headlines': {'nsamples': 750,\n",
       "   'pearson': (0.30858642176016265, 5.202433717123146e-18),\n",
       "   'spearman': SpearmanrResult(correlation=0.30047958392160706, pvalue=4.087419263459101e-17)}},\n",
       " 'STS14': {'OnWN': {'nsamples': 750,\n",
       "   'pearson': (0.13495201475831842, 0.00021004588531773239),\n",
       "   'spearman': SpearmanrResult(correlation=0.19518669302333042, pvalue=7.10871057435418e-08)},\n",
       "  'all': {'pearson': {'mean': 0.23573767456033798,\n",
       "    'wmean': 0.2615492418576007},\n",
       "   'spearman': {'mean': 0.2741601729177288, 'wmean': 0.292553611332301}},\n",
       "  'deft-forum': {'nsamples': 450,\n",
       "   'pearson': (0.156784942801156, 0.0008458976964767222),\n",
       "   'spearman': SpearmanrResult(correlation=0.18466330729597857, pvalue=8.136498539192537e-05)},\n",
       "  'deft-news': {'nsamples': 300,\n",
       "   'pearson': (0.07327643492260316, 0.20566038816830512),\n",
       "   'spearman': SpearmanrResult(correlation=0.18054609654412698, pvalue=0.0016898349432055404)},\n",
       "  'headlines': {'nsamples': 750,\n",
       "   'pearson': (0.26463587403452904, 1.7451281547738147e-13),\n",
       "   'spearman': SpearmanrResult(correlation=0.245732305648832, pvalue=8.895765565765384e-12)},\n",
       "  'images': {'nsamples': 750,\n",
       "   'pearson': (0.4466387586578476, 4.70476518566636e-38),\n",
       "   'spearman': SpearmanrResult(correlation=0.46627739473892865, pvalue=9.440809367485879e-42)},\n",
       "  'tweet-news': {'nsamples': 750,\n",
       "   'pearson': (0.33813802218757344, 1.6208217071906041e-21),\n",
       "   'spearman': SpearmanrResult(correlation=0.3725552402551761, pvalue=4.164134071189017e-26)}},\n",
       " 'STS15': {'all': {'pearson': {'mean': 0.2724451880320972,\n",
       "    'wmean': 0.31685872675093085},\n",
       "   'spearman': {'mean': 0.2882838350353617, 'wmean': 0.3327150079550635}},\n",
       "  'answers-forums': {'nsamples': 375,\n",
       "   'pearson': (0.039706422734896954, 0.4432914403188235),\n",
       "   'spearman': SpearmanrResult(correlation=0.04068902993621121, pvalue=0.43208160190150313)},\n",
       "  'answers-students': {'nsamples': 750,\n",
       "   'pearson': (0.28705987824827633, 1.0775132175080352e-15),\n",
       "   'spearman': SpearmanrResult(correlation=0.30579921985230657, pvalue=1.0645513662148402e-17)},\n",
       "  'belief': {'nsamples': 375,\n",
       "   'pearson': (0.14987564357862884, 0.0036248415408096798),\n",
       "   'spearman': SpearmanrResult(correlation=0.1804292567768978, pvalue=0.0004461019865200121)},\n",
       "  'headlines': {'nsamples': 750,\n",
       "   'pearson': (0.3818721603396246, 1.8961210851164007e-27),\n",
       "   'spearman': SpearmanrResult(correlation=0.3751911032601324, pvalue=1.7553466470148097e-26)},\n",
       "  'images': {'nsamples': 750,\n",
       "   'pearson': (0.5037118352590594, 1.6789112333633647e-49),\n",
       "   'spearman': SpearmanrResult(correlation=0.5393105653512604, pvalue=8.059509148311837e-58)}},\n",
       " 'STS16': {'all': {'pearson': {'mean': 0.3037491915973627,\n",
       "    'wmean': 0.31286456501095683},\n",
       "   'spearman': {'mean': 0.3748289458274702, 'wmean': 0.3827874432860581}},\n",
       "  'answer-answer': {'nsamples': 254,\n",
       "   'pearson': (0.33687184761736494, 3.7108326715175343e-08),\n",
       "   'spearman': SpearmanrResult(correlation=0.3881588272050183, pvalue=1.4670713101796024e-10)},\n",
       "  'headlines': {'nsamples': 249,\n",
       "   'pearson': (0.37639387167679833, 8.426999310924058e-10),\n",
       "   'spearman': SpearmanrResult(correlation=0.3761729728472852, pvalue=8.635789491144841e-10)},\n",
       "  'plagiarism': {'nsamples': 230,\n",
       "   'pearson': (0.32884426285695534, 3.3498363949391107e-07),\n",
       "   'spearman': SpearmanrResult(correlation=0.47740807184303585, pvalue=1.702272150989612e-14)},\n",
       "  'postediting': {'nsamples': 244,\n",
       "   'pearson': (0.4719643243971815, 6.13050772417937e-15),\n",
       "   'spearman': SpearmanrResult(correlation=0.5642863914496445, pvalue=6.455918799404908e-22)},\n",
       "  'question-question': {'nsamples': 209,\n",
       "   'pearson': (0.00467165143851349, 0.9464761414626301),\n",
       "   'spearman': SpearmanrResult(correlation=0.06811846579236705, pvalue=0.3270811925573087)}},\n",
       " 'STSBenchmark': {'devpearson': 0.5512091403813993,\n",
       "  'mse': 2.0149316274007862,\n",
       "  'ndev': 1500,\n",
       "  'ntest': 1379,\n",
       "  'pearson': 0.49412586914630235,\n",
       "  'spearman': 0.47672368959625055,\n",
       "  'yhat': array([2.34865983, 2.34609645, 2.41522208, ..., 3.80854183, 3.47457961,\n",
       "         3.54439748])}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{'MR': {'acc': 65.61, 'devacc': 65.59, 'ndev': 10662, 'ntest': 10662},\n",
    " 'SICKEntailment': {'acc': 72.74, 'devacc': 73.8, 'ndev': 500, 'ntest': 4927},\n",
    " 'SICKRelatedness': {'devpearson': 0.6288833057276259,\n",
    "  'mse': 0.6256310523376608,\n",
    "  'ndev': 500,\n",
    "  'ntest': 4927,\n",
    "  'pearson': 0.6283092600845445,\n",
    "  'spearman': 0.5837184862936838,\n",
    "  'yhat': array([3.67965652, 3.8552741 , 1.12607261, ..., 3.38653953, 3.89661515,\n",
    "         4.70715554])},\n",
    " 'STS12': {'MSRpar': {'nsamples': 750,\n",
    "   'pearson': (0.14258749810529842, 8.912221970582458e-05),\n",
    "   'spearman': SpearmanrResult(correlation=0.15110999162593566, pvalue=3.248632461976632e-05)},\n",
    "  'MSRvid': {'nsamples': 750,\n",
    "   'pearson': (0.4007830218587774, 2.6189184388944674e-30),\n",
    "   'spearman': SpearmanrResult(correlation=0.42664203634904674, pvalue=1.5750695641919273e-34)},\n",
    "  'SMTeuroparl': {'nsamples': 459,\n",
    "   'pearson': (0.3271387330066187, 6.558578218961659e-13),\n",
    "   'spearman': SpearmanrResult(correlation=0.44585313264930493, pvalue=8.423250166894389e-24)},\n",
    "  'all': {'pearson': {'mean': 0.2958716812971679, 'wmean': 0.3028944887220307},\n",
    "   'spearman': {'mean': 0.3631602533857541, 'wmean': 0.36154137754353355}},\n",
    "  'surprise.OnWN': {'nsamples': 750,\n",
    "   'pearson': (0.4010845876707513, 2.349634411380406e-30),\n",
    "   'spearman': SpearmanrResult(correlation=0.483258535961669, pvalue=3.785359493165831e-45)},\n",
    "  'surprise.SMTnews': {'nsamples': 399,\n",
    "   'pearson': (0.2077645658443937, 2.8793575626173877e-05),\n",
    "   'spearman': SpearmanrResult(correlation=0.30893757034281394, pvalue=2.852474352467182e-10)}},\n",
    " 'STS13': {'FNWN': {'nsamples': 189,\n",
    "   'pearson': (0.1796998687694542, 0.013351592089388827),\n",
    "   'spearman': SpearmanrResult(correlation=0.17312087397497752, pvalue=0.0172074164275323)},\n",
    "  'OnWN': {'nsamples': 561,\n",
    "   'pearson': (-0.036391665423154564, 0.38961542391537196),\n",
    "   'spearman': SpearmanrResult(correlation=-0.0064082359237449225, pvalue=0.8796247798086678)},\n",
    "  'all': {'pearson': {'mean': 0.1506315417021541,\n",
    "    'wmean': 0.16332491147677275},\n",
    "   'spearman': {'mean': 0.15573074065761325, 'wmean': 0.1696563418461701}},\n",
    "  'headlines': {'nsamples': 750,\n",
    "   'pearson': (0.30858642176016265, 5.202433717123146e-18),\n",
    "   'spearman': SpearmanrResult(correlation=0.30047958392160706, pvalue=4.087419263459101e-17)}},\n",
    " 'STS14': {'OnWN': {'nsamples': 750,\n",
    "   'pearson': (0.13495201475831842, 0.00021004588531773239),\n",
    "   'spearman': SpearmanrResult(correlation=0.19518669302333042, pvalue=7.10871057435418e-08)},\n",
    "  'all': {'pearson': {'mean': 0.23573767456033798,\n",
    "    'wmean': 0.2615492418576007},\n",
    "   'spearman': {'mean': 0.2741601729177288, 'wmean': 0.292553611332301}},\n",
    "  'deft-forum': {'nsamples': 450,\n",
    "   'pearson': (0.156784942801156, 0.0008458976964767222),\n",
    "   'spearman': SpearmanrResult(correlation=0.18466330729597857, pvalue=8.136498539192537e-05)},\n",
    "  'deft-news': {'nsamples': 300,\n",
    "   'pearson': (0.07327643492260316, 0.20566038816830512),\n",
    "   'spearman': SpearmanrResult(correlation=0.18054609654412698, pvalue=0.0016898349432055404)},\n",
    "  'headlines': {'nsamples': 750,\n",
    "   'pearson': (0.26463587403452904, 1.7451281547738147e-13),\n",
    "   'spearman': SpearmanrResult(correlation=0.245732305648832, pvalue=8.895765565765384e-12)},\n",
    "  'images': {'nsamples': 750,\n",
    "   'pearson': (0.4466387586578476, 4.70476518566636e-38),\n",
    "   'spearman': SpearmanrResult(correlation=0.46627739473892865, pvalue=9.440809367485879e-42)},\n",
    "  'tweet-news': {'nsamples': 750,\n",
    "   'pearson': (0.33813802218757344, 1.6208217071906041e-21),\n",
    "   'spearman': SpearmanrResult(correlation=0.3725552402551761, pvalue=4.164134071189017e-26)}},\n",
    " 'STS15': {'all': {'pearson': {'mean': 0.2724451880320972,\n",
    "    'wmean': 0.31685872675093085},\n",
    "   'spearman': {'mean': 0.2882838350353617, 'wmean': 0.3327150079550635}},\n",
    "  'answers-forums': {'nsamples': 375,\n",
    "   'pearson': (0.039706422734896954, 0.4432914403188235),\n",
    "   'spearman': SpearmanrResult(correlation=0.04068902993621121, pvalue=0.43208160190150313)},\n",
    "  'answers-students': {'nsamples': 750,\n",
    "   'pearson': (0.28705987824827633, 1.0775132175080352e-15),\n",
    "   'spearman': SpearmanrResult(correlation=0.30579921985230657, pvalue=1.0645513662148402e-17)},\n",
    "  'belief': {'nsamples': 375,\n",
    "   'pearson': (0.14987564357862884, 0.0036248415408096798),\n",
    "   'spearman': SpearmanrResult(correlation=0.1804292567768978, pvalue=0.0004461019865200121)},\n",
    "  'headlines': {'nsamples': 750,\n",
    "   'pearson': (0.3818721603396246, 1.8961210851164007e-27),\n",
    "   'spearman': SpearmanrResult(correlation=0.3751911032601324, pvalue=1.7553466470148097e-26)},\n",
    "  'images': {'nsamples': 750,\n",
    "   'pearson': (0.5037118352590594, 1.6789112333633647e-49),\n",
    "   'spearman': SpearmanrResult(correlation=0.5393105653512604, pvalue=8.059509148311837e-58)}},\n",
    " 'STS16': {'all': {'pearson': {'mean': 0.3037491915973627,\n",
    "    'wmean': 0.31286456501095683},\n",
    "   'spearman': {'mean': 0.3748289458274702, 'wmean': 0.3827874432860581}},\n",
    "  'answer-answer': {'nsamples': 254,\n",
    "   'pearson': (0.33687184761736494, 3.7108326715175343e-08),\n",
    "   'spearman': SpearmanrResult(correlation=0.3881588272050183, pvalue=1.4670713101796024e-10)},\n",
    "  'headlines': {'nsamples': 249,\n",
    "   'pearson': (0.37639387167679833, 8.426999310924058e-10),\n",
    "   'spearman': SpearmanrResult(correlation=0.3761729728472852, pvalue=8.635789491144841e-10)},\n",
    "  'plagiarism': {'nsamples': 230,\n",
    "   'pearson': (0.32884426285695534, 3.3498363949391107e-07),\n",
    "   'spearman': SpearmanrResult(correlation=0.47740807184303585, pvalue=1.702272150989612e-14)},\n",
    "  'postediting': {'nsamples': 244,\n",
    "   'pearson': (0.4719643243971815, 6.13050772417937e-15),\n",
    "   'spearman': SpearmanrResult(correlation=0.5642863914496445, pvalue=6.455918799404908e-22)},\n",
    "  'question-question': {'nsamples': 209,\n",
    "   'pearson': (0.00467165143851349, 0.9464761414626301),\n",
    "   'spearman': SpearmanrResult(correlation=0.06811846579236705, pvalue=0.3270811925573087)}},\n",
    " 'STSBenchmark': {'devpearson': 0.5512091403813993,\n",
    "  'mse': 2.0149316274007862,\n",
    "  'ndev': 1500,\n",
    "  'ntest': 1379,\n",
    "  'pearson': 0.49412586914630235,\n",
    "  'spearman': 0.47672368959625055,\n",
    "  'yhat': array([2.34865983, 2.34609645, 2.41522208, ..., 3.80854183, 3.47457961,\n",
    "         3.54439748])}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiNLI\n",
    "https://www.kaggle.com/takahirokubo0/multinli-dataset-analysis\n",
    "\n",
    "More code mixed: http://mt-archive.info/EMNLP-2008-Solorio.pdf\n",
    "Miami Dataset\n",
    "http://bangortalk.org.uk/chats/\n",
    "https://github.com/SeedlingsBabylab/parse_clan2\n",
    "https://github.com/SeedlingsBabylab/clancomments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_logits(preds, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample an index from a logit vector.\n",
    "\n",
    "    :param preds:\n",
    "    :param temperature:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        return np.argmax(preds)\n",
    "\n",
    "    preds = preds / temperature\n",
    "    preds = preds - logsumexp(preds)\n",
    "\n",
    "    choice = np.random.choice(len(preds), 1, p=np.exp(preds))\n",
    "\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model : Model, seed, size, out_num=3, temperature=1.0):\n",
    "    \"\"\"\n",
    "    :param model: The complete RNN language model\n",
    "    :param seed: The first few wordas of the sequence to start generating from\n",
    "    :param size: The total size of the sequence to generate\n",
    "    :param temperature: This controls how much we follow the probabilities provided by the network. For t=1.0 we just\n",
    "        sample directly according to the probabilities. Lower temperatures make the high-probability words more likely\n",
    "        (providing more likely, but slightly boring sentences) and higher temperatures make the lower probabilities more\n",
    "        likely (resulting is weirder sentences). For temperature=0.0, the generation is _greedy_, i.e. the word with the\n",
    "        highest probability is always chosen.\n",
    "    :return: A list of integers representing a samples sentence\n",
    "    \"\"\"\n",
    "\n",
    "    ls = seed.shape[0]\n",
    "\n",
    "    # Due to the way Keras RNNs work, we feed the model a complete sequence each time. At first it's just the seed,\n",
    "    # zero-padded to the right length. With each iteration we sample and set the next character.\n",
    "    \n",
    "    # tokens = np.concatenate([seed, np.zeros(size - ls)])\n",
    "    tokens_all = []\n",
    "    for i in range(out_num):\n",
    "        tokens_all.append(np.concatenate([seed, np.zeros(size - ls)]))\n",
    "\n",
    "    for i in range(ls, size):\n",
    "        \n",
    "        tokens_to_predict = []\n",
    "        for j in range(out_num):\n",
    "            tokens_to_predict.append(tokens_all[j][None,:])\n",
    "        \n",
    "        all_probs = model.predict(tokens_to_predict)\n",
    "\n",
    "        # Extract the i-th probability vector and sample an index from it\n",
    "        for j, probs in enumerate(all_probs):\n",
    "            next_token = util.sample_logits(probs[0, i-1, :], temperature=temperature)\n",
    "            tokens_all[j][i] = next_token\n",
    "\n",
    "    return [tokens.astype('int') for tokens in tokens_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = sentence_es[600][:1]\n",
    "# seed = np.insert(seed, 0, 2)\n",
    "a = generate_seq(model, seed, 25, out_num=3, temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> i welcome the rapporteur <unk> d <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "<sos> por lo tanto slo me gustara entender que de esta razn se trata de un tema que cabe decir a los problemas medioambientales en\n",
      "\n",
      "<sos> now that unfortunately has buy up using drugs as many nature many rights including anti self competition could help simplify imports against each state\n"
     ]
    }
   ],
   "source": [
    "print(decode(a[0]))\n",
    "print()\n",
    "print(decode(a[1]))\n",
    "print()\n",
    "print(decode(a[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## Data Pre-processing Note\n",
    "\n",
    "### Genism\n",
    "https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html uses [WikiCorpus of gensim](https://radimrehurek.com/gensim/corpora/wikicorpus.html). I wasn't able to find a way to preseve line ending and that sucks.\n",
    "\n",
    "### wikiextractor\n",
    "Using a modified version of https://tiefenauer.github.io/blog/wiki-n-gram-lm/. Uses https://github.com/attardi/wikiextractor in its first step followed by a bash and weird script.\n",
    "The script being\n",
    "\n",
    "```\n",
    "result=$(find ./cleaned_wiki/ -name '*bz2' -exec bzcat {} \\+ \\\n",
    "        | pv \\\n",
    "        | tee >(    sed 's/<[^>]*>//g' \\\n",
    "                  | sed 's|[\"'\\'']||g' \\\n",
    "                  | python3 ./wiki_cleaner2.py es >> wiki_es2.txt \\\n",
    "               ) \\\n",
    "        | grep -e \"<doc\" \\\n",
    "        | wc -l);\n",
    "\n",
    "```\n",
    "\n",
    "## news\n",
    "\n",
    "cat news.es.all | ../normalize-punctuation.perl -l es | ../scripts/tokenizer.perl -l es -no-escape -threads 40 > new.es.all.tok\n",
    "cat news*es.shuffled > news.es.all \n",
    "\n",
    "\n",
    "## Making Vocab\n",
    "\n",
    "```\n",
    "from gensim.corpora import WikiCorpus\n",
    "wiki = WikiCorpus('datasets/wiki_dataset/raw/eswiki-latest-pages-articles-multistream.xml.bz2')\n",
    "from gensim.corpora import MmCorpus\n",
    "MmCorpus.serialize('datasets/wiki_dataset/wiki_es.mm', wiki)\n",
    "wiki.dictionary.save('datasets/wiki_dataset/wiki_es.vocab')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model : Model, seed, size, temperature=1.0):\n",
    "    \"\"\"\n",
    "    :param model: The complete RNN language model\n",
    "    :param seed: The first few wordas of the sequence to start generating from\n",
    "    :param size: The total size of the sequence to generate\n",
    "    :param temperature: This controls how much we follow the probabilities provided by the network. For t=1.0 we just\n",
    "        sample directly according to the probabilities. Lower temperatures make the high-probability words more likely\n",
    "        (providing more likely, but slightly boring sentences) and higher temperatures make the lower probabilities more\n",
    "        likely (resulting is weirder sentences). For temperature=0.0, the generation is _greedy_, i.e. the word with the\n",
    "        highest probability is always chosen.\n",
    "    :return: A list of integers representing a samples sentence\n",
    "    \"\"\"\n",
    "\n",
    "    ls = seed.shape[0]\n",
    "\n",
    "    # Due to the way Keras RNNs work, we feed the model a complete sequence each time. At first it's just the seed,\n",
    "    # zero-padded to the right length. With each iteration we sample and set the next character.\n",
    "\n",
    "    tokens = np.concatenate([seed, np.zeros(size - ls)])\n",
    "\n",
    "    for i in range(ls, size):\n",
    "\n",
    "        probs = model.predict([tokens[None,:],tokens[None,:]])\n",
    "\n",
    "        # Extract the i-th probability vector and sample an index from it\n",
    "        next_token = util.sample_logits(probs[0][0, i-1, :], temperature=temperature)\n",
    "\n",
    "        tokens[i] = next_token\n",
    "\n",
    "    return [int(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generate_seq(model, x_en[21][:10], 60, temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK> southern portion <UNK> <UNK> county has <UNK> strong history <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> times <UNK> any or birds lakers transposition <UNK> <UNK> gerolamo <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> his <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> detected games protection <UNK> music <UNK> <UNK> an <UNK> num <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
